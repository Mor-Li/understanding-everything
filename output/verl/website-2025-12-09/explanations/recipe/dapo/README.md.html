<h1>recipe/dapo/README.md</h1>
<p>这份文档确实比较硬核，它是一个名为 <strong>DAPO</strong> (Decoupled Clip and Dynamic Sampling Policy Optimization) 的人工智能训练算法的说明书。</p>
<p>简单来说，这是一个用来<strong>训练大模型（比如 Qwen）做数学题</strong>的“秘方”（Recipe）。作者通过改进传统的强化学习（PPO）方法，让模型在 AIME（美国数学邀请赛）这种高难度数学题上取得了比 DeepSeek-R1-Zero 更好的成绩。</p>
<p>为了让你听懂，我把这份文档的内容拆解成一个<strong>“训练 AI 做数学题的 Todo List”</strong>，我们一步步来看它是怎么改进训练过程的。</p>
<hr />
<h3>📋 任务清单：如何用 DAPO 训练一个更聪明的 AI</h3>
<p>想象你是一个严厉的老师，正在教一个学生（AI 模型）解奥数题。DAPO 就是你的教学大纲，它包含以下几个关键步骤：</p>
<h4>✅ 第一步：准备教材和环境 (Quickstart)</h4>
<ul>
<li><strong>文档内容</strong>：<code>bash prepare_dapo_data.sh</code> 和 <code>run_dapo_qwen2.5_32b.sh</code></li>
<li><strong>解释</strong>：<ul>
<li>你需要先下载数学题库（Dataset）。</li>
<li>你需要准备好计算集群（Ray Cluster），因为训练这种模型需要很多显卡（比如 16 台 H800）。</li>
<li><strong>观点</strong>：DAPO 是基于 <code>verl</code> 框架开发的，强调开源和易用性，只要有算力，你也能复现。</li>
</ul>
</li>
</ul>
<h4>✅ 第二步：改进“评分标准” —— 动态截断 (Separated Clip Epsilons)</h4>
<p>这是 DAPO 的核心创新点之一。
*   <strong>文档关键词</strong>：<code>Clip-Higher</code>, <code>clip_ratio_low</code>, <code>clip_ratio_high</code>
*   <strong>传统做法 (PPO)</strong>：为了防止学生（模型）步子迈太大扯着蛋，通常会限制模型每次更新的幅度（Clip）。不管模型是变好了还是变坏了，限制范围是一样的。
*   <strong>DAPO 的观点</strong>：
    *   如果学生<strong>做对了</strong>（Advantage &gt; 0），我们可以允许它步子迈大一点（<code>clip_ratio_high</code> 设得高一点，比如 0.28），鼓励它多往这个方向学。
    *   如果学生<strong>做错了</strong>，我们要严格一点（<code>clip_ratio_low</code> 保持 0.2），防止它学偏。
    *   <strong>总结</strong>：<strong>“奖励要大方，惩罚要克制”</strong>，这样学得更快。</p>
<h4>✅ 第三步：改进“出题策略” —— 动态采样 (Dynamic Sampling)</h4>
<p>这是 DAPO 的另一个核心。
*   <strong>文档关键词</strong>：<code>filter_groups</code>, <code>gen_batch_size</code>
*   <strong>痛点</strong>：有时候你给 AI 一道题，让它生成 10 个答案。结果这 10 个答案<strong>全是对的</strong>，或者<strong>全是错的</strong>。
    *   全对或全错在强化学习中很难产生“对比”，AI 学不到东西（因为它不知道什么是更好的）。
*   <strong>DAPO 的观点</strong>：
    *   我们需要<strong>有对比</strong>的数据。
    *   <strong>做法</strong>：如果这一批生成的答案全是一样的结果（比如全对），那就<strong>扔掉重做</strong>，或者继续生成，直到凑够了一批既有对又有错的数据。
    *   这就是代码里 <code>Keep generating...</code> 的意思。
    *   <strong>总结</strong>：<strong>“没有对比就没有伤害（也就没有进步），必须强制制造对比。”</strong></p>
<h4>✅ 第三步半：改进“算分方式” (Token-level Loss)</h4>
<ul>
<li><strong>文档关键词</strong>：<code>loss_agg_mode: "token-mean"</code></li>
<li><strong>痛点</strong>：一道题的答案很长。以前算误差（Loss）时，通常是把一整句话算一个平均分。</li>
<li><strong>DAPO 的观点</strong>：<ul>
<li>不要按“句子”平均，要按“字（Token）”平均。把整个批次里所有的字放在一起算平均误差。</li>
<li><strong>总结</strong>：这种算分方式在数学推理这种长文本任务中更稳定。</li>
</ul>
</li>
</ul>
<h4>✅ 第四步：防止“废话连篇” (Overlong Reward Shaping)</h4>
<ul>
<li><strong>文档关键词</strong>：<code>Overlong Reward Shaping</code>, <code>overlong_buffer</code></li>
<li><strong>痛点</strong>：为了解出难题，AI 有时候会像 DeepSeek-R1 那样进行超长的思考。但有时候它会陷入死循环，废话连篇，直到把显存撑爆。</li>
<li><strong>DAPO 的观点</strong>：<ul>
<li>设置一个“缓冲区”。比如最大长度限制是 20480 个字。</li>
<li>如果 AI 写到 16384 个字还没写完，从这里开始，每多写一个字，就<strong>扣一点分</strong>（Penalty）。</li>
<li><strong>总结</strong>：<strong>“鼓励思考，但禁止啰嗦。快超时了就要开始罚分，逼它赶紧收尾。”</strong></li>
</ul>
</li>
</ul>
<hr />
<h3>🏆 最终结果 (Results)</h3>
<p>通过上面这一套组合拳（Todo List），文档展示了成果：
1.  <strong>准确率更高</strong>：在 AIME 2024 数学竞赛题上达到了 <strong>52%</strong> 的准确率。
2.  <strong>效率更高</strong>：比之前的 SOTA（DeepSeek-R1-Zero-Qwen-32B）用的训练步数少了一半。</p>
<h3>总结一下它的核心逻辑：</h3>
<p>如果你要复现这个项目，你的心理活动应该是这样的：
1.  <strong>下数据，配环境。</strong>
2.  <strong>改配置</strong>：告诉算法，如果模型学得好，允许它多变一点（Clip-Higher）。
3.  <strong>改采样</strong>：告诉算法，如果生成的答案没有区分度，就一直生成，直到有区分度为止（Dynamic Sampling）。
4.  <strong>改惩罚</strong>：告诉算法，如果废话太多快超长了，就狠狠地扣分。</p>
<p>这个文档主要就是把上面这些逻辑变成了代码配置 (<code>yaml</code>) 和 Python 代码片段展示给你看。</p>