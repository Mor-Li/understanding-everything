<h1>recipe/dapo/test_dapo_qwen3_30b_math_single_node.sh</h1>
<p>这份脚本实际上是一个<strong>启动“大模型强化学习训练”的配置文件</strong>。</p>
<p>你可以把它想象成是一个<strong>复杂的“烹饪菜谱”</strong>。虽然里面全是代码，但它的逻辑其实就是告诉计算机：“用什么原料（数据、模型），在什么锅里（GPU配置），用多大的火候（超参数），煮多长时间（训练步数）”。</p>
<p>为了让你更容易理解，我把你这个脚本的逻辑拆解成一个<strong>任务清单（To-Do List）</strong>，然后一步步给你解释。</p>
<hr />
<h3>📋 任务清单 (Task To-Do List)</h3>
<p>运行这个脚本时，计算机实际上在按顺序执行以下任务：</p>
<ol>
<li><strong>【起名与定调】</strong>：给这次训练起个名字，确定我们要用什么算法（这里是 GRPO）。</li>
<li><strong>【设定规则】</strong>：规定模型能读多长、写多长，以及“惩罚”和“奖励”的力度。</li>
<li><strong>【准备硬件环境】</strong>：确认有多少台机器、多少张显卡，以及如何连接它们。</li>
<li><strong>【指定原料】</strong>：告诉程序去哪里找基础模型（Qwen3-30B）和训练数据（数学题）。</li>
<li><strong>【系统性能优化】</strong>：因为模型很大（30B），需要配置显存切分和并行策略，防止显存爆炸。</li>
<li><strong>【启动主程序】</strong>：把上面所有设定打包，传给 Python 程序开始真正的训练。</li>
</ol>
<hr />
<h3>🧐 逐步详解 (Step-by-Step Explanation)</h3>
<p>下面我们按照文件内容的顺序，把上面的 Task 展开讲讲：</p>
<h4>第一步：起名与定调 (Basic Config)</h4>
<div class="codehilite"><pre><span></span><code><span class="nv">project_name</span><span class="o">=</span><span class="s1">&#39;DAPO&#39;</span>
<span class="nv">exp_name</span><span class="o">=</span><span class="s1">&#39;DAPO-Qwen3-30B-A3B-Base-MATH-0719a1&#39;</span>
<span class="nv">adv_estimator</span><span class="o">=</span>grpo
</code></pre></div>

<ul>
<li><strong>含义</strong>：<ul>
<li>我们要做的项目叫 <code>DAPO</code>。</li>
<li>这次实验的具体代号是 <code>DAPO-Qwen3-30B...</code>（方便以后在日志里找）。</li>
<li><strong>核心点</strong>：<code>adv_estimator=grpo</code>。这说明它用的不是传统的 PPO，而是 <strong>GRPO</strong>（一种更省显存的强化学习算法，最近 DeepSeek-R1 也在用类似的思路）。</li>
</ul>
</li>
</ul>
<h4>第二步：设定规则 (Hyperparameters)</h4>
<div class="codehilite"><pre><span></span><code><span class="nv">use_kl_in_reward</span><span class="o">=</span>False
<span class="nv">kl_coef</span><span class="o">=</span><span class="m">0</span>.0
...
<span class="nv">max_prompt_length</span><span class="o">=</span><span class="k">$((</span><span class="m">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">2</span><span class="k">))</span><span class="w">  </span><span class="c1"># 2048</span>
<span class="nv">max_response_length</span><span class="o">=</span><span class="k">$((</span><span class="m">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">4</span><span class="k">))</span><span class="w"> </span><span class="c1"># 4096</span>
</code></pre></div>

<ul>
<li><strong>含义</strong>：<ul>
<li><code>kl_coef=0.0</code>：通常强化学习会通过 KL 散度防止模型“学歪了”或者偏离原模型太远。这里设为 0，说明这次训练允许模型大幅度探索，不受原模型束缚（比较激进）。</li>
<li><strong>长度限制</strong>：题目最长 2048 个 token，模型回答最长 4096 个 token。这是为了做数学题（Math），因为解题步骤通常很长。</li>
</ul>
</li>
</ul>
<h4>第三步：准备硬件与路径 (Infrastructure &amp; Paths)</h4>
<div class="codehilite"><pre><span></span><code><span class="nv">NNODES</span><span class="o">=</span><span class="si">${</span><span class="nv">NNODES</span><span class="k">:-</span><span class="nv">1</span><span class="si">}</span><span class="w">           </span><span class="c1"># 用几台机器？默认1台</span>
<span class="nv">NGPUS_PER_NODE</span><span class="o">=</span><span class="si">${</span><span class="nv">NGPUS_PER_NODE</span><span class="k">:-</span><span class="nv">8</span><span class="si">}</span><span class="w"> </span><span class="c1"># 每台机器几张卡？默认8张</span>
...
<span class="nv">MODEL_PATH</span><span class="o">=</span>.../Qwen3-30B-A3B-Base<span class="w">  </span><span class="c1"># 学生模型在哪？</span>
<span class="nv">TRAIN_FILE</span><span class="o">=</span>.../dapo-math-17k.parquet<span class="w"> </span><span class="c1"># 教材（训练题）在哪？</span>
<span class="nv">TEST_FILE</span><span class="o">=</span>.../aime-2024.parquet<span class="w">      </span><span class="c1"># 考卷（测试题）在哪？</span>
</code></pre></div>

<ul>
<li><strong>含义</strong>：<ul>
<li>这是一个单机 8 卡的训练任务。</li>
<li><strong>学生</strong>：Qwen3-30B（通义千问3代，300亿参数版本）。</li>
<li><strong>教材</strong>：DAPO Math 17k 数据集。</li>
<li><strong>考试</strong>：AIME 2024（美国数学邀请赛题目），用来验证模型是不是变聪明了。</li>
</ul>
</li>
</ul>
<h4>第四步：系统性能优化 (Performance - <em>最难懂的部分</em>)</h4>
<div class="codehilite"><pre><span></span><code><span class="nv">sp_size</span><span class="o">=</span><span class="m">4</span><span class="w">        </span><span class="c1"># 序列并行 (Sequence Parallel)</span>
<span class="nv">offload</span><span class="o">=</span>True<span class="w">     </span><span class="c1"># 卸载到CPU (Offload)</span>
<span class="nv">gen_tp</span><span class="o">=</span><span class="m">4</span><span class="w">         </span><span class="c1"># 生成时的张量并行 (Tensor Parallel)</span>
<span class="nv">fsdp_size</span><span class="o">=</span><span class="m">8</span><span class="w">      </span><span class="c1"># 全切片数据并行 (FSDP)</span>
</code></pre></div>

<ul>
<li><strong>含义</strong>：<ul>
<li><strong>背景</strong>：30B 的模型很大，普通的显卡显存根本装不下，更别提还要训练（训练需要的显存是推理的数倍）。</li>
<li><strong>解决办法</strong>：<ul>
<li><code>gen_tp=4</code>：把模型切成 4 份，由 4 张显卡拼在一起进行推理（生成答案）。</li>
<li><code>fsdp_size=8</code>：把模型的参数和梯度切碎，分散在 8 张卡上。</li>
<li><code>offload=True</code>：显存不够时，把暂时不用的数据搬到内存（CPU）里去，虽然慢点，但能跑起来。</li>
</ul>
</li>
<li><strong>总结</strong>：这一段全是<strong>为了让 30B 的大模型能在有限的显卡上跑起来</strong>而做的技术妥协和优化。</li>
</ul>
</li>
</ul>
<h4>第五步：启动主程序 (Execution)</h4>
<div class="codehilite"><pre><span></span><code>python3<span class="w"> </span>-m<span class="w"> </span>verl.trainer.main_ppo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.train_files<span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">TRAIN_FILE</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>...
<span class="w">    </span>reward_model.reward_manager<span class="o">=</span>dapo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trainer.total_epochs<span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>...
</code></pre></div>

<ul>
<li><strong>含义</strong>：<ul>
<li>最后这一大段 <code>python3 ...</code> 才是真正的“点火发射”。</li>
<li>它调用了 <code>verl</code> 框架（一个专门做大模型强化学习的库）。</li>
<li><strong>关键点</strong>：<code>reward_model.reward_manager=dapo</code>。这说明奖励函数（Reward Function）是定制的，叫 <code>dapo</code>。这通常是这篇论文或项目的核心创新点——怎么给模型的答案打分。</li>
<li><strong>节奏</strong>：训练 10 个周期 (<code>total_epochs=10</code>)，总共走 300 步 (<code>total_training_steps=300</code>)。</li>
</ul>
</li>
</ul>
<hr />
<h3>💡 总结 (Summary)</h3>
<p><strong>这个文件的作用是：</strong></p>
<p>使用 <strong>GRPO 算法</strong>，在 <strong>8张显卡</strong> 上，对 <strong>Qwen3-30B</strong> 模型进行<strong>数学解题能力</strong>的强化学习训练。</p>
<ul>
<li><strong>它的输入是</strong>：一些数学题（Math 17k）。</li>
<li><strong>它的目标是</strong>：让模型学会做更难的数学题（用 AIME 2024 验证）。</li>
<li><strong>它的手段是</strong>：利用 <code>DAPO</code> 这种特殊的奖励机制来指导模型。</li>
<li><strong>它的难点是</strong>：模型太大，所以脚本里写满了各种显存优化技巧（Offload, TP, SP, FSDP）。</li>
</ul>