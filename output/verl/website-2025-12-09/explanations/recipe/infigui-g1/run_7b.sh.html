<h1>recipe/infigui-g1/run_7b.sh</h1>
<p>这份文件是一个 <strong>Shell 脚本 (<code>.sh</code>)</strong>，它的作用是<strong>启动一个 AI 模型的训练任务</strong>。</p>
<p>简单来说，这就像是给电脑下达的一张“详细指令单”，告诉电脑：“我要用什么教材、训练哪个大脑、用什么方法训练、以及用多少显卡来跑”。</p>
<p>为了让你能看懂，我把它拆解成一个 <strong>“6步走的任务清单 (To-Do List)”</strong>。我们按照这个逻辑一步步来看：</p>
<hr />
<h3>✅ Task 1: 搞清楚我们要干什么 (核心目标)</h3>
<p><strong>代码对应：</strong> <code>python3 -m verl.trainer.main_ppo ...</code>
*   <strong>解读：</strong>
    *   这行命令是整个脚本的主角。它启动了一个 Python 程序，使用的是 <code>verl</code> 这个库。
    *   <code>main_ppo</code> 里的 <strong>PPO</strong> (Proximal Policy Optimization) 是强化学习（Reinforcement Learning）的一种经典算法。
    *   <strong>通俗理解：</strong> 我们不是在做普通的“背书”训练（SFT），而是在做“<strong>实战演练</strong>”。让模型去尝试完成任务，做得好给奖励，做得差给惩罚，以此来进化模型。</p>
<h3>✅ Task 2: 确认“受训学生”是谁 (模型配置)</h3>
<p><strong>代码对应：</strong></p>
<blockquote>
<p><code>actor_rollout_ref.model.path=Qwen/Qwen2.5-VL-7B-Instruct</code>
<code>data.image_key=images</code>
*   <strong>解读：</strong>
    *   <strong>学生是谁？</strong> 是 <code>Qwen2.5-VL-7B-Instruct</code>。这是一个通义千问的视觉-语言模型（VL = Vision Language），参数量是 70亿（7B）。
    *   <strong>能力：</strong> 因为是 VL 模型，所以它能“看图”。脚本里也专门指定了 <code>image_key</code>，说明训练数据里包含图片。
    *   <strong>通俗理解：</strong> 我们要训练一个能看懂图片（比如电脑屏幕截图）的 AI 助手。</p>
</blockquote>
<h3>✅ Task 3: 准备“教材”和“考题” (数据配置)</h3>
<p><strong>代码对应：</strong></p>
<blockquote>
<p><code>data.train_files=.../omniact_filtered_train.parquet</code>
<code>data.max_prompt_length=7168</code>
*   <strong>解读：</strong>
    *   <strong>教材：</strong> 数据集来自 <code>omniact</code>。这是一个专门针对 GUI（图形用户界面）操作的数据集。
    *   <strong>长度：</strong> <code>max_prompt_length=7168</code>。这非常长！说明模型需要处理很长的上下文（比如复杂的屏幕操作历史或很大的网页代码）。
    *   <strong>通俗理解：</strong> 我们给 AI 准备了一堆电脑操作的任务（比如“点击在这个按钮上”），让它学习如何操作电脑界面。</p>
</blockquote>
<h3>✅ Task 4: 制定“评分标准” (奖励函数)</h3>
<p><strong>代码对应：</strong></p>
<blockquote>
<p><code>custom_reward_function.path=.../reward_fn.py</code>
<code>algorithm.adv_estimator=rloo</code>
*   <strong>解读：</strong>
    *   <strong>怎么评分？</strong> 强化学习最关键的是“奖励”。这里加载了一个自定义的 Python 文件 <code>reward_fn.py</code>。
    *   <strong>RLOO：</strong> 这是一种具体的算法优化手段（REINFORCE Leave-One-Out），用来更高效地计算模型做得“有多好”。
    *   <strong>通俗理解：</strong> AI 尝试操作后，这个脚本会自动判断：“你点对了吗？”。点对了加分，点错了没分。</p>
</blockquote>
<h3>✅ Task 5: 设定“训练强度” (超参数与生成)</h3>
<p><strong>代码对应：</strong></p>
<blockquote>
<p><code>actor_rollout_ref.rollout.n=8</code>
<code>actor_rollout_ref.rollout.name=sglang</code>
<code>trainer.total_epochs=6</code>
*   <strong>解读：</strong>
    *   <strong>尝试次数 (<code>n=8</code>)：</strong> 对于同一个问题，让模型生成 <strong>8 种</strong> 不同的尝试方案，然后从中挑最好的学习。
    *   <strong>加速引擎 (<code>sglang</code>)：</strong> 使用 <code>sglang</code> 这个工具来加速推理，让模型生成答案快一点。
    *   <strong>训练时长：</strong> 整个数据集要跑 6 轮 (<code>epochs=6</code>)。
    *   <strong>通俗理解：</strong> 这是一个高强度的特训营。遇到一个问题，让 AI 试错 8 次，反复训练 6 遍。</p>
</blockquote>
<h3>✅ Task 6: 配置“后勤保障” (硬件与日志)</h3>
<p><strong>代码对应：</strong></p>
<blockquote>
<p><code>trainer.n_gpus_per_node=8</code>
<code>trainer.project_name='infigui-g1'</code>
<code>trainer.logger=['console','wandb']</code>
*   <strong>解读：</strong>
    *   <strong>显卡：</strong> 使用 8 张 GPU 并行训练。
    *   <strong>记录：</strong> 训练过程会打印在控制台 (<code>console</code>)，并同步上传到 <code>wandb</code>（一个可视化的实验看板），项目名叫 <code>infigui-g1</code>。
    *   <strong>通俗理解：</strong> 动用了 8 台“发动机”一起跑，并且有一个在线仪表盘实时监控训练进度。</p>
</blockquote>
<hr />
<h3>总结：这脚本到底在干啥？</h3>
<p><strong>一句话总结：</strong>
这个脚本在使用 8 张显卡，通过强化学习（RLOO算法），训练 <strong>Qwen2.5-VL-7B</strong> 这个能看图的模型，让它在 <strong>OmniAct</strong>（电脑操作/GUI导航）任务上表现得更好。</p>
<p><strong>它在模拟这样的过程：</strong>
1.  给模型看一张电脑屏幕截图和一句指令。
2.  让模型尝试操作 8 次。
3.  用奖励函数判断哪次操作是对的。
4.  修改模型的大脑，让它记住正确的操作逻辑。</p>