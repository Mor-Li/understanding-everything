<h1>recipe/infigui-g1/run_3b.sh</h1>
<p>这份文件本质上是一个<strong>启动脚本（Shell Script）</strong>。就像是你做菜前的“备菜清单”和“烹饪步骤”，它告诉计算机：“我要用什么食材（数据）、用什么锅（模型）、用什么火候（参数）来训练这个AI。”</p>
<p>具体来说，这是在使用 <strong>VeRL</strong>（一个强化学习训练框架）来训练一个多模态大模型（Qwen2.5-VL-3B）。</p>
<p>为了让你好理解，我把你当作这个AI项目的“项目经理”，把这个脚本拆解成一个 <strong>“训练任务 To-Do List”</strong>，一步步告诉你它在安排什么工作：</p>
<hr />
<h3>✅ Task 1: 确定训练的核心方法 (Algorithm)</h3>
<p><strong>脚本内容：</strong> <code>verl.trainer.main_ppo</code>, <code>algorithm.adv_estimator=rloo</code>
*   <strong>白话解释：</strong>
    我们要用的训练方法叫做 <strong>PPO (Proximal Policy Optimization)</strong> 的一种变体（这里用了 RLOO）。
    *   这不是简单的“背书”（预训练/SFT），而是“试错学习”（强化学习）。
    *   就像教小狗握手：它做对了给奖励，做错了没奖励。模型会尝试生成答案，我们给它打分，它根据分数自我调整。</p>
<h3>✅ Task 2: 准备教材 (Data)</h3>
<p><strong>脚本内容：</strong> <code>data.train_files</code>, <code>data.image_key=images</code>, <code>max_prompt_length=7168</code>
*   <strong>白话解释：</strong>
    *   <strong>教材来源</strong>：训练数据在 <code>./data/omniact_grounding_filtered/</code> 目录下的 parquet 文件里。
    *   <strong>特别注意</strong>：这是一个 <strong>多模态任务</strong>（<code>image_key=images</code>），意味着模型不仅要读文字，还要<strong>看图</strong>。
    *   <strong>课文长度</strong>：允许输入的提示词（Prompt）非常长（最多7000多字/token），说明可能包含复杂的GUI界面描述或长指令。</p>
<h3>✅ Task 3: 选定“学生” (Model)</h3>
<p><strong>脚本内容：</strong> <code>actor_rollout_ref.model.path=Qwen/Qwen2.5-VL-3B-Instruct</code>
*   <strong>白话解释：</strong>
    *   我们要训练的底座模型是 <strong>Qwen2.5-VL-3B</strong>。
    *   这是一个由阿里云开发的、能看懂图片的、参数量为30亿的轻量级模型。</p>
<h3>✅ Task 4: 制定评分标准 (Reward Function)</h3>
<p><strong>脚本内容：</strong> <code>custom_reward_function.path=...reward_fn.py</code>, <code>name=aer_gui_reward_function</code>
*   <strong>白话解释：</strong>
    *   这是强化学习的灵魂。
    *   脚本指定了一个外部的 Python 文件（<code>reward_fn.py</code>）作为“考官”。
    *   当模型生成一个操作或回答时，这个函数会判断它做得好不好（比如：点击的位置对不对？），并给出一个分数（Reward）。</p>
<h3>✅ Task 5: 安排“模拟考” (Rollout)</h3>
<p><strong>脚本内容：</strong> <code>actor_rollout_ref.rollout.name=sglang</code>, <code>rollout.n=8</code>, <code>rollout.temperature=1.0</code>
*   <strong>白话解释：</strong>
    *   在强化学习中，模型需要先“尝试”做题。
    *   <strong>怎么试？</strong> 针对同一个问题，让模型生成 <strong>8个</strong> 不同的回答（<code>n=8</code>）。
    *   <strong>加速器</strong>：使用了 <code>sglang</code>，这是一个非常快的推理引擎，为了让模型生成答案的速度更快。</p>
<h3>✅ Task 6: 设定学习纪律 (Actor &amp; Optimization)</h3>
<p><strong>脚本内容：</strong> <code>optim.lr=1e-6</code>, <code>ppo_micro_batch_size_per_gpu=1</code>
*   <strong>白话解释：</strong>
    *   <strong>学习率 (lr)</strong>：设得很低（1e-6），意味着“步子迈小点，慢慢改，别改废了”。
    *   <strong>显存管理</strong>：为了防止显卡内存爆炸，它开启了很多节省显存的技术（如 <code>offload=True</code>, <code>gradient_checkpointing=True</code>）。</p>
<h3>✅ Task 7: 安排后勤与监控 (Trainer)</h3>
<p><strong>脚本内容：</strong> <code>trainer.n_gpus_per_node=8</code>, <code>trainer.logger=['console','wandb']</code>, <code>total_epochs=6</code>
*   <strong>白话解释：</strong>
    *   <strong>硬件</strong>：这台机器上有 <strong>8张显卡</strong> 一起跑。
    *   <strong>工期</strong>：整个训练要把教材过 <strong>6遍</strong> (<code>total_epochs=6</code>)。
    *   <strong>监控</strong>：训练过程的数据会打印在屏幕上，并且同步上传到 <strong>WandB</strong>（一个可视化的实验看板），方便你远程查看训练曲线。</p>
<hr />
<h3>总结：这段代码到底在干嘛？</h3>
<p>这段代码在指挥 8 张显卡，加载 <strong>Qwen-3B-VL</strong> 模型，让它看图做任务（GUI操作）。</p>
<p>它采用 <strong>强化学习</strong> 的循环：
1.  <strong>出题</strong>（从数据集中拿图和指令）。
2.  <strong>做题</strong>（用 sglang 快速生成 8 种操作方案）。
3.  <strong>打分</strong>（用 reward_fn.py 判断哪个操作是对的）。
4.  <strong>复盘</strong>（根据分数更新模型参数，让它下次更倾向于做对的操作）。</p>
<p>最终目的是得到一个<strong>更会操作图形界面（GUI Agent）</strong>的 AI 模型。</p>