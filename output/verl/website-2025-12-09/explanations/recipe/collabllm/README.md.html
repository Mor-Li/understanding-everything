<h1>recipe/collabllm/README.md</h1>
<p>没问题。这份文档其实是一个<strong>技术操作手册</strong>，介绍如何使用 <code>verl</code> 这个框架来复现一篇名为 <strong>CollabLLM</strong>（ICML 2025会议论文）的算法。</p>
<p>如果不理解背景，直接看代码和命令确实会很晕。为了让你理解它的核心观点和操作流程，我为你整理了一个<strong>由浅入深的“学习与执行 To-Do List”</strong>。</p>
<p>你可以把这个过程想象成：<strong>你要训练一个 AI，让它从“只会答题的机器”变成一个“懂得和你商量着办的合作伙伴”。</strong></p>
<hr />
<h3>✅ Task 1：理解核心理念 (What &amp; Why)</h3>
<p><strong>目标：</strong> 搞懂 CollabLLM 到底是想干嘛。</p>
<ul>
<li><strong>背景：</strong> 现在的很多大模型（LLM）是“被动响应者”（Passive Responders）。你问它一个很难的问题，它往往会直接硬答，哪怕信息不全也硬猜。</li>
<li><strong>CollabLLM 的观点：</strong> 模型应该变成“主动协作者”（Active Collaborators）。在多轮对话中，如果信息不够，它应该懂得<strong>追问</strong>、或者引导用户提供更多信息，从而更精准地解决问题。</li>
<li><strong>文档中的体现：</strong> 开头第一段就说了，这是为了训练模型在“多轮对话”（multi-turn conversations）中更有效地协作。</li>
</ul>
<h3>✅ Task 2：准备“陪练”环境 (Environment)</h3>
<p><strong>目标：</strong> 搭建训练所需的软件环境。</p>
<ul>
<li><strong>你需要做什么：</strong><ol>
<li>安装 <code>verl</code>（这是训练框架）。</li>
<li>安装 <code>litellm</code> 并设置 API Key。</li>
</ol>
</li>
<li><strong>为什么要这么做（关键点）：</strong><ul>
<li>文档里提到 API 模型用于 <strong>"User Simulators"（用户模拟器）</strong>。</li>
<li><strong>观点解读：</strong> 既然要训练“多轮协作”，就需要有人跟模型聊天。在训练中找真人太慢了，所以 CollabLLM 用另一个强大的 AI（比如 GPT-4）来扮演“用户”，跟正在训练的模型进行模拟对话。</li>
</ul>
</li>
</ul>
<h3>✅ Task 3：准备教材 (Data Preparation)</h3>
<p><strong>目标：</strong> 准备好用于训练的数据集。</p>
<ul>
<li><strong>你需要做什么：</strong><ul>
<li>运行 <code>process_dataset.py</code> 脚本。</li>
<li>下载特定的数据集，比如 <code>math-hard</code>（高难度数学题）或 <code>bigcodebench</code>（代码题）。</li>
</ul>
</li>
<li><strong>观点解读：</strong><ul>
<li>普通的问答数据（一问一答）不行。</li>
<li>必须是 <strong>Multiturn（多轮）</strong> 数据集。因为协作能力只有在多轮来回拉扯的对话中才能体现出来。</li>
</ul>
</li>
</ul>
<h3>✅ Task 4：设计“奖惩机制” (The Core: Reward Design)</h3>
<p><strong>目标：</strong> 这是最核心的一步，理解如何通过强化学习（RL）告诉模型什么是“好”的表现。</p>
<p>文档中提到了三个核心奖励（Reward）权重，这代表了 CollabLLM 的<strong>价值观</strong>：</p>
<ol>
<li><strong><code>accuracy=1</code> (准确率)：</strong><ul>
<li><strong>含义：</strong> 最终问题解决了吗？</li>
<li><strong>观点：</strong> 无论怎么协作，解决问题是底线。</li>
</ul>
</li>
<li><strong><code>interactivity=1</code> (交互性)：</strong><ul>
<li><strong>含义：</strong> 模型是否进行了有效的互动（比如是否问了关键问题，是否引导了用户）？</li>
<li><strong>观点：</strong> 鼓励模型不要闭门造车，要和用户互动。</li>
</ul>
</li>
<li><strong><code>token_amount=-0.0001</code> (废话惩罚)：</strong><ul>
<li><strong>含义：</strong> 这是一个负分项。</li>
<li><strong>观点：</strong> 协作不代表啰嗦。我们希望模型高效，用最少的字把事儿办成。如果模型为了骗取“交互分”而疯狂灌水，这个惩罚项会制止它。</li>
</ul>
</li>
</ol>
<h3>✅ Task 5：开始特训 (Training Execution)</h3>
<p><strong>目标：</strong> 实际运行代码让模型进化。</p>
<ul>
<li><strong>步骤 A：SFT (监督微调) - <em>可选</em></strong><ul>
<li><code>bash train_sft_collabllm.sh</code></li>
<li><strong>解释：</strong> 先让模型看一些好的协作范例，模仿一下，打个底子。</li>
</ul>
</li>
<li><strong>步骤 B：RL (强化学习) - <em>核心</em></strong><ul>
<li><code>bash train_rl_collabllm.sh</code></li>
<li><strong>解释：</strong> 让模型进入“模拟实战”。<ul>
<li>模型和“用户模拟器”对话。</li>
<li>根据 Task 4 中的三个指标打分。</li>
<li>分数高就保留策略，分数低就改进。</li>
</ul>
</li>
<li>文档还提到你可以把 <code>accuracy</code> 换成 <code>bleu_score</code>（另一种文本相似度评分），这说明这个框架很灵活，不仅能做数学题，也能做翻译或文案任务。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结</h3>
<p>这篇文档其实就是告诉你：
<strong>“我们要用强化学习（RL）训练一个模型。为了让它学会协作，我们不仅看它答没答对（Accuracy），还看它会不会互动（Interactivity），同时还要它别讲废话（Token Penalty）。为了实现这个，我们需要用另一个AI来扮演用户跟它对练。”</strong></p>