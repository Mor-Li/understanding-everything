<h1>recipe/collabllm/metrics/pass_rate.py</h1>
<p>这个文件看起来确实有点复杂，因为它涉及到了<strong>大模型（LLM）评估</strong>的一个核心难点：<strong>如何在一个多轮对话中，提取出最终正确的代码并进行测试？</strong></p>
<p>我们可以把这个文件看作是一个<strong>“自动阅卷老师”</strong>。它的工作不是写代码，而是负责检查别的AI写好的代码能不能跑通。</p>
<p>为了让你更容易理解，我把你（作为这个阅卷程序）需要完成的任务列成了一个 <strong>Task List (待办清单)</strong>。我们一步一步来勾选这些任务，你就明白代码在干什么了。</p>
<hr />
<h3>📝 阅卷老师的 Task List (待办清单)</h3>
<h4>✅ Task 1: 面对混乱的卷面 (理解背景)</h4>
<ul>
<li><strong>情景</strong>：用户和 AI 聊了好几轮（Multi-turn）。比如用户让 AI 写个贪吃蛇，AI 写了一段，报错了；用户指出来，AI 又改了一段。</li>
<li><strong>难点</strong>：代码散落在对话的历史记录里，有时候是片段，有时候是修改版。</li>
<li><strong>代码对应</strong>：<ul>
<li>函数 <code>compute_score</code> 的输入参数 <code>messages</code> 就是这段乱乱的聊天记录。</li>
<li><code>parse_messages(messages, ...)</code> 这行代码就是在整理这些聊天记录。</li>
</ul>
</li>
</ul>
<h4>✅ Task 2: 聘请一个“助教”来整理答案 (构建 Prompt)</h4>
<ul>
<li><strong>情景</strong>：你不想自己去聊天记录里拼凑代码，太累了。你决定找另一个聪明的大模型（比如 GPT-4）当“助教”。</li>
<li><strong>任务</strong>：你需要给这个助教写一份详细的<strong>工作指南</strong>，告诉它：“把这段对话里最新、最完整、能运行的代码给我提取出来。”</li>
<li><strong>代码对应</strong>：<ul>
<li>那个超长的字符串 <code>EXTRACT_MULTITURN_COMPLETION_PROMPT</code> 就是这份<strong>工作指南</strong>。</li>
<li><strong>指南里的核心要求</strong>：<ol>
<li><strong>Identify (找最新)</strong>：看完整的对话，找最后修改过的版本。</li>
<li><strong>Integrate (整合)</strong>：如果 AI 前面写了函数头，后面改了函数体，你要把它们拼成一个完整的整体。</li>
<li><strong>Completeness (完整性)</strong>：提取出的代码必须包含 import 等所有能跑的要素，但不要包含测试用例（<code>if __name__ == "__main__":</code> 之后的东西不要）。</li>
</ol>
</li>
<li><strong>输出格式</strong>：要求助教必须返回 JSON 格式，包含 <code>thought</code> (思考过程) 和 <code>final_completion</code> (最终代码)。</li>
</ul>
</li>
</ul>
<h4>✅ Task 3: 给助教派活 (调用 LLM API)</h4>
<ul>
<li><strong>情景</strong>：工作指南写好了，现在把“聊天记录”和“指南”打包发给助教（大模型 API）。</li>
<li><strong>代码对应</strong>：<ul>
<li>代码里检查了 <code>litellm</code> 或 <code>openai</code> 库。</li>
<li><code>await litellm.acompletion(...)</code> 或 <code>client.chat.completions.create(...)</code>：这就是在打电话给助教：“嘿，按我刚才给你的指南，把这段对话里的代码整理给我。”</li>
</ul>
</li>
</ul>
<h4>✅ Task 4: 验收助教的整理结果 (解析 JSON)</h4>
<ul>
<li><strong>情景</strong>：助教回复了一大段话。你需要把里面的<strong>最终代码</strong>抠出来。</li>
<li><strong>代码对应</strong>：<ul>
<li><code>full_response = extract_json(full_response)</code>：把助教的回复变成 Python 字典。</li>
<li><code>assert ...</code>：检查助教有没有按规矩办事（是不是包含了 <code>final_completion</code> 和 <code>thought</code> 这两个字段）。</li>
<li><code>final_completion = full_response.pop("final_completion")</code>：拿到了！这就是我们要测的代码。</li>
</ul>
</li>
</ul>
<h4>✅ Task 5: 上机考试 (运行测试)</h4>
<ul>
<li><strong>情景</strong>：代码提取出来了，但它对不对呢？光看没用，得跑一下。你需要把这段代码放到一个沙盒环境里，用标准的测试用例去跑。</li>
<li><strong>代码对应</strong>：<ul>
<li><code>untrusted_check(...)</code>：这是最关键的一步。它来自 <code>bigcodebench</code> 库。</li>
<li>它接收 <code>final_completion</code> (提取出的代码) 和 <code>metadata["test"]</code> (标准答案/测试用例)。</li>
<li>它会在一个受限的环境里运行代码，防止代码有毒（比如删除系统文件），并检查输出对不对。</li>
</ul>
</li>
</ul>
<h4>✅ Task 6: 打分 (返回结果)</h4>
<ul>
<li><strong>情景</strong>：测试跑完了，通过就是满分，没通过就是零分。</li>
<li><strong>代码对应</strong>：<ul>
<li><code>passed = res[0] == "pass"</code>：看测试结果是不是 "pass"。</li>
<li><code>return float(passed)</code>：如果是 True 返回 <code>1.0</code>，False 返回 <code>0.0</code>。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：这个文件讲了啥观点？</h3>
<p>这个文件的核心观点是：<strong>评估多轮对话的代码能力，不能只看最后一次回复。</strong></p>
<ol>
<li><strong>多轮对话很复杂</strong>：用户和 AI 的交互过程中，代码是动态演进的。直接截取最后一段可能不完整（比如漏了 import）。</li>
<li><strong>LLM 辅助评估 (LLM-as-a-Judge)</strong>：用程序写死规则去提取代码很难（正则很难匹配所有情况），所以<strong>用魔法打败魔法</strong>——用一个强大的 LLM (助教) 来理解上下文，提取出“应该被测试的代码”。</li>
<li><strong>沙盒测试才是硬道理</strong>：提取出来后，必须通过真实的执行（Execution-based evaluation）来判断对错，而不是仅仅靠文本相似度。</li>
</ol>
<p>希望这个 List 能帮你理解这个文件的逻辑！</p>