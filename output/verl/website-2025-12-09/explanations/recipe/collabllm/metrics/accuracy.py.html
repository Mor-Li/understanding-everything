<h1>recipe/collabllm/metrics/accuracy.py</h1>
<p>没问题，这段代码初看确实有点绕，因为它涉及到<strong>“用一个AI去给另一个AI打分”</strong>（LLM-as-a-Judge）的逻辑。</p>
<p>简单来说，这个文件的作用就是<strong>阅卷老师</strong>。它负责把考生的回答（AI的回复）和标准答案（Ground Truth）拿给一个更厉害的AI（比如GPT-4），让那个AI判断对错。</p>
<p>我们可以把这段代码的逻辑拆解成一个<strong>“阅卷任务清单 (To-Do List)”</strong>，一共分 5 步。</p>
<hr />
<h3>📋 阅卷任务 To-Do List</h3>
<h4>✅ Task 1: 制定评分标准 (编写提示词)</h4>
<p><strong>代码对应部分：</strong> <code>ACCURACY_PROMPT = '''...'''</code></p>
<ul>
<li><strong>这是在干嘛？</strong>
    这就好比给阅卷老师写一份<strong>“工作手册”</strong>。你不能直接把试卷扔给它，你得告诉它：“你是一个严谨的评估员，你的任务是看考生的回答对不对。如果是对的给1分，错的给0分，并且要用JSON格式把理由写出来。”</li>
<li><strong>关键点：</strong><ul>
<li><code>Target Question</code>: 题目是什么。</li>
<li><code>Ground Truth Answer</code>: 标准答案是什么。</li>
<li><code>chat_history</code>: 考生（AI）是怎么回答的。</li>
<li><code>Rating criteria</code>: 1代表正确，0代表错误。</li>
</ul>
</li>
</ul>
<h4>✅ Task 2: 准备阅卷工具 (加载依赖库)</h4>
<p><strong>代码对应部分：</strong> <code>compute_score</code> 函数开头的 <code>try...except</code> 块</p>
<ul>
<li><strong>这是在干嘛？</strong>
    阅卷需要用到“大脑”（大模型API）。<ul>
<li>代码首先尝试找 <code>litellm</code> 这个工具（一个能调用各种大模型的万能工具）。</li>
<li>如果找不到，就退而求其次，使用默认的 <code>openai</code> 工具。</li>
</ul>
</li>
<li><strong>通俗理解：</strong> 看看手头有没有“万能计算器”，没有的话就拿那个“普通计算器”来凑合用。</li>
</ul>
<h4>✅ Task 3: 整理试卷 (填空与格式化)</h4>
<p><strong>代码对应部分：</strong></p>
<div class="codehilite"><pre><span></span><code><span class="n">chat_history</span> <span class="o">=</span> <span class="n">parse_messages</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">ACCURACY_PROMPT</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</code></pre></div>

<ul>
<li><strong>这是在干嘛？</strong>
    刚才 Task 1 里的“工作手册”里有很多空（<code>{}</code>）需要填。这一步就是把真实的<strong>题目</strong>、<strong>标准答案</strong>和<strong>考生的回答</strong>填进那个模板里，生成一份完整的指令。</li>
<li><strong>流程：</strong><ol>
<li>把聊天记录整理干净 (<code>parse_messages</code>)。</li>
<li>把题目 (<code>single_turn_prompt</code>)、标准答案 (<code>ground_truth</code>) 和聊天记录塞进 <code>ACCURACY_PROMPT</code>。</li>
</ol>
</li>
</ul>
<h4>✅ Task 4: 提交给阅卷老师 (调用 AI 模型)</h4>
<p><strong>代码对应部分：</strong> <code>if use_litellm: ... else: ...</code></p>
<ul>
<li><strong>这是在干嘛？</strong>
    这是最核心的一步。代码把刚才整理好的那一大段指令（Prompt），发送给一个高级的大模型（比如 GPT-4）。</li>
<li><strong>动作：</strong><ul>
<li>“嘿，GPT-4，请根据我刚才发你的这段话，判断一下这个回答的准确率。”</li>
<li>然后代码会<strong>等待</strong> (<code>await</code>) 模型返回结果。</li>
</ul>
</li>
</ul>
<h4>✅ Task 5: 登记分数 (解析结果)</h4>
<p><strong>代码对应部分：</strong></p>
<div class="codehilite"><pre><span></span><code><span class="n">full_response</span> <span class="o">=</span> <span class="n">extract_json</span><span class="p">(</span><span class="n">full_response</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">full_response</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
<span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</code></pre></div>

<ul>
<li><strong>这是在干嘛？</strong>
    阅卷老师（大模型）返回的话可能很多，比如：“经过我的深思熟虑，我觉得他是对的... JSON: {"accuracy": 1, "thought": "..."}”。
    我们需要从这一大段话里，把那个 <strong>JSON 对象</strong> 抠出来，再把里面的 <strong>分数 (0 或 1)</strong> 拿出来。</li>
<li><strong>最后结果：</strong> 返回一个数字（0.0 或 1.0），这就完成了这次评估。</li>
</ul>
<hr />
<h3>总结</h3>
<p>这段代码虽然看起来复杂，其实只做了一件事：</p>
<p><strong>它把“题目+标准答案+AI的回答”打包成一段话，发给一个裁判AI，问裁判：“这算对(1)还是错(0)？”，然后把裁判给的分数拿回来。</strong></p>