<h1>recipe/prime/run_prime_qwen.sh</h1>
<p>这份文件其实是一个<strong>“训练指挥书”</strong>。</p>
<p>把它想象成你在给一个 AI（学生）安排一整套<strong>数学特训计划</strong>。这个脚本就是告诉电脑：用什么教材、请哪个老师、怎么考试、考场怎么布置。</p>
<p>为了让你看懂，我把这个脚本拆解成一个 <strong>Task To-Do List（任务清单）</strong>，我们一步步来完成这个特训设置：</p>
<hr />
<h3>✅ Task 1: 准备教材 (Data Preparation)</h3>
<p><strong>目标</strong>：告诉 AI 去哪里找数学题来做。</p>
<ul>
<li><strong>脚本对应</strong>：
    <code>bash
    gsm8k_train_path=$HOME/data/gsm8k/train.parquet
    ...
    train_files="['$gsm8k_train_path', '$math_train_path']"</code></li>
<li><strong>解读</strong>：
    这里指定了两个著名的数学数据集：<strong>GSM8K</strong>（小学数学）和 <strong>MATH</strong>（难度较高的竞赛数学）。<ul>
<li><strong>观点</strong>：想要训练逻辑推理能力，必须用高质量的数学题作为“教材”。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 2: 选定学生/底座模型 (Model Selection)</h3>
<p><strong>目标</strong>：确定我们要训练哪个 AI 模型。</p>
<ul>
<li><strong>脚本对应</strong>：
    <code>bash
    model_path=PRIME-RL/Eurus-2-7B-SFT</code></li>
<li><strong>解读</strong>：
    我们要训练的模型叫 <code>Eurus-2-7B-SFT</code>。<ul>
<li><strong>观点</strong>：这是一个已经经过初步教学（SFT，监督微调）的模型，现在我们要通过强化学习（RL）让它更进一步。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 3: 制定刷题策略 —— “只做适合难度的题” (Data Filtering)</h3>
<p><strong>目标</strong>：这是文件中<strong>最有趣</strong>的一个观点。不要什么题都做，要挑题。</p>
<ul>
<li><strong>脚本对应</strong>：
    <code>bash
    data.filter_accuracy=True \
    data.accuracy_lower_bound=0.2 \
    data.accuracy_upper_bound=0.8 \</code></li>
<li><strong>解读</strong>：<ul>
<li><code>lower_bound=0.2</code>：如果这道题模型以前做10次只对不到2次（太难了），<strong>放弃，不学了</strong>，学了也白学。</li>
<li><code>upper_bound=0.8</code>：如果这道题模型以前做10次能对8次以上（太简单），<strong>跳过，不练了</strong>，浪费时间。</li>
<li><strong>核心观点</strong>：<strong>主要训练那些处于“学习区”的题目</strong>（即模型努努力能做对，但还没完全掌握的题）。这能极大提高训练效率。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 4: 安排模拟考试 (Actor &amp; Rollout)</h3>
<p><strong>目标</strong>：让模型针对每道题尝试写出答案，并生成多个版本。</p>
<ul>
<li><strong>脚本对应</strong>：
    <code>bash
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.n=4 \</code></li>
<li><strong>解读</strong>：<ul>
<li><code>rollout.n=4</code>：对于每一道数学题，让模型生成 <strong>4 个不同的解题过程</strong>。</li>
<li><code>vllm</code>：使用 vLLM 这个工具来加速生成（因为它生成速度非常快）。</li>
<li><strong>观点</strong>：强化学习需要“探索”。让模型多尝试几种解法，然后我们告诉它哪种好，哪种坏。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 5: 批改与奖惩机制 (Algorithm &amp; Reward)</h3>
<p><strong>目标</strong>：如何根据那 4 个答案来更新模型的大脑。</p>
<ul>
<li><strong>脚本对应</strong>：
    <code>bash
    algorithm.adv_estimator=rloo \
    algorithm.use_kl_in_reward=True \
    ...
    reward_model.model.path=$model_path \</code></li>
<li><strong>解读</strong>：<ul>
<li><code>rloo</code>：这是一种具体的算法（REINFORCE Leave-One-Out）。简单说，就是把这 4 个答案互相比较，好的给奖励，差的给惩罚。</li>
<li><code>reward_model</code>：这里似乎直接复用了原来的模型作为裁判（或者加载特定的奖励模型），来判断答案对不对。</li>
<li><strong>观点</strong>：通过对比自己生成的不同答案（RLOO算法），模型能学会哪种推理路径更容易得出正确结果。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 6: 考场硬件配置 (Trainer &amp; Hardware)</h3>
<p><strong>目标</strong>：确保电脑跑得动，不爆显存。</p>
<ul>
<li><strong>脚本对应</strong>：
    <code>bash
    trainer.n_gpus_per_node=8 \
    actor_rollout_ref.actor.fsdp_config.param_offload=True \</code></li>
<li><strong>解读</strong>：<ul>
<li>使用 <strong>8张显卡</strong> 并行训练。</li>
<li>开启 <code>fsdp</code> 和 <code>offload</code>：这是一种显存优化技术。如果显卡存不下，就把一部分数据暂时存到内存里。</li>
<li><code>data.train_batch_size=64</code>：一次打包训练64道题。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：这个脚本到底在干嘛？</h3>
<p>如果用一句话概括：
<strong>这个脚本启动了一个强化学习训练，让 <code>Eurus-7B</code> 模型在 <code>GSM8K</code> 和 <code>MATH</code> 数据集上刷题。它专门挑那些“难度适中”的题目，每题尝试 4 种解法，利用 RLOO 算法进行自我博弈和优化，最终目的是提高做数学题的准确率。</strong></p>