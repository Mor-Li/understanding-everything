<h1>recipe/prime/run_prime_qwen_code.sh</h1>
<p>这份脚本文件确实包含了很多技术细节，如果你不熟悉大模型训练（特别是强化学习 RL），看起来会像天书一样。</p>
<p>为了让你看懂，我们可以把这个脚本想象成<strong>“给一个学生（AI模型）制定的一份期末突击复习计划”</strong>。</p>
<p>我把这份代码拆解成一个 <strong>Task To-Do List（任务清单）</strong>，带你一步步看懂它在安排什么。</p>
<hr />
<h3>核心任务：训练一个能写代码的 AI</h3>
<p>这个脚本的目的是启动一个程序，用一种叫 <strong>PRIME</strong> 的强化学习方法，来训练一个叫 <strong>Eurus-2-7B</strong> 的模型，让它写代码的能力更强。</p>
<p>下面是具体的执行步骤清单：</p>
<hr />
<h3>Task 1: 准备教材 (数据准备)</h3>
<p><strong>代码位置：</strong> 第 4-9 行</p>
<div class="codehilite"><pre><span></span><code><span class="nv">code_train_path</span><span class="o">=</span><span class="nv">$HOME</span>/data/code/train.parquet
...
<span class="nv">train_files</span><span class="o">=</span><span class="s2">&quot;[&#39;</span><span class="nv">$code_train_path</span><span class="s2">&#39;]&quot;</span>
</code></pre></div>

<ul>
<li><strong>解读：</strong><ul>
<li>这里定义了“教科书”在哪里。</li>
<li><code>train.parquet</code> 是<strong>练习题集</strong>（训练数据）。</li>
<li><code>test.parquet</code> 是<strong>模拟考试题</strong>（测试数据）。</li>
<li><strong>通俗理解：</strong> 告诉程序：“去书架上把《五年模拟三年高考》拿来。”</li>
</ul>
</li>
</ul>
<h3>Task 2: 选定学生 (指定模型)</h3>
<p><strong>代码位置：</strong> 第 11 行</p>
<div class="codehilite"><pre><span></span><code><span class="nv">model_path</span><span class="o">=</span>PRIME-RL/Eurus-2-7B-SFT
</code></pre></div>

<ul>
<li><strong>解读：</strong><ul>
<li>指定了我们要训练哪个模型。这里选的是 <code>Eurus-2-7B-SFT</code>。</li>
<li><strong>通俗理解：</strong> 告诉程序：“今天我们要辅导的学生是小明（Eurus-7B）。”</li>
</ul>
</li>
</ul>
<h3>Task 3: 制定刷题规则 (数据加载配置)</h3>
<p><strong>代码位置：</strong> 第 16-24 行 (<code>data.</code> 开头的参数)</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>data.train_batch_size<span class="o">=</span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.max_prompt_length<span class="o">=</span><span class="m">1024</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.filter_accuracy<span class="o">=</span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.accuracy_lower_bound<span class="o">=</span><span class="m">0</span>.2<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.accuracy_upper_bound<span class="o">=</span><span class="m">0</span>.8<span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>解读：</strong><ul>
<li><code>train_batch_size=64</code>：一次做 64 道题。</li>
<li><code>max_prompt_length=1024</code>：题目长度不能超过 1024 个字。</li>
<li><strong>关键点</strong> (<code>filter_accuracy</code> 等)：这是一种<strong>因材施教</strong>的策略。<ul>
<li>它过滤掉了正确率低于 20%（太难了，学不会）和高于 80%（太简单了，不用学）的数据。</li>
</ul>
</li>
<li><strong>通俗理解：</strong> “小明，太难和太简单的题咱不做了，只做那些努努力能做对的题，效率最高。”</li>
</ul>
</li>
</ul>
<h3>Task 4: 设定答题与思考模式 (Actor/Rollout 配置)</h3>
<p><strong>代码位置：</strong> 第 25-38 行 (<code>actor_rollout_ref.</code> 开头的参数)</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>actor_rollout_ref.model.path<span class="o">=</span><span class="nv">$model_path</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>actor_rollout_ref.rollout.name<span class="o">=</span>vllm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>actor_rollout_ref.rollout.n<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>解读：</strong><ul>
<li>这是强化学习的核心部分。</li>
<li><strong>Actor (演员/学生)</strong>：负责做题。</li>
<li><code>rollout.name=vllm</code>：使用 <code>vllm</code> 这个加速引擎来生成答案（为了快）。</li>
<li><code>rollout.n=4</code>：对于每道题，尝试生成 4 个不同的答案（以此来探索哪种写法更好）。</li>
<li><strong>通俗理解：</strong> “小明，每道题你试着写 4 种不同的解法，要写得快一点（用 vllm 引擎）。”</li>
</ul>
</li>
</ul>
<h3>Task 5: 设定评分标准 (算法与奖励模型)</h3>
<p><strong>代码位置：</strong> 第 39-50 行 (<code>algorithm.</code> 和 <code>reward_model.</code> 开头的参数)</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>algorithm.adv_estimator<span class="o">=</span>rloo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>reward_model.model.path<span class="o">=</span><span class="nv">$model_path</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>解读：</strong><ul>
<li><strong>Algorithm (教学法)</strong>：<code>rloo</code> 是一种具体的强化学习算法，用来计算哪个答案好。</li>
<li><strong>Reward Model (阅卷老师)</strong>：这里比较特殊，阅卷老师用的模型路径和学生模型是一样的（<code>$model_path</code>），这通常意味着模型在进行某种形式的“自我反思”或者用自身的能力来评估代码是否正确（比如能否跑通测试用例）。</li>
<li><strong>通俗理解：</strong> “写完这 4 个答案后，我们会用 RLOO 这套评分标准来打分，告诉你哪个解法能得分。”</li>
</ul>
</li>
</ul>
<h3>Task 6: 安排教室与课程表 (训练器配置)</h3>
<p><strong>代码位置：</strong> 第 51-60 行 (<code>trainer.</code> 开头的参数)</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>trainer.project_name<span class="o">=</span><span class="s1">&#39;prime_example&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trainer.n_gpus_per_node<span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trainer.save_freq<span class="o">=</span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>解读：</strong><ul>
<li><code>n_gpus_per_node=8</code>：动用 8 张显卡来训练（这可是个大教室）。</li>
<li><code>logger='["console","wandb"]'</code>：把训练日记写在控制台和 WandB（一个在线图表工具）上。</li>
<li><code>save_freq=64</code>：每走 64 步存个档，防止电脑死机白练了。</li>
<li><strong>通俗理解：</strong> “我们租了 8 个顶级家教（GPU）同时教你，每过一会就记录一下学习进度。”</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：这段代码到底在干嘛？</h3>
<p>如果用一句话概括：
<strong>这是一个启动脚本，它命令 8 张显卡，使用 PRIME 算法，让 Eurus-7B 模型通过反复做“中等难度”的代码题（每题尝试 4 种解法），来提升写代码的能力。</strong></p>
<p>现在你再回头看代码，是不是大概能对应上了？
1.  <strong>前面几行</strong>：定义文件在哪里。
2.  <strong><code>python ...</code></strong>：开始上课。
3.  <strong><code>data...</code></strong>：选什么题。
4.  <strong><code>actor...</code></strong>：怎么答题。
5.  <strong><code>algorithm...</code></strong>：怎么打分。
6.  <strong><code>trainer...</code></strong>：怎么保存进度。</p>