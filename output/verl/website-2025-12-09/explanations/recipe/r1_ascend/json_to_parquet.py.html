<h1>recipe/r1_ascend/json_to_parquet.py</h1>
<p>这份代码其实就是一个<strong>“数据搬运工 + 翻译官”</strong>。</p>
<p>它的核心任务是：把一种原始格式（JSON）的数据，经过清洗和包装，转换成大模型训练框架（比如 VerL）能读懂的高效格式（Parquet）。</p>
<p>为了让你彻底看懂，我把这个脚本的工作流程拆解成了一个<strong>“流水线工人的待办清单（To-Do List）”</strong>。我们一步步来看：</p>
<hr />
<h3>📋 任务清单：数据加工流水线</h3>
<h4>✅ Task 1: 接收工单 (解析参数)</h4>
<p><strong>代码位置：</strong> <code>parse_arguments()</code>
*   <strong>他在做什么：</strong> 脚本启动时，先问你几个问题：
    *   原始数据在哪里？(<code>--json_path</code>)
    *   加工好的数据放哪里？(<code>--output_dir</code>)
    *   多少数据用来训练，多少用来考试？(<code>--train_data_ratio</code>，默认 90% 训练)
    *   随机种子是多少？(为了结果可复现)</p>
<h4>✅ Task 2: 安检 (校验参数)</h4>
<p><strong>代码位置：</strong> <code>validate_arguments()</code>
*   <strong>他在做什么：</strong> 确保你给的路径是存在的，确保切分比例是合理的（0到1之间）。如果文件不存在，直接报错罢工。</p>
<h4>✅ Task 3: 进货 (读取数据)</h4>
<p><strong>代码位置：</strong> <code>convert_json_to_parquet</code> -&gt; <code>json.load</code>
*   <strong>他在做什么：</strong> 打开那个 <code>deepscaler.json</code> 文件，把里面的原始题目全部读到内存里。
*   <strong>原始长相：</strong> 里面可能只有简单的 <code>problem</code>（题目）、<code>answer</code>（答案）、<code>solution</code>（解析）。</p>
<h4>✅ Task 4: 精加工 &amp; 包装 (核心逻辑！此处包含重要观点)</h4>
<p><strong>代码位置：</strong> <code>for item in original_data:</code> 循环内部
*   <strong>他在做什么：</strong> 这是全篇最重要的地方。他把每一道题重新包装成了一个复杂的字典。
*   <strong>其中的重要观点（R1 模板）：</strong>
    *   你会看到代码里定义了一长串 <code>r1_template</code>。
    *   <strong>观点：</strong> 这是一个<strong>System Prompt（系统提示词）</strong>。它强制要求 AI 模型在回答问题前，必须先进行“思考”。
    *   <strong>具体指令：</strong> 它告诉 AI：“你是助手，你要先在 <code>&lt;think&gt;</code> 标签里写下推理过程，然后在 <code>&lt;answer&gt;</code> 标签里写答案，最后把结果放在 <code>\\boxed{}</code> 里。”
    *   <strong>目的：</strong> 这是为了训练像 DeepSeek-R1 那样的<strong>推理模型</strong>。如果没有这个模板，模型可能直接蹦出答案，而不会产生高质量的思维链（CoT）。</p>
<ul>
<li><strong>数据重组：</strong><ul>
<li><code>prompt</code>: 把“系统指令”和“用户题目”打包在一起。</li>
<li><code>reward_model</code>: 把“标准答案”存好，这是为了后面做强化学习（RL）时，用来给模型打分的（答对了给糖吃，答错了惩罚）。</li>
</ul>
</li>
</ul>
<h4>✅ Task 5: 分堆 (划分训练集/测试集)</h4>
<p><strong>代码位置：</strong> <code>split_index = ...</code>
*   <strong>他在做什么：</strong> 假设你有 100 道题，比例是 0.9。
    *   他拿把刀切一下，前 90 道给 <code>train</code>（训练用），后 10 道给 <code>test</code>（测试用）。
    *   给它们分别打上标签 <code>item["split"] = "train"</code> 或 <code>"test"</code>。</p>
<h4>✅ Task 6: 压箱装车 (保存为 Parquet)</h4>
<p><strong>代码位置：</strong> <code>pd.DataFrame</code>, <code>to_parquet</code>
*   <strong>他在做什么：</strong>
    *   把处理好的列表转换成表格形式（Pandas DataFrame）。
    *   把表格保存成 <code>.parquet</code> 文件。
*   <strong>观点：为什么要转成 Parquet？</strong>
    *   JSON 是文本，体积大，读取慢。
    *   Parquet 是二进制列式存储，体积极其小，读取速度极快。对于动辄几 GB、几 TB 的 AI 训练数据，必须用这种高效格式。</p>
<hr />
<h3>💡 总结一下文中的核心观点</h3>
<p>如果把这个脚本看作一个人，他在对你说：</p>
<ol>
<li><strong>格式即规矩：</strong> 原始数据太乱，我要把它统一成 <code>prompt</code>（提示词）+ <code>reward_model</code>（奖励标准）的标准格式，方便后续的强化学习算法直接调用。</li>
<li><strong>思维链至上：</strong> 我必须在每一条数据里强行插入一段 <code>r1_template</code>，<strong>强迫</strong>模型学会使用 <code>&lt;think&gt;</code> 标签。这是训练推理能力的关键。</li>
<li><strong>效率优先：</strong> 不要给我塞 JSON 文件训练，太慢了，全部给我转成 Parquet 格式再上机。</li>
</ol>
<p><strong>一句话概括：</strong>
这是一个<strong>数据预处理脚本</strong>，它把普通的数学题库，转换成了<strong>专门用于训练具备“思考能力”的 AI 模型</strong>（类似 DeepSeek R1）的高效二进制数据格式。</p>