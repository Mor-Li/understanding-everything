<h1>recipe/one_step_off_policy/shell/dapo_7b_math_fsdp2_64_64.sh</h1>
<p>这份脚本实际上是一个 <strong>“指挥官指令清单”</strong>。它的作用是指挥 <strong>64张显卡</strong>（8台机器，每台8张卡）协同工作，去训练一个擅长做数学题的 AI 模型。</p>
<p>简单来说，这是一个 <strong>强化学习（RL）的启动脚本</strong>，专门用来训练像 Qwen2.5-Math 这样的模型，让它通过“做题-评分-改进”的过程变得更聪明。</p>
<p>为了让你更容易理解，我把它拆解成一个 <strong>“项目经理的 To-Do List”</strong>，一步步带你看它在安排什么任务：</p>
<hr />
<h3>✅ Task 1: 基础设施准备 (你是谁？东西在哪？)</h3>
<p>脚本的前几行是在定义身份和路径，就像做饭前要把食材摆好。</p>
<ul>
<li><strong>项目代号</strong>: <code>project_name='DAPO'</code>。这是这个训练项目的名字。</li>
<li><strong>实验名称</strong>: <code>dapo_qwen2-7B-math...</code>。这次具体实验的标签，方便以后查日志。</li>
<li><strong>定位模型</strong>: <code>MODEL_PATH</code> 指向了 <code>Qwen2.5-Math-7B</code>。这就是我们要训练的“大脑”。</li>
<li><strong>定位教材</strong>:<ul>
<li><code>TRAIN_FILE</code>: 训练数据是 <code>dapo-math-17k</code>（1.7万道数学题）。</li>
<li><code>TEST_FILE</code>: 考试题目是 <code>aime-2024</code>（美国数学邀请赛题目，很难）。</li>
</ul>
</li>
</ul>
<h3>✅ Task 2: 制定学习策略 (怎么学？)</h3>
<p>这里定义了 AI “刷题”的方法论。</p>
<ul>
<li><strong>核心算法</strong>: <code>adv_estimator=grpo</code>。<ul>
<li><strong>翻译</strong>: 它没有用传统的 PPO，而是用了 <strong>GRPO</strong> (Group Relative Policy Optimization)。这是最近 DeepSeek-R1 等推理模型常用的方法，通过对比一组答案的优劣来学习，而不是单纯靠一个评分模型。</li>
</ul>
</li>
<li><strong>奖惩机制</strong>:<ul>
<li><code>use_kl_in_reward=False</code>: 不在奖励里直接加 KL 散度惩罚（通常为了让模型放飞自我，探索更多解题路径）。</li>
<li><code>clip_ratio</code>: 限制每次更新幅度，防止模型“步子迈太大扯着蛋”。</li>
</ul>
</li>
</ul>
<h3>✅ Task 3: 设定思考长度 (允许想多久？)</h3>
<p>这是针对 <strong>“推理模型” (Reasoning Model)</strong> 的关键设置。</p>
<ul>
<li><strong>输入限制</strong>: <code>max_prompt_length=2048</code>。题目长度限制。</li>
<li><strong>输出限制</strong>: <code>max_response_length=28672</code> (1024 * 28)。<ul>
<li><strong>重点</strong>: 这个数字非常大！通常模型输出只有 4k 或 8k。这里允许 AI 生成 <strong>2.8万</strong> 个 token 的回答。这意味着它鼓励 AI 进行极长的 <strong>“思维链” (Chain of Thought)</strong> 推理，把解题步骤写得非常详细。</li>
</ul>
</li>
</ul>
<h3>✅ Task 4: 硬件资源调度 (怎么塞进显存？)</h3>
<p>这是最硬核的部分。因为要处理超长的上下文（28k token），显存压力巨大，必须把模型切碎了放在不同的显卡上。</p>
<ul>
<li><strong>算力规模</strong>:<ul>
<li><code>NNODES=8</code>, <code>NGPUS_PER_NODE=8</code>：一共调用 <strong>64张 GPU</strong>。</li>
</ul>
</li>
<li><strong>并行策略 (为了不爆显存)</strong>:<ul>
<li><code>fsdp_size=8</code>: 使用 FSDP2 (Fully Sharded Data Parallel) 将模型参数切分到8张卡上。</li>
<li><code>gen_tp=4</code>: 生成时的张量并行 (Tensor Parallelism)。</li>
<li><code>sp_size=4</code>: 序列并行 (Sequence Parallelism)，这是为了处理那个 28k 超长文本的关键技术，把长文本切段处理。</li>
</ul>
</li>
<li><strong>推理引擎</strong>: <code>actor_rollout_ref.rollout.name=vllm</code>。<ul>
<li><strong>翻译</strong>: 使用 <strong>vLLM</strong> 这个库来加速 AI 做题（生成答案）的过程，比传统的 HuggingFace 生成要快得多。</li>
</ul>
</li>
</ul>
<h3>✅ Task 5: 启动训练引擎 (执行命令)</h3>
<p>脚本的最后一大段 <code>python -m recipe.one_step_off_policy.main_ppo ...</code> 才是真正的执行动作。</p>
<p>它把上面定义的所有变量（路径、算法、长度、硬件设置）打包通过命令行传给 Python 程序。</p>
<ul>
<li><strong>关键参数解读</strong>:<ul>
<li><code>actor_rollout_ref.rollout.n=16</code>: 对于每一道数学题，让 AI 尝试生成 <strong>16个</strong> 不同的解题过程。</li>
<li><code>trainer.total_epochs=10</code>: 这本教材一共学10遍。</li>
<li><code>trainer.val_before_train=True</code>: 还没开始练之前，先考一次试（摸底测试）。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：这个脚本在干嘛？</h3>
<p><strong>一句话解释：</strong>
这是一个<strong>土豪级</strong>的训练脚本，它动用了 <strong>64张显卡</strong>，使用最新的 <strong>GRPO 算法</strong>，配合 <strong>vLLM 加速</strong>，去训练一个 <strong>7B大小的数学模型</strong>。它特意允许模型进行 <strong>超长思维链推理 (28k token)</strong>，目的是让模型学会像人类数学家一样，通过写出极其详细的步骤来解决复杂的数学竞赛题。</p>
<p><strong>你的 Todo List (如果你要运行它):</strong>
1.  <strong>检查钱/卡</strong>: 确认你有 64 张显卡的集群权限（或者修改脚本里的 <code>NNODES</code> 减少卡数）。
2.  <strong>下载模型</strong>: 确保 <code>Qwen2.5-Math-7B</code> 已经下载好。
3.  <strong>准备数据</strong>: 确保 parquet 格式的数学题目文件在指定路径。
4.  <strong>修改配置</strong>: 必须按脚本注释提示，去修改模型 <code>config.json</code> 里的 <code>max_position_embeddings</code> 为 32768，否则处理不了长文本会报错。
5.  <strong>运行</strong>: <code>bash dapo_7b_math_fsdp2_64_64.sh</code>。</p>