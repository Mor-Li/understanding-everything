<h1>recipe/fapo/run_baseline_7b.sh</h1>
<p>这个脚本确实看起来密密麻麻的，全是参数。别担心，我们把它想象成<strong>“启动一个大型机器（训练AI模型）的操作手册”</strong>。</p>
<p>这个脚本的核心目的是：<strong>使用 GRPO 算法（一种强化学习算法）来训练一个 7B 大小的数学模型（Qwen2.5-Math-7B），让它做题做得更好。</strong></p>
<p>为了让你听懂，我把这个脚本拆解成一个<strong>“任务清单 (Task To-Do List)”</strong>，按逻辑顺序一步步告诉你它在干什么。</p>
<hr />
<h3>任务清单 (Task To-Do List)</h3>
<p>要把这个训练任务跑起来，脚本里其实就在做这 5 件事：</p>
<ol>
<li><strong>【定调子】</strong>：给这次训练起名字，选定核心算法（GRPO）。</li>
<li><strong>【定规矩】</strong>：设定模型“学习”时的奖惩力度、输入输出的长度限制。</li>
<li><strong>【找东西】</strong>：确认模型在哪里、训练数据在哪里。</li>
<li><strong>【搭环境】</strong>：配置分布式计算环境（Ray）和硬件优化参数（为了省显存）。</li>
<li><strong>【按按钮】</strong>：把上面所有配置打包，传给训练程序 (<code>verl.trainer.main_ppo</code>) 开始干活。</li>
</ol>
<hr />
<h3>逐步详解 (Step-by-Step Explanation)</h3>
<p>下面我按照文件内容的顺序，把上面 5 个任务对应到代码里给你讲。</p>
<h4>第一步：【定调子】基本信息与算法</h4>
<p>代码开头部分：</p>
<div class="codehilite"><pre><span></span><code><span class="nv">project_name</span><span class="o">=</span><span class="s1">&#39;FAPO-Reproduce&#39;</span><span class="w">   </span><span class="c1"># 项目叫 FAPO复现</span>
<span class="nv">exp_name</span><span class="o">=</span><span class="s1">&#39;Baseline-7B&#39;</span><span class="w">          </span><span class="c1"># 实验叫 Baseline-7B</span>
<span class="nv">adv_estimator</span><span class="o">=</span>grpo<span class="w">              </span><span class="c1"># 【重点】核心算法是用 GRPO，而不是普通的 PPO</span>
</code></pre></div>

<ul>
<li><strong>解读</strong>：这里决定了我们在做什么。GRPO (Group Relative Policy Optimization) 是最近（特别是 DeepSeek-R1 之后）非常火的一种强化学习方法，专门用来让模型通过“自我思考”提升推理能力。</li>
</ul>
<h4>第二步：【定规矩】超参数与长度</h4>
<div class="codehilite"><pre><span></span><code><span class="nv">use_kl_in_reward</span><span class="o">=</span>False<span class="w">          </span><span class="c1"># 不把 KL 散度放进奖励里</span>
<span class="nv">kl_coef</span><span class="o">=</span><span class="m">0</span>.0<span class="w">                     </span><span class="c1"># KL 系数为 0（意味着鼓励模型大胆探索，不受原始模型束缚太紧）</span>

<span class="nv">max_prompt_length</span><span class="o">=</span><span class="k">$((</span><span class="m">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">2</span><span class="k">))</span><span class="w"> </span><span class="c1"># 题目最长 2048 token</span>
<span class="nv">max_response_length</span><span class="o">=</span><span class="k">$((</span><span class="m">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">8</span><span class="k">))</span><span class="w"> </span><span class="c1"># 回答最长 8192 token（数学推理需要很长的步骤）</span>
</code></pre></div>

<ul>
<li><strong>解读</strong>：这里设定了模型“答题”的限制。特别是 <code>max_response_length</code> 设置得很大（8k），说明这是为了训练<strong>长思维链（Chain of Thought）</strong>，让模型一步步写出解题过程。</li>
</ul>
<h4>第三步：【找东西】路径与资源</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># Ray 相关的设置（分布式计算框架）</span>
<span class="nv">RAY_ADDRESS</span><span class="o">=</span><span class="si">${</span><span class="nv">RAY_ADDRESS</span><span class="k">:-</span><span class="s2">&quot;http://localhost:8265&quot;</span><span class="si">}</span>

<span class="c1"># 模型和数据路径</span>
<span class="nv">MODEL_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">MODEL_PATH</span><span class="k">:-</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">RAY_DATA_HOME</span><span class="si">}</span><span class="s2">/models/Qwen2.5-Math-7B&quot;</span><span class="si">}</span><span class="w"> </span><span class="c1"># 基础模型是 Qwen 数学版</span>
<span class="nv">TRAIN_FILE</span><span class="o">=</span>...<span class="w"> </span><span class="c1"># 训练数据 parquet 文件</span>
<span class="nv">TEST_FILE</span><span class="o">=</span>...<span class="w">  </span><span class="c1"># 测试数据 parquet 文件</span>
</code></pre></div>

<ul>
<li><strong>解读</strong>：告诉机器去哪里搬运“原材料”。</li>
<li><strong>注意</strong>：脚本里有一句注释 <code>very important! please modify the max_position_embeddings...</code>，这是提醒你下载模型后要手动改一下配置文件，否则处理不了那么长的数学题。</li>
</ul>
<h4>第四步：【搭环境】硬件与性能优化</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># 性能相关参数</span>
<span class="nv">sp_size</span><span class="o">=</span><span class="m">1</span><span class="w">               </span><span class="c1"># 序列并行大小</span>
<span class="nv">use_dynamic_bsz</span><span class="o">=</span>True<span class="w">    </span><span class="c1"># 动态批大小（为了提高显卡利用率）</span>
<span class="nv">offload</span><span class="o">=</span>True<span class="w">            </span><span class="c1"># 【省钱技巧】把部分参数卸载到 CPU 内存，防止显存爆了</span>
actor_rollout_ref.rollout.name<span class="o">=</span>vllm<span class="w"> </span><span class="c1"># 使用 vLLM 引擎来加速生成（推理）</span>
</code></pre></div>

<ul>
<li><strong>解读</strong>：这里全是关于“怎么跑得快”和“怎么不爆显存”的设置。<ul>
<li><strong>vLLM</strong>：是一个非常快的推理库，训练过程中模型需要自己先做题（Rollout），用 vLLM 做题速度会快很多。</li>
<li><strong>Offload</strong>：因为这是强化学习，需要同时加载好几个模型（Actor, Ref, Reward），显存压力极大，所以开启卸载功能。</li>
</ul>
</li>
</ul>
<h4>第五步：【按按钮】提交任务</h4>
<p>这部分是脚本里最长的那一大段 <code>ray job submit ...</code>。它其实就是把上面定义的所有变量，翻译成 Python 程序能看懂的参数，传给 <code>verl.trainer.main_ppo</code>。</p>
<p>我挑几个关键的参数给你解释：</p>
<ol>
<li>
<p><strong>Actor/Rollout/Ref 结构</strong>：</p>
<ul>
<li>强化学习通常有三个角色：<ul>
<li><strong>Actor (演员)</strong>：正在学习的学生模型。</li>
<li><strong>Ref (参考)</strong>：老师模型（通常是没训练前的旧模型），用来防止学生走火入魔（KL 散度计算）。</li>
<li><strong>Rollout (推演)</strong>：负责让学生做题，生成答案。</li>
</ul>
</li>
<li>脚本里大量的 <code>actor_rollout_ref...</code> 就是在配置这三个角色的参数（比如学习率 <code>lr=1e-6</code>，生成时的温度 <code>temperature=1.0</code>）。</li>
</ul>
</li>
<li>
<p><strong>奖励机制 (Reward)</strong>：
    <code>bash
    reward_model.reward_manager=dapo
    custom_reward_function.path=recipe/fapo/reward_fn_reasoning.py</code></p>
<ul>
<li><strong>解读</strong>：这是强化学习的灵魂。模型做完题，怎么判断好坏？这里指定了一个 Python 文件 <code>reward_fn_reasoning.py</code>。对于数学题，通常是<strong>“答案对不对”</strong>（Answer Correctness）以及<strong>“格式对不对”</strong>。</li>
</ul>
</li>
<li>
<p><strong>训练进度</strong>：
    <code>bash
    trainer.n_gpus_per_node=8   # 用 8 张卡
    trainer.total_epochs=10     # 训练 10 轮
    trainer.total_training_steps=200 # 总共走 200 步更新</code></p>
</li>
</ol>
<hr />
<h3>总结：这个脚本讲了个什么故事？</h3>
<p>如果把这个脚本看作一个<strong>“补习班计划书”</strong>：</p>
<ol>
<li><strong>招生对象</strong>：Qwen2.5-Math-7B（一个数学还不错的底模）。</li>
<li><strong>教学目标</strong>：通过大量做题，学会更复杂的推理（Baseline-7B 实验）。</li>
<li><strong>教学方法</strong>：GRPO（一种让学生分组做题，对比组内好坏来学习的方法）。</li>
<li><strong>教室设施</strong>：8张 GPU，使用 vLLM 这种“高速答题纸”，并且开启了“显存节省模式”。</li>
<li><strong>考试规则</strong>：题目最长 2k 字，答案允许写 8k 字。做完题后，由 <code>reward_fn_reasoning.py</code> 这位阅卷老师打分，分数高的思路会被模型记住。</li>
</ol>
<p><strong>你需要做的操作</strong>：
如果你要运行它，主要是确保 <code>MODEL_PATH</code>（模型路径）和 <code>TRAIN_FILE</code>（数据路径）是你机器上真实存在的路径，然后直接运行这个 <code>.sh</code> 脚本即可。</p>