<h1>recipe/fapo/prepare_fapo_data.py</h1>
<p>这份代码其实就像是一个<strong>“数据加工流水线”</strong>。它的核心目的是：<strong>从网上下载各种原始的数学和问答题目，把它们清洗、格式化成统一的样子，最后打包保存，准备喂给 AI 模型去训练或测试。</strong></p>
<p>为了让你听懂，我把这份代码的工作拆解成一个<strong>“5步走”的 Task To-Do List</strong>：</p>
<h3>📋 Task 1: 制定统一的“模具” (定义格式)</h3>
<p><strong>代码对应部分：</strong> <code>example_map_fn</code> 函数
*   <strong>任务说明：</strong> 不管原始数据长什么样（有的叫"Problem"，有的叫"Question"），最后都要变成同一个标准的字典格式。
*   <strong>具体步骤：</strong>
    1.  把题目变成用户提问 (<code>prompt</code>)。
    2.  把正确答案放进 <code>reward_model</code> 里（用来判断 AI 对不对）。
    3.  打上标签，比如这题是考数学 (<code>Math</code>) 还是考常识 (<code>General</code>)。
    4.  最后打包成一个标准的 JSON/字典结构。</p>
<hr />
<h3>📋 Task 2: 采购并处理“原材料” (加载数据集)</h3>
<p>代码里定义了四个函数来分别处理四种不同的数据源，就像处理四种不同的食材：</p>
<h4>✅ Sub-task 2.1: 处理 AIME 2024 数学竞赛题</h4>
<p><strong>代码对应部分：</strong> <code>build_aime2024_dataset</code>
*   <strong>动作：</strong> 从 HuggingFace 下载 <code>Maxwell-Jia/AIME_2024</code>。
*   <strong>加工：</strong> 在题目后面加上一句话：“请一步步推理，并把最终答案放在 <code>\boxed{}</code> 里”。(这是为了让 AI 学会规范答题格式)。
*   <strong>归类：</strong> 标记为“测试集” (<code>split="test"</code>)。</p>
<h4>✅ Sub-task 2.2: 处理 AIME 2025 数学竞赛题</h4>
<p><strong>代码对应部分：</strong> <code>build_aime2025_dataset</code>
*   <strong>动作：</strong> 下载 <code>yentinglin/aime_2025</code>。
*   <strong>加工：</strong> 同样加上“请一步步推理...”的指令。
*   <strong>归类：</strong> 标记为“测试集”。</p>
<h4>✅ Sub-task 2.3: 处理 GPQA 钻石级难题 (科学/常识)</h4>
<p><strong>代码对应部分：</strong> <code>build_gpqa_diamond_dataset</code>
*   <strong>动作：</strong> 下载 <code>Idavidrein/gpqa</code>。
*   <strong>特殊加工（重点）：</strong>
    *   这是一个选择题数据集。代码会把 1 个正确选项和 3 个错误选项<strong>随机打乱顺序</strong>（ABCD）。
    *   防止 AI 只是死记硬背“选C”，而是真正去理解内容。
    *   最后要求 AI 输出选项字母。
*   <strong>归类：</strong> 标记为“测试集”。</p>
<h4>✅ Sub-task 2.4: 处理 DAPO 数学训练题</h4>
<p><strong>代码对应部分：</strong> <code>build_dapo_train_dataset</code>
*   <strong>动作：</strong> 下载 <code>open-r1/DAPO-Math-17k-Processed</code>。
*   <strong>加工：</strong> 标准化题目和答案。
*   <strong>归类：</strong> 标记为“训练集” (<code>split="train"</code>)。这是真正用来教 AI 学习的材料。</p>
<hr />
<h3>📋 Task 3: 组装与“增量” (数据拼接)</h3>
<p><strong>代码对应部分：</strong> <code>if __name__ == "__main__":</code> 下的前半部分
*   <strong>任务说明：</strong> 把上面处理好的数据拼起来，并且进行<strong>复制粘贴</strong>（这在强化学习或推理评估中很常见）。
*   <strong>具体步骤：</strong>
    1.  <strong>训练集 (Train)：</strong> 把 DAPO 数据集复制 <strong>20份</strong> 拼在一起。（目的是让模型在训练时多见几次这些数据，增加训练量）。
    2.  <strong>测试集 (Test)：</strong>
        *   AIME 2024 复制 <strong>32份</strong>。
        *   AIME 2025 复制 <strong>32份</strong>。
        *   GPQA 复制 <strong>4份</strong>。
        *   <em>解释：为什么测试集要复制？</em> 通常是为了做 <strong>Majority Voting（多数投票）</strong> 或者 <strong>Pass@K</strong> 测试。也就是让 AI 对同一道题做 32 次，看看它能做对几次，或者取出现次数最多的答案。</p>
<hr />
<h3>📋 Task 4: 打包封箱 (保存文件)</h3>
<p><strong>代码对应部分：</strong> <code>.to_parquet(...)</code>
*   <strong>任务说明：</strong> 把内存里处理好的庞大数据，存成硬盘上的文件。
*   <strong>具体步骤：</strong>
    1.  把训练数据存为 <code>fapo-train-boxed.parquet</code>。
    2.  把测试数据存为 <code>fapo-test-full-boxed.parquet</code>。
    *   <em>注：Parquet 是一种压缩率高、读取速度快的文件格式，大数据常用。</em></p>
<hr />
<h3>📋 Task 5: 发货 (上传云端 - 可选)</h3>
<p><strong>代码对应部分：</strong> <code>if hdfs_dir is not None:</code>
*   <strong>任务说明：</strong> 如果用户指定了 HDFS（Hadoop 分布式文件系统）的地址，就把生成的文件上传上去。
*   <strong>目的：</strong> 方便在多台服务器集群上进行大规模训练。</p>
<hr />
<h3>💡 总结</h3>
<p>这脚本就是干了这么一件事：
<strong>“下载 4 种试卷 -&gt; 统一排版格式 -&gt; 疯狂复印 (训练卷印20份，测试卷印32份) -&gt; 装订成册 -&gt; 存到硬盘里。”</strong></p>