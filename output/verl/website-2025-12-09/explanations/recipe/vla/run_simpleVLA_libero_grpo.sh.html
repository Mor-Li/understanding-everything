<h1>recipe/vla/run_simpleVLA_libero_grpo.sh</h1>
<p>这份脚本实际上是一个<strong>“AI 机器人特训启动清单”</strong>。</p>
<p>简单来说，它的目的是：<strong>加载一个已经懂点基础的机器人模型（OpenVLA），把它扔进一个仿真环境（Isaac Sim/Libero）里，通过强化学习（具体是 GRPO/Reinforce++ 算法）让它通过不断的尝试和反馈，学会更好地完成任务。</strong></p>
<p>为了让你看懂，我把这份脚本转化成一个<strong>项目经理给电脑下达的 Task List (待办清单)</strong>，我们一步步来看：</p>
<hr />
<h3>📋 Task Phase 1: 准备后勤 (配置路径与环境)</h3>
<p>在真正干活之前，脚本先要把文件放在哪里、用什么工具都交代清楚。</p>
<ul>
<li><strong>[TODO 1] 确认教材（数据）在哪里：</strong><ul>
<li>脚本定义了 <code>train_files</code> 和 <code>test_files</code>。</li>
<li><em>含义：</em> 告诉电脑去 <code>$HOME/data/libero_rl/</code> 目录下找训练集和测试集（parquet 格式的文件）。</li>
</ul>
</li>
<li><strong>[TODO 2] 确认办公桌（输出目录）在哪里：</strong><ul>
<li>定义 <code>OUTPUT_DIR</code> 和 <code>VIDEO_OUTPUT</code>。</li>
<li><em>含义：</em> 训练出来的模型存放在 <code>models/vla_libero_grpo</code>，训练过程中的录像存在 <code>video</code> 文件夹。</li>
</ul>
</li>
<li><strong>[TODO 3] 确认底子（基础模型）：</strong><ul>
<li>定义 <code>SFT_MODEL_PATH</code>。</li>
<li><em>含义：</em> 我们不是从零开始教机器人，而是基于一个已经经过“监督微调（SFT）”的模型（<code>Openvla-oft-SFT...</code>）继续深造。</li>
</ul>
</li>
</ul>
<h3>💻 Task Phase 2: 分配算力 (硬件资源)</h3>
<p>这是多卡训练的关键部分，脚本在这里分配“谁负责干什么”。</p>
<ul>
<li><strong>[TODO 4] 清点人数（GPU）：</strong><ul>
<li><code>NUM_NODES=1</code>, <code>NUM_GPUS=8</code>。</li>
<li><em>含义：</em> 我们有一台机器，一共 8 张显卡。</li>
</ul>
</li>
<li><strong>[TODO 5] 分工合作（环境 vs 训练）：</strong><ul>
<li><code>NUM_ENV_GPUS=4</code>。</li>
<li><em>含义：</em> 这是一个很重要的观点。<strong>这 8 张卡被劈成两半</strong>：<ul>
<li><strong>4 张卡</strong> 专门用来跑<strong>仿真环境</strong>（Isaac Sim），负责模拟物理世界，让机器人去“动”。</li>
<li><strong>剩下 4 张卡</strong> (<code>8 - 4</code>) 专门用来跑<strong>AI模型</strong>，负责计算梯度、更新大脑。</li>
</ul>
</li>
</ul>
</li>
<li><strong>[TODO 6] 适配新型号显卡（Hopper 架构）：</strong><ul>
<li>脚本里有一段 <code>if echo "$gpu_name" | grep "NVIDIA H"; then ...</code></li>
<li><em>含义：</em> 如果发现显卡是 NVIDIA H100/H800 系列，强制开启 <code>osmesa</code> 模式。这是为了解决新显卡在渲染仿真画面时的兼容性问题。</li>
</ul>
</li>
</ul>
<h3>🎮 Task Phase 3: 搭建训练场 (仿真器设置)</h3>
<ul>
<li><strong>[TODO 7] 选择物理引擎：</strong><ul>
<li><code>SIM_TYPE="isaac"</code>。</li>
<li><em>含义：</em> 指定使用 NVIDIA 的 <strong>Isaac Sim</strong> 作为物理仿真器（而不是传统的 Mujoco），因为 Isaac Sim 的视觉效果和物理模拟更逼真，适合 VLA（视觉-语言-动作）模型。</li>
</ul>
</li>
<li><strong>[TODO 8] 找到专用的 Python：</strong><ul>
<li>因为 Isaac Sim 很特殊，它自带了一个 Python 环境。脚本会优先使用 <code>/workspace/isaaclab/.../python.sh</code> 来运行代码，而不是系统默认的 python。</li>
</ul>
</li>
</ul>
<h3>🚀 Task Phase 4: 开始特训 (运行核心命令)</h3>
<p>这是脚本最长的一段，执行 <code>recipe.vla.main_ppo</code>。这相当于给教官下达了详细的“教学大纲”。</p>
<p><strong>我们将这些复杂的参数拆解为几个核心观点：</strong></p>
<h4>1. 教学模式 (算法设置)</h4>
<ul>
<li><strong>[TODO 9] 确定学习方法：</strong><ul>
<li><code>algorithm.adv_estimator=reinforce_plus_plus</code></li>
<li><em>观点：</em> 使用 <strong>Reinforce++</strong> (一种强化学习算法的变体，常用于 GRPO) 来估计优势。这意味着机器人在环境里做动作，做对了加分，做错了减分，以此来调整模型。</li>
</ul>
</li>
<li><strong>[TODO 10] 设定探索程度：</strong><ul>
<li><code>algorithm.kl_ctrl.kl_coef=0.00</code></li>
<li><em>观点：</em> 这里把 KL 散度系数设为 0。通常 KL 惩罚是用来防止模型偏离“初心（原始模型）”太远。设为 0 意味着<strong>允许模型为了拿高分（完成任务）可以大幅度改变自己的行为模式</strong>，放飞自我去学。</li>
</ul>
</li>
</ul>
<h4>2. 课程安排 (训练参数)</h4>
<ul>
<li><strong>[TODO 11] 设定学时和频率：</strong><ul>
<li><code>trainer.total_epochs=20</code> (总共学20个大周期)</li>
<li><code>env.train.max_episode_steps=512</code> (每次尝试最多做512个动作，超时就算失败)</li>
<li><code>trainer.save_freq=30</code> (每30步存个档，防止白练)</li>
</ul>
</li>
<li><strong>[TODO 12] 设定学习率：</strong><ul>
<li><code>actor.optim.lr=5e-6</code></li>
<li><em>观点：</em> 学习率非常小（0.000005）。因为这是一个微调阶段，模型已经很聪明了，我们只需要<strong>微调</strong>，步子大了容易把模型“学傻了”。</li>
</ul>
</li>
</ul>
<h4>3. 模型细节 (VLA配置)</h4>
<ul>
<li><strong>[TODO 13] 视觉与动作配置：</strong><ul>
<li><code>env.actor.model.action_dim=7</code> (机器人有7个自由度，比如6个关节+1个夹爪)</li>
<li><code>actor.fsdp_config.model_dtype=bfloat16</code> (使用半精度计算，为了省显存并加速，且 bfloat16 训练更稳定)</li>
<li><code>actor.model.enable_gradient_checkpointing=False</code> (关闭梯度检查点，用显存换速度，跑得更快)。</li>
</ul>
</li>
</ul>
<h4>4. 考试与监控 (验证与日志)</h4>
<ul>
<li><strong>[TODO 14] 录像留念：</strong><ul>
<li><code>env.train.video_cfg.save_video=True</code></li>
<li><em>观点：</em> 必须把机器人操作的过程录成视频存下来。因为是强化学习，光看分数（Reward）是不够的，必须人眼看看它是不是真的学会了抓取物体，还是在卡Bug。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：这个脚本在干嘛？</h3>
<p>如果把这个过程比作<strong>“训练飞行员”</strong>：</p>
<ol>
<li><strong>Phase 1</strong> 是准备好飞行手册和训练基地。</li>
<li><strong>Phase 2</strong> 是把 8 台模拟机分配好，4 台用来模拟天气和飞机物理反馈（环境），4 台用来计算飞行员的大脑反应（训练）。</li>
<li><strong>Phase 3</strong> 是指定使用最先进的“Isaac”模拟软件。</li>
<li><strong>Phase 4</strong> 是具体的飞行教官指令：<ul>
<li>基于一个已经会飞的学员（SFT Model）。</li>
<li>让他不断尝试降落（Reinforce++）。</li>
<li>只要能停稳，姿势怎么变都行（KL=0）。</li>
<li>每次尝试都要录像回放（Save Video）。</li>
</ul>
</li>
</ol>
<p>你看懂了吗？这个脚本就是为了启动这样一个复杂的<strong>机器人强化学习闭环</strong>。</p>