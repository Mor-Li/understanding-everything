<h1>recipe/vla/envs/action_utils.py</h1>
<p>这个文件 <code>action_utils.py</code> 其实是一个<strong>给机器人（或者AI Agent）打下手的“工具箱”</strong>。</p>
<p>在训练机器人（VLA: Vision-Language-Action 模型）的时候，我们需要处理很多琐碎的事情，比如把数据格式转来转去、把机器人的动作指令标准化、或者把机器人看到的画面做成视频存下来。</p>
<p>为了让你更容易理解，我把你当作这个AI项目的“大管家”，我把这个文件里的代码拆解成一个 <strong>“任务清单 (ToDo List)”</strong>。每当你需要做某件事时，这个文件里就有一个对应的工具函数来帮你完成。</p>
<hr />
<h3>📋 任务清单：AI 训练与环境交互的大管家</h3>
<h4>Task 1: 翻译官 —— 处理机器人的动作指令</h4>
<p><strong>场景</strong>：AI 模型（大脑）输出了一串数字作为动作，但仿真器或真实机器人（手脚）可能听不懂，或者定义的标准不一样（比如夹爪是开是闭）。
*   <strong>待办事项</strong>：把“大脑”的原始指令，转换成“手脚”能执行的标准指令。
*   <strong>对应函数</strong>：
    *   <code>prepare_actions</code> 和 <code>prepare_actions_simplevla</code>
    *   <strong>解释</strong>：这两个函数负责把原始动作数据进行<strong>归一化</strong>（Normalize，把数值缩放到特定范围）和<strong>反转夹爪控制</strong>（Invert Gripper，比如把 0变1，1变0，因为有的机器人0是开，有的是闭）。</p>
<h4>Task 2: 搬运工 —— 数据格式转换</h4>
<p><strong>场景</strong>：仿真器给出的数据通常是 NumPy 数组（CPU上的），但 AI 模型训练需要用 PyTorch Tensor（通常在 GPU 上）。
*   <strong>待办事项</strong>：把各种乱七八糟的数据（字典、数组、列表）统统打包，扔到显卡（GPU）上去。
*   <strong>对应函数</strong>：
    *   <code>to_tensor</code>
    *   <strong>解释</strong>：就像一个万能插头，不管你来的是什么数据格式，都给你转成 PyTorch 能用的 Tensor，并放到指定的设备（Device）上。
    *   <code>list_of_dict_to_dict_of_list</code>
    *   <strong>解释</strong>：把“一堆字典的列表”变成“一个包含列表的字典”。方便批量处理数据（Batching）。</p>
<h4>Task 3: 剪辑师 —— 图像拼接与视频制作</h4>
<p><strong>场景</strong>：机器人可能有好几个摄像头（左眼、右眼、手上的眼）。为了方便人类观察，我们需要把这些画面拼在一起，甚至做成视频回放。
*   <strong>待办事项 A</strong>：把多张图片拼成一张大图。
    *   <strong>对应函数</strong>：<code>tile_images</code>
    *   <strong>解释</strong>：就像监控室的屏幕墙，把好几个摄像头的画面按行按列拼在一起，变成一张大图。
*   <strong>待办事项 B</strong>：把一连串的图片存成 MP4 视频。
    *   <strong>对应函数</strong>：<code>save_rollout_video</code>
    *   <strong>解释</strong>：把机器人这一局（Episode）看到的所有画面连起来，保存成一个视频文件，方便你看它是不是在“发癫”或者成功完成了任务。</p>
<h4>Task 4: 字幕组 —— 给画面加文字说明</h4>
<p><strong>场景</strong>：光看视频不知道机器人当前状态如何（比如现在的得分是多少？成功率是多少？）。
*   <strong>待办事项</strong>：在图片上写字，显示关键信息。
    *   <strong>对应函数</strong>：
        *   <code>put_text_on_image</code>：在图片上写几行字，如果字太长还会自动换行。
        *   <code>put_info_on_image</code>：专门用来把数据字典（比如 <code>{'reward': 1.0, 'success': True}</code>）格式化成文字，印在图片上。</p>
<h4>Task 5: 美工 —— 图片预处理</h4>
<p><strong>场景</strong>：AI 模型对输入的图片尺寸和质量有特定的要求，直接传进去可能会报错或者效果不好。
*   <strong>待办事项</strong>：调整图片大小、裁剪图片。
    *   <strong>对应函数</strong>：
        *   <code>resize_image</code>：把图片缩放到指定的大小（比如 256x256）。<strong>注意</strong>：这个函数里有一个奇怪的操作（存成JPEG再读回来），这是为了模拟某种特定的图像压缩噪声，让训练数据更真实。
        *   <code>center_crop_image</code>：把图片的中心部分切出来，放大回原尺寸。这通常是为了让机器人更关注画面中心的操作区域，或者是为了做数据增强。</p>
<hr />
<h3>总结：这个文件讲了啥？</h3>
<p><strong>一句话总结：</strong>
这是一个<strong>杂役脚本</strong>，它不包含核心的 AI 算法，但它负责处理<strong>动作数据的清洗</strong>、<strong>图像数据的修剪与拼接</strong>、以及<strong>生成可视化的调试视频</strong>。</p>
<p><strong>它的工作流大概是这样的：</strong>
1.  环境给图片 -&gt; 用 <strong>Task 5 (美工)</strong> 处理图片大小。
2.  模型要计算 -&gt; 用 <strong>Task 2 (搬运工)</strong> 转成 Tensor。
3.  模型出动作 -&gt; 用 <strong>Task 1 (翻译官)</strong> 转成机器人指令。
4.  运行完一局 -&gt; 用 <strong>Task 3 &amp; 4 (剪辑师 &amp; 字幕组)</strong> 把过程拼成带字幕的视频存下来给人类看。</p>