<h1>recipe/deepeyes/configs/deepeyes_multiturn_grpo.yaml</h1>
<p>这份配置文件 (<code>deepeyes_multiturn_grpo.yaml</code>) 其实是在<strong>训练一个多模态（能看图）的 AI Agent（智能体）</strong>。</p>
<p>更具体地说，它是在配置一个 <strong>强化学习（RL）</strong> 的训练流程，目的是让这个 AI 学会<strong>使用工具</strong>（比如放大图片）来解决复杂的视觉任务。</p>
<p>为了让你更容易理解，我们可以把这个配置文件的内容想象成<strong>“训练一个实习生（AI）完成看图找茬任务”</strong>的教学大纲。</p>
<p>下面是一个 <strong>Task Todo List</strong>，我们按照这个流程一步步拆解文件里的内容：</p>
<hr />
<h3>✅ Task 1: 准备教材与考场 (配置数据)</h3>
<p><strong>对应代码段：</strong> <code>data</code> 部分</p>
<ul>
<li><strong>Todo:</strong> 设定每次考试（训练）的数据规模和限制。</li>
<li><strong>解读:</strong><ul>
<li><code>max_prompt_length: 2048</code>: 告诉实习生，题目的长度（文字+图片描述）最多这么长。</li>
<li><code>train_batch_size: 256</code>: 每次让 256 个“分身”同时做题，批量学习。</li>
<li><code>custom_cls</code>: 指定了题目从哪里来。这里指向了一个叫 <code>CustomRLHFDataset</code> 的自定义数据集，说明这不是普通的问答，而是专门定制的“DeepEyes”视觉任务数据。</li>
</ul>
</li>
</ul>
<h3>✅ Task 2: 规范沟通语言 (配置聊天模板)</h3>
<p><strong>对应代码段：</strong> <code>actor_rollout_ref</code> -&gt; <code>model</code> -&gt; <code>custom_chat_template</code></p>
<ul>
<li><strong>Todo:</strong> 规定实习生怎么听懂人话，怎么看图，以及怎么申请使用工具。</li>
<li><strong>解读:</strong><ul>
<li>那一一大坨看着很乱的代码（<code>{% set image_count ...</code>）其实是<strong>翻译规则</strong>。</li>
<li>它定义了 AI 如何理解输入：<ul>
<li><strong>角色定义</strong>: 哪里是用户说的话 (<code>user</code>)，哪里是系统指令 (<code>system</code>)。</li>
<li><strong>视觉输入</strong>: 哪里插入了图片 (<code>&lt;|vision_start|&gt;</code>) 或视频。</li>
<li><strong>工具调用</strong>: 最关键的是，它教 AI 如果想用工具，必须输出 <code>&lt;tool_call&gt;</code> 这样的特殊格式，而不是直接瞎在那儿喊。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>✅ Task 3: 赋予行动能力 (配置多轮交互与工具)</h3>
<p><strong>对应代码段：</strong> <code>actor_rollout_ref</code> -&gt; <code>rollout</code></p>
<ul>
<li><strong>Todo:</strong> 允许实习生在回答最终答案前，先尝试动手操作。</li>
<li><strong>解读:</strong><ul>
<li><code>name: sglang</code>: 使用 <code>sglang</code> 这个引擎来生成回答（因为它生成速度快，适合大规模训练）。</li>
<li><strong><code>multi_turn: enable: True</code> (核心点!)</strong>: 开启多轮对话模式。这意味着 AI 不是“看图 -&gt; 直接给答案”，而是可以“看图 -&gt; <strong>操作工具</strong> -&gt; 再看结果 -&gt; 再操作 -&gt; 最后给答案”。</li>
<li><code>max_assistant_turns: 5</code>: 限制 AI 最多折腾 5 次，防止它陷入死循环（比如不停地放大图片）。</li>
<li><code>tool_config_path</code>: 给 AI 配备了一个具体的工具包，路径指向 <code>image_zoom_in_tool_config.yaml</code>。这暗示了这个 AI 的核心技能是<strong>“放大图片看细节”</strong>。</li>
</ul>
</li>
</ul>
<h3>✅ Task 4: 制定评分标准 (配置奖励函数)</h3>
<p><strong>对应代码段：</strong> <code>custom_reward_function</code></p>
<ul>
<li><strong>Todo:</strong> 决定实习生做得好不好，给它打分。</li>
<li><strong>解读:</strong><ul>
<li>强化学习的核心是“奖励”。</li>
<li>这里指向了 <code>deepeyes.py</code> 里的 <code>compute_score</code> 函数。</li>
<li><strong>逻辑是：</strong> AI 操作完工具并给出最终答案后，这个函数会自动判断：<ol>
<li>你答案对不对？</li>
<li>你工具用得合不合理？（比如有没有乱放大，有没有在不需要放大的时候放大）</li>
</ol>
</li>
<li>根据表现给出一个分数，AI 会为了拿高分而调整自己的策略。</li>
</ul>
</li>
</ul>
<h3>✅ Task 5: 整体训练架构 (Hydra &amp; Defaults)</h3>
<p><strong>对应代码段：</strong> <code>defaults</code></p>
<ul>
<li><strong>Todo:</strong> 确定用什么教学方法。</li>
<li><strong>解读:</strong><ul>
<li><code>- ppo_trainer</code>: 虽然文件名叫 GRPO，但这里继承了 PPO（一种经典的强化学习算法）的配置。这说明整个流程是基于“试错 -&gt; 打分 -&gt; 修正模型”的循环。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：这个文件到底想干啥？</h3>
<p><strong>一句话总结：</strong>
这是一个训练脚本配置，目的是训练一个 AI，让它在面对一张图片时，<strong>学会主动使用“放大镜”工具（Zoom In）去查看细节</strong>，经过几轮观察后，给出正确的结论。</p>
<p><strong>它眼中的世界是这样的：</strong>
1.  <strong>看到图片</strong>（Data）。
2.  <strong>思考</strong>：“这图太糊了，我看不清，我要用放大镜”（Model &amp; Template）。
3.  <strong>行动</strong>：发出 <code>&lt;tool_call&gt;zoom_in&lt;/tool_call&gt;</code> 指令（Rollout &amp; Tool）。
4.  <strong>环境反馈</strong>：看到了一张放大的局部图（Multi-turn）。
5.  <strong>最终回答</strong>：“哦，我看清了，这是个猫”（Answer）。
6.  <strong>老师打分</strong>：“做得好，加分！”或者“你看得是狗，扣分！”（Reward）。</p>