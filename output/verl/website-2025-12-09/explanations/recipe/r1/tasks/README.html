<h1>recipe/r1/tasks</h1>
<p>没问题！我们跳出代码细节，用最生活化的方式来重新认识这个文件夹。</p>
<p>你可以把 <code>recipe/r1/tasks</code> 这个文件夹想象成一个 <strong>“自动阅卷组”</strong>。</p>
<h3>1. 当前这个文件夹主要负责什么功能？</h3>
<p><strong>核心功能：判卷子。</strong></p>
<p>你的 AI 模型就像一个正在备考的学生。这个文件夹里的代码，就是用来判断学生<strong>做题对不对</strong>的工具。
无论学生是做“选择题”（GPQA）还是做“编程题”（LiveCodeBench），这里都规定了具体的<strong>评分标准</strong>和<strong>判分手段</strong>。</p>
<hr />
<h3>2. 这个文件夹下的各个文件是干什么的？</h3>
<p>我们把这三个文件比作阅卷组里的三个角色：</p>
<ul>
<li>
<p><strong><code>__init__.py</code> —— 办公室的门牌</strong></p>
<ul>
<li><strong>作用</strong>：它没有任何实际工作能力。</li>
<li><strong>比喻</strong>：它就像挂在门口的<strong>“阅卷组”牌子</strong>。它的存在只是为了告诉外面的人（Python解释器）：“这间屋子是正规的，里面有工作人员，你可以进来找人办事。”</li>
</ul>
</li>
<li>
<p><strong><code>gpqa.py</code> —— 负责改选择题的“读卡机”</strong></p>
<ul>
<li><strong>作用</strong>：处理文本类的难题（GPQA数据集）。</li>
<li><strong>比喻</strong>：这是一个<strong>无情的“答题卡扫描仪”</strong>。它不管学生在试卷上写了多少推理过程（废话），它只死死盯着最后一行写没写“Answer: C”。如果写了 C，且标准答案是 C，就给分；否则零分。</li>
</ul>
</li>
<li>
<p><strong><code>livecodebench.py</code> —— 负责改编程题的“机房监考员”</strong></p>
<ul>
<li><strong>作用</strong>：处理代码生成类的题目（LiveCodeBench数据集）。</li>
<li><strong>比喻</strong>：这是一个<strong>谨慎的“实验课老师”</strong>。它把学生写的代码拿过来，扔进一个<strong>防爆玻璃房</strong>（独立进程）里运行一下。如果代码跑通了且结果正确，就给过；如果代码报错或者死循环（超时），直接判挂科。</li>
</ul>
</li>
</ul>
<hr />
<h3>3. 高层认知：如何快速理解这部分代码？</h3>
<p>要把这部分代码看懂，你只需要记住一个词：<strong>“反馈信号” (Reward Signal)</strong>。</p>
<p>在训练像 DeepSeek-R1 这种拥有“推理能力”的模型时，模型会生成很长的思考过程。但计算机怎么知道模型想对了没？</p>
<ul>
<li><strong>这部分代码就是那个“裁判”</strong>。</li>
<li>模型说了一大堆 -&gt; 裁判（tasks文件夹）介入 -&gt; 运行代码或比对答案 -&gt; <strong>告诉模型：“你做对了” (Reward 1) 或 “你做错了” (Reward 0)</strong>。</li>
</ul>
<p><strong>总结一句话：</strong>
这就是一套<strong>自动化打分系统</strong>，专门用来验证 AI 最终生成的答案是否正确，从而指导 AI 往正确的方向进化。</p>