<h1>recipe/r1/data_process.py</h1>
<p>这份代码其实就是一个<strong>“数据备菜员”</strong>。</p>
<p>它的核心任务是：从网上下载几个特定的考试数据集（数学、代码、科学问答），把它们清洗、整理成<strong>统一的格式</strong>，最后打包成一个 <code>.parquet</code> 文件，方便后续用来<strong>测试（评估）</strong> AI 模型的能力。</p>
<p>为了让你更容易理解，我把它拆解成一个 <strong>Task To-Do List（任务清单）</strong>，模拟这个脚本在一步步做什么：</p>
<hr />
<h3>📋 任务清单：R1 数据预处理流程</h3>
<h4>✅ Task 1: 制定“统一模具” (Standardization)</h4>
<p><strong>代码对应：</strong> <code>example_map_fn</code> 函数
*   <strong>目的：</strong> 因为不同的数据集长得都不一样（有的叫 "Question"，有的叫 "Problem"），我们需要把它们变成同一个样子。
*   <strong>动作：</strong> 定义了一个标准字典格式，包含：
    *   <code>prompt</code>: 也就是要把题目塞进 <code>[{"role": "user", "content": ...}]</code> 这样的对话框里。
    *   <code>reward_model</code>: 存放标准答案（Ground Truth），用于后面判卷子。
    *   <code>ability</code>: 标记这道题考的是什么能力（比如 "Math", "Code"）。</p>
<h4>✅ Task 2: 处理 AIME 2024 数据集 (数学竞赛)</h4>
<p><strong>代码对应：</strong> <code>build_aime2024_dataset</code> 函数
*   <strong>动作：</strong>
    1.  从 HuggingFace 下载 <code>Maxwell-Jia/AIME_2024</code>。
    2.  提取题目和答案。
    3.  打上标签：能力是 <code>English</code> (这里可能是指英文数学题)，用途是 <code>test</code>。
    4.  用 Task 1 的模具把它格式化。</p>
<h4>✅ Task 3: 处理 GPQA Diamond 数据集 (高难度科学问答)</h4>
<p><strong>代码对应：</strong> <code>build_gpqa_dimond_dataset</code> 函数
*   <strong>难点：</strong> 这是一个多项选择题库。
*   <strong>动作：</strong>
    1.  下载数据。
    2.  <strong>随机打乱选项</strong>：代码里有一个 <code>random.shuffle</code>，把正确答案和错误干扰项混在一起，防止模型只背“选C”。
    3.  生成 Prompt：把题目拼凑成 "A) ... B) ... C) ... D) ..." 的格式。
    4.  记录正确选项（比如 "B"）。
    5.  打上标签：能力是 <code>Math</code> (广义理科)。</p>
<h4>✅ Task 4: 处理 CNMO 2024 数据集 (中国数学奥赛)</h4>
<p><strong>代码对应：</strong> <code>build_cnmo2024_dataset</code> 函数
*   <strong>动作：</strong>
    1.  分别下载英文版 (<code>CNMO_en</code>) 和中文版 (<code>CNMO_cn</code>) 的试题。
    2.  分别处理格式。
    3.  <strong>合并</strong>：把中英文两个数据集拼在一起。</p>
<h4>✅ Task 5: 处理 LiveCodeBench 数据集 (代码编程)</h4>
<p><strong>代码对应：</strong> <code>build_livecodebench_dataset</code> 函数
*   <strong>最复杂的步骤：</strong> 代码题不仅有题目，还有测试用例（Test Cases）。
*   <strong>动作：</strong>
    1.  <strong>时间过滤</strong>：代码里特意写了只保留 <code>2024-08</code> 到 <code>2025-01</code> 期间的题目（为了测试最新的 R1 模型，避免数据泄露或太旧）。
    2.  <strong>构建 Prompt</strong>：告诉模型“请写一个Python程序解决这个问题...”。如果有起始代码（starter code）也要放进去。
    3.  <strong>压缩测试用例</strong>：代码里用了 <code>zlib</code> 和 <code>base64</code>。这是因为测试用例（输入输出数据）可能很大，直接存文本太占地儿，所以把它压缩编码成一串字符存起来。</p>
<h4>✅ Task 6: 总控与打包 (Main Execution)</h4>
<p><strong>代码对应：</strong> <code>if __name__ == "__main__":</code> 及其后
*   <strong>动作：</strong>
    1.  <strong>听指挥</strong>：读取命令行参数。你可以告诉它只处理 <code>aime2024</code>，或者默认处理 <code>all</code> (全部)。
    2.  <strong>大合并</strong>：把上面 Task 2, 3, 4, 5 处理好的数据全部 <code>concatenate</code>（串联）成一个巨大的列表。
    3.  <strong>存盘</strong>：保存为 <code>test.parquet</code> 文件到本地目录。
    4.  <strong>上传 (可选)</strong>：如果你配置了 HDFS（大数据分布式存储），它还会把文件上传到云端。</p>
<hr />
<h3>💡 总结文中的核心观点</h3>
<p>这段代码虽然是技术脚本，但传达了 <strong>DeepSeek-R1 (或复现项目)</strong> 在评估阶段的几个关键策略：</p>
<ol>
<li><strong>格式统一化</strong>：无论原始数据是选择题、简答题还是代码题，最终都转化为 <strong>User Prompt (题目) + Ground Truth (答案)</strong> 的统一对话格式，方便模型统一推理。</li>
<li><strong>防作弊/防过拟合</strong>：在 GPQA 处理中加入了选项随机打乱，确保模型是真的会做题，而不是记住了答案的位置。</li>
<li><strong>时效性关注</strong>：在代码题（LiveCodeBench）中严格筛选了 <strong>2024年底到2025年初</strong> 的题目。这是为了测试模型在<strong>训练数据截止之后</strong>出现的新题目上的表现（泛化能力），这是评估大模型非常重要的一环。</li>
<li><strong>多领域综合评估</strong>：脚本覆盖了数学（AIME, CNMO）、硬核科学（GPQA）、编程（LiveCodeBench），说明该模型注重理科和逻辑推理能力的综合评测。</li>
</ol>