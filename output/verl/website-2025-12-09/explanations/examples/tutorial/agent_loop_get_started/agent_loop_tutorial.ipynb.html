<h1>examples/tutorial/agent_loop_get_started/agent_loop_tutorial.ipynb</h1>
<p>这份教程主要是在教你<strong>如何利用 <code>verl</code> 框架训练一个能够“写代码并运行代码”来解决数学问题的 AI（智能体）</strong>。</p>
<p>简单来说，就是把大模型（LLM）变成一个会用 Python 计算器的考生。</p>
<p>为了让你看懂，我把整个 Notebook 的流程拆解成了一个 <strong>Task Todo List</strong>，你可以把它想象成一个开发项目的步骤清单：</p>
<hr />
<h3>📝 核心目标：训练一个 ReAct Agent（会思考+会行动的 AI）</h3>
<h4>第一阶段：准备工作 (环境与素材)</h4>
<ul>
<li>
<p>[ ] <strong>Task 1: 安装与初始化</strong></p>
<ul>
<li><strong>做了啥</strong>：安装 <code>verl</code> 库，启动 Ray（一个分布式计算框架，用来管理后台任务）。</li>
<li><strong>目的</strong>：搭建基础设施，相当于把电脑开机并装好系统。</li>
</ul>
</li>
<li>
<p>[ ] <strong>Task 2: 准备模型和题目</strong></p>
<ul>
<li><strong>做了啥</strong>：下载 Qwen3-1.7B（作为 AI 的大脑）和 MATH 数据集（作为 AI 要做的数学卷子）。</li>
<li><strong>目的</strong>：有了大脑（模型）和考题（数据），才能开始训练。</li>
</ul>
</li>
<li>
<p>[ ] <strong>Task 3: 启动推理服务</strong></p>
<ul>
<li><strong>做了啥</strong>：配置并启动 <code>vllm</code> 或 <code>sglang</code> 服务。</li>
<li><strong>目的</strong>：这是一个高性能的“回答生成器”，专门用来让 AI 快速吐字。</li>
</ul>
</li>
</ul>
<h4>第二阶段：热身练习 (学习如何调用工具)</h4>
<ul>
<li>
<p>[ ] <strong>Task 4: 定义一个简单的工具 (查天气)</strong></p>
<ul>
<li><strong>做了啥</strong>：写了一个假的 <code>WeatherTool</code> 类。</li>
<li><strong>目的</strong>：先别急着写代码，先让 AI 学会最简单的“按按钮”。如果用户问“巴黎天气怎么样”，AI 应该知道去调用这个工具，而不是瞎编。</li>
</ul>
</li>
<li>
<p>[ ] <strong>Task 5: 测试工具调用流程</strong></p>
<ul>
<li><strong>做了啥</strong>：<ol>
<li>用户提问。</li>
<li>AI 返回一个“调用请求”（而不是直接回答）。</li>
<li>我们手动执行这个工具，拿到结果（26.1度）。</li>
<li>把结果告诉 AI。</li>
<li>AI 根据结果生成最终回答：“巴黎现在 26.1 度”。</li>
</ol>
</li>
<li><strong>目的</strong>：这是 <strong>ReAct (Reasoning + Acting)</strong> 的雏形：<strong>思考 -&gt; 行动(调工具) -&gt; 观察结果 -&gt; 回答</strong>。</li>
</ul>
</li>
</ul>
<h4>第三阶段：实战演练 (构建代码沙盒)</h4>
<p>这是本教程的核心部分。因为大模型算术很差，我们要给它配一个 Python 解释器。</p>
<ul>
<li>
<p>[ ] <strong>Task 6: 搭建“代码沙盒” (Sandbox)</strong></p>
<ul>
<li><strong>做了啥</strong>：用 FastAPI 写了一个简易的服务器，它能接收 Python 代码，运行它，然后返回打印结果（stdout）或报错信息（stderr）。</li>
<li><strong>目的</strong>：<strong>安全隔离</strong>。你肯定不想让 AI 在你的主系统里随便删文件，所以要在一个隔离环境（沙盒）里跑 AI 写的代码。</li>
</ul>
</li>
<li>
<p>[ ] <strong>Task 7: 定义“沙盒工具” (SandboxTool)</strong></p>
<ul>
<li><strong>做了啥</strong>：写一个工具类，把 AI 生成的 Python 代码提取出来，发送给 Task 6 里的那个服务器去跑。</li>
<li><strong>目的</strong>：连接 AI 和 沙盒的桥梁。</li>
</ul>
</li>
<li>
<p>[ ] <strong>Task 8: 测试沙盒 (Good Case &amp; Bad Case)</strong></p>
<ul>
<li><strong>做了啥</strong>：<ul>
<li>测好的：发 <code>print(sqrt(3))</code>，看能不能算出 1.732。</li>
<li>测坏的：发一段错误代码，看能不能抓到报错信息。</li>
</ul>
</li>
<li><strong>目的</strong>：确保工具好用。报错信息很重要，因为 AI 看到报错后会尝试自我修正代码。</li>
</ul>
</li>
<li>
<p>[ ] <strong>Task 9: 手动跑通“做数学题”的闭环</strong></p>
<ul>
<li><strong>做了啥</strong>：找一道真实的数学题，写一个 <code>while True</code> 循环：<ol>
<li>AI 审题，觉得需要计算。</li>
<li>AI 生成 Python 代码。</li>
<li>脚本检测到代码，丢进沙盒运行。</li>
<li>运行结果贴回对话历史。</li>
<li>AI 看到结果，算出最终答案。</li>
</ol>
</li>
<li><strong>目的</strong>：验证这一套流程（Agent Loop）在逻辑上是通的。</li>
</ul>
</li>
</ul>
<h4>第四阶段：正式训练 (End-to-End Training)</h4>
<p>手动跑通只是第一步，现在要用强化学习（RL）让 AI 变得更聪明，更擅长使用在这个工具。</p>
<ul>
<li>
<p>[ ] <strong>Task 10: 导出工具配置</strong></p>
<ul>
<li><strong>做了啥</strong>：把沙盒工具的配置保存为 <code>tool_config.json</code>。</li>
<li><strong>目的</strong>：训练脚本需要读取这个配置，知道去哪里调用沙盒。</li>
</ul>
</li>
<li>
<p>[ ] <strong>Task 11: 配置 PPO 训练参数</strong></p>
<ul>
<li><strong>做了啥</strong>：设置 <code>verl</code> 的训练配置（config）。<ul>
<li>算法：GRPO (一种强化学习算法)。</li>
<li>数据：用刚才下载的数学题。</li>
<li>关键设置：<code>rollout.agent.default_agent_loop=tool_agent</code>（告诉系统，训练的时候要允许 AI 多轮对话调用工具）。</li>
</ul>
</li>
<li><strong>目的</strong>：制定训练计划。</li>
</ul>
</li>
<li>
<p>[ ] <strong>Task 12: 开始训练</strong></p>
<ul>
<li><strong>做了啥</strong>：运行 <code>main(config)</code>。</li>
<li><strong>目的</strong>：启动训练。AI 会不断做题、写代码、看结果、修正。做对了给奖励，做错了（或者代码跑不通）给惩罚。通过这种方式，AI 会越来越擅长用 Python 解决数学问题。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：这到底是在讲什么观点？</h3>
<p>这篇文章的核心观点是：<strong>解决复杂逻辑问题（如数学），不能光靠大模型“瞎想”，得让它学会用“工具”。</strong></p>
<ol>
<li><strong>ReAct 模式</strong>：不要让 AI 一次性把话说完。要让它 <strong>想一步 -&gt; 做一步(写代码) -&gt; 看一步(看运行结果) -&gt; 再想下一步</strong>。</li>
<li><strong>环境支持</strong>：要实现这一点，你需要一个能够执行代码的环境（Sandbox）和一个能够支持多轮交互的训练框架（Verl）。</li>
<li><strong>强化学习</strong>：通过 PPO/GRPO 算法，我们可以专门训练 AI 掌握这种“写代码解题”的能力，而不是仅仅依赖它预训练的知识。</li>
</ol>