<h1>examples/sft/gsm8k/run_qwen_05_peft.sh</h1>
<p>没问题。面对这种全是代码和参数的脚本，感到头大是非常正常的。</p>
<p>我们可以把这个脚本看作是一个<strong>“装修工程的任务清单”</strong>。你的目标是装修一个房子（训练一个AI模型），这个脚本就是告诉施工队（计算机）具体该怎么干。</p>
<p>为了让你听懂，我把这个过程拆解成一个 <strong>6步走的 To-Do List</strong>。</p>
<hr />
<h3>任务清单：训练你的数学小天才</h3>
<h4>✅ 第一步：搞清楚我们在干什么（核心概念）</h4>
<p>在看代码前，先明白这个脚本的目标：
*   <strong>动作</strong>：SFT (Supervised Fine-Tuning)。俗称“<strong>微调</strong>”。
*   <strong>比喻</strong>：你有一个已经读过很多书的大学生（Qwen模型），但他数学不太好。你现在要拿一本小学奥数习题集（GSM8K数据集），对他进行特训，让他专门精通数学题。
*   <strong>方法</strong>：PEFT (LoRA)。俗称“<strong>省钱模式</strong>”。
*   <strong>比喻</strong>：重新教育这个大学生太贵了。我们不改变他原本的大脑，只是给他脑子上贴几个“外挂芯片”（LoRA），只训练这几个芯片，既快又省显卡。</p>
<hr />
<h4>✅ 第二步：检查开工条件（脚本开头）</h4>
<p>代码的前几行是在做“安检”。</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$#</span><span class="s2">&quot;</span><span class="w"> </span>-lt<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Usage: ...&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="nv">nproc_per_node</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">save_path</span><span class="o">=</span><span class="nv">$2</span>
<span class="nb">shift</span><span class="w"> </span><span class="m">2</span>
</code></pre></div>

<ul>
<li><strong>解读</strong>：<ul>
<li>脚本说：“嘿，你要运行我，必须告诉我两件事：1. 你有几张显卡（<code>nproc_per_node</code>）？ 2. 训练好的模型存哪里（<code>save_path</code>）？”</li>
<li>如果你没给这两个参数，它就报错退出。</li>
<li><code>shift 2</code> 是个命令处理技巧，意思是“把这两个参数拿走，剩下的参数留给后面用”。</li>
</ul>
</li>
</ul>
<hr />
<h4>✅ 第三步：启动施工队（Torchrun）</h4>
<p>这是最长的那行命令的开头。</p>
<div class="codehilite"><pre><span></span><code>torchrun<span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="nv">$nproc_per_node</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-m<span class="w"> </span>verl.trainer.fsdp_sft_trainer<span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>解读</strong>：<ul>
<li><code>torchrun</code>：这是包工头。它负责协调多张显卡一起工作。</li>
<li><code>-m verl.trainer.fsdp_sft_trainer</code>：这是施工图纸。告诉包工头，我们要运行 <code>verl</code> 这个库里的 <code>fsdp_sft_trainer</code> 程序（专门用来做微调的程序）。</li>
</ul>
</li>
</ul>
<hr />
<h4>✅ 第四步：提供教材（数据设置）</h4>
<p>接下来的几行是在告诉模型：<strong>“去看哪本书？”</strong></p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>data.train_files<span class="o">=</span><span class="nv">$HOME</span>/data/gsm8k/train.parquet<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.val_files<span class="o">=</span><span class="nv">$HOME</span>/data/gsm8k/test.parquet<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.prompt_key<span class="o">=</span>extra_info<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.response_key<span class="o">=</span>extra_info<span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>解读</strong>：<ul>
<li><code>train_files</code>：这是<strong>课本</strong>。路径指向 GSM8K 的训练数据。</li>
<li><code>val_files</code>：这是<strong>模拟考卷</strong>。用来测试训练效果的数据。</li>
<li><code>prompt_key</code> 和 <code>response_key</code>：告诉程序，数据里哪一列是“题目”，哪一列是“答案”。</li>
</ul>
</li>
</ul>
<hr />
<h4>✅ 第五步：指定学生和学习强度（模型与训练参数）</h4>
<p>这里定义了谁来学，以及怎么学。</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>optim.lr<span class="o">=</span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>model.partial_pretrain<span class="o">=</span>Qwen/Qwen2.5-0.5B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trainer.total_epochs<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.micro_batch_size_per_gpu<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>解读</strong>：<ul>
<li><code>model.partial_pretrain</code>：<strong>指定学生</strong>。这里用的是阿里出的 Qwen 2.5 版本，参数量 0.5B（这是一个比较小的模型，跑得快）。</li>
<li><code>optim.lr=1e-4</code>：<strong>学习率</strong>。步子迈多大？这里设为 0.0001。</li>
<li><code>trainer.total_epochs=1</code>：<strong>学几遍</strong>？把课本从头到尾看 1 遍就结束。</li>
<li><code>micro_batch_size</code>：<strong>一口吃多少</strong>？一次给显卡喂 4 条数据。</li>
</ul>
</li>
</ul>
<hr />
<h4>✅ 第六步：安装“外挂”（LoRA 设置）</h4>
<p>这是最关键的“省钱”部分。</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>model.lora_rank<span class="o">=</span><span class="m">32</span><span class="se">\</span>
<span class="w">    </span>model.lora_alpha<span class="o">=</span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>model.target_modules<span class="o">=</span>all-linear
</code></pre></div>

<ul>
<li><strong>解读</strong>：<ul>
<li>如果不用这几行，就是在训练整个大模型（全量微调），非常吃显卡。</li>
<li>用了这几行，就是开启了 <strong>LoRA 模式</strong>。</li>
<li><code>lora_rank=32</code>：外挂芯片的容量大小。</li>
<li><code>target_modules=all-linear</code>：把外挂贴在哪里？这里是说贴在模型里所有的线性层（linear layers）上。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：如何执行它？</h3>
<p>看完上面的解释，你现在知道怎么运行它了吗？</p>
<p>假设你有 <strong>4张显卡</strong>，想把模型保存在 <strong>./my_model</strong> 目录下，你只需要在终端输入：</p>
<div class="codehilite"><pre><span></span><code>bash<span class="w"> </span>examples/sft/gsm8k/run_qwen_05_peft.sh<span class="w"> </span><span class="m">4</span><span class="w"> </span>./my_model
</code></pre></div>

<p><strong>发生了什么：</strong>
1.  脚本收到了 <code>4</code> 和 <code>./my_model</code>。
2.  它启动了 <code>torchrun</code>，调用了 4 张显卡。
3.  它加载了 Qwen-0.5B 模型。
4.  它给模型挂上了 Rank=32 的 LoRA 外挂。
5.  它读取 GSM8K 数学题，训练 1 轮。
6.  最后把训练好的“外挂参数”保存在 <code>./my_model</code> 里。</p>
<p>这样讲，是不是清晰一点了？</p>