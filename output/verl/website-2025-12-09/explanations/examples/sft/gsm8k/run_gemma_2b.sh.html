<h1>examples/sft/gsm8k/run_gemma_2b.sh</h1>
<p>这份脚本看起来像是一串“天书”，但其实它只是一个<strong>启动命令</strong>。它的核心目的是：<strong>教一个叫 Gemma 的 AI 模型做小学数学题（GSM8K 数据集）。</strong></p>
<p>为了让你听懂，我把这个脚本拆解成一个 <strong>“项目经理给机器下达的任务清单 (ToDo List)”</strong>。我们把计算机想象成一个实习生，你现在要按顺序给他布置任务。</p>
<hr />
<h3>任务清单：训练 AI 的六个步骤</h3>
<h4>步骤 1：检查“入职材料” (参数检查)</h4>
<p><strong>代码对应：</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$#</span><span class="s2">&quot;</span><span class="w"> </span>-lt<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Usage: ...&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>
<span class="nv">nproc_per_node</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">save_path</span><span class="o">=</span><span class="nv">$2</span>
<span class="nb">shift</span><span class="w"> </span><span class="m">2</span>
</code></pre></div>

<p><strong>解读：</strong>
这是在跟实习生（计算机）说：“听着，如果你没告诉我这<strong>两件事</strong>，我就不干了”：
1.  <strong><code>nproc_per_node</code></strong>：你要用几张显卡（GPU）来干活？
2.  <strong><code>save_path</code></strong>：干完活后的成果（模型文件）存到哪个文件夹？</p>
<p>如果这两个没给全，脚本就报错退出。<code>shift 2</code> 的意思是把这两个已经用掉的参数拿走，准备处理剩下的。</p>
<h4>步骤 2：启动“多显卡协作引擎” (Torchrun)</h4>
<p><strong>代码对应：</strong></p>
<div class="codehilite"><pre><span></span><code>torchrun<span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="nv">$nproc_per_node</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-m<span class="w"> </span>verl.trainer.fsdp_sft_trainer<span class="w"> </span><span class="se">\</span>
</code></pre></div>

<p><strong>解读：</strong>
*   <strong><code>torchrun</code></strong>：这是总指挥。因为训练大模型通常一张显卡搞不定，需要多张卡配合。这个命令就是让多张显卡连起来一起工作。
*   <strong><code>-m verl.trainer.fsdp_sft_trainer</code></strong>：这是告诉指挥官去运行哪个<strong>主程序</strong>。
    *   <code>verl</code>：这是代码库的名字。
    *   <code>SFT</code> (Supervised Fine-Tuning)：<strong>有监督微调</strong>。意思是“拿着有标准答案的教材去教模型”。
    *   <code>FSDP</code>：一种省显存的技术（不用深究，知道是用来省资源的就行）。</p>
<h4>步骤 3：指定“教材” (数据设置)</h4>
<p><strong>代码对应：</strong></p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>data.train_files<span class="o">=</span><span class="nv">$HOME</span>/data/gsm8k/train.parquet<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.val_files<span class="o">=</span><span class="nv">$HOME</span>/data/gsm8k/test.parquet<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.prompt_key<span class="o">=</span>extra_info<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.response_key<span class="o">=</span>extra_info<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.prompt_dict_keys<span class="o">=[</span><span class="s1">&#39;question&#39;</span><span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>+data.response_dict_keys<span class="o">=[</span><span class="s1">&#39;answer&#39;</span><span class="o">]</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<p><strong>解读：</strong>
这里是在告诉程序去哪里找书，以及怎么看书：
*   <strong>教材位置</strong>：训练集在 <code>gsm8k/train.parquet</code>，考试题（验证集）在 <code>gsm8k/test.parquet</code>。GSM8K 是一个很有名的小学数学题库。
*   <strong>阅读指南</strong>：数据文件像 Excel 表格一样有很多列。
    *   <code>prompt</code> (提示词/问题) 对应表格里的 <code>question</code> 列。
    *   <code>response</code> (回答/答案) 对应表格里的 <code>answer</code> 列。
    *   简单说就是：<strong>“把 Question 当作题目，把 Answer 当作标准答案，去学习怎么解题。”</strong></p>
<h4>步骤 4：指定“学生” (模型设置)</h4>
<p><strong>代码对应：</strong></p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>model.partial_pretrain<span class="o">=</span>google/gemma-2b-it<span class="w"> </span><span class="se">\</span>
</code></pre></div>

<p><strong>解读：</strong>
*   我们要训练哪个 AI？这里选的是 Google 开发的 <strong>Gemma-2b-it</strong>。
*   <code>2b</code> 意思是它有 20 亿个参数（属于轻量级小模型，跑得快）。
*   <code>it</code> 意思是它已经经过基础的指令训练（Instruction Tuned），比较听话，现在我们要让它专精数学。</p>
<h4>步骤 5：制定“课程表” (训练参数)</h4>
<p><strong>代码对应：</strong></p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>data.micro_batch_size_per_gpu<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trainer.default_local_dir<span class="o">=</span><span class="nv">$save_path</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trainer.total_epochs<span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<p><strong>解读：</strong>
*   <strong><code>micro_batch_size_per_gpu=4</code></strong>：每次看几道题？答：每张显卡一次看 4 道题。
*   <strong><code>total_epochs=2</code></strong>：这本教材要学几遍？答：从头到尾学 2 遍。
*   <strong><code>default_local_dir</code></strong>：学完存哪里？答：存到一开始指定的 <code>save_path</code>。</p>
<h4>步骤 6：写“日记” (日志记录)</h4>
<p><strong>代码对应：</strong></p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>trainer.project_name<span class="o">=</span>gsm8k-sft<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trainer.experiment_name<span class="o">=</span>gsm8k-sft-gemma-2b-it<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trainer.logger<span class="o">=</span><span class="s1">&#39;[&quot;console&quot;,&quot;wandb&quot;]&#39;</span><span class="w"> </span><span class="nv">$@</span>
</code></pre></div>

<p><strong>解读：</strong>
*   训练过程很长，需要记录进度。
*   <strong><code>project_name</code></strong>: 项目叫“GSM8K微调”。
*   <strong><code>logger</code></strong>: 记录员是谁？
    *   <code>console</code>: 直接在黑底白字的屏幕上打印进度。
    *   <code>wandb</code>: 发送到 "Weights &amp; Biases" 网站，生成漂亮的图表（损失函数曲线等）。</p>
<hr />
<h3>总结：这一大坨代码到底在干嘛？</h3>
<p>如果用一句人话总结这个脚本的功能：</p>
<blockquote>
<p><strong>“嘿，电脑！请调用 2 到 4 张显卡，加载 Google 的 Gemma-2B 模型，用 GSM8K 这个数学题库里的‘问题’和‘答案’对它进行 2 轮强化特训，训练好的新模型存到我指定的路径，顺便把训练过程画成图表发给我看。”</strong></p>
</blockquote>
<h3>你该怎么运行它？</h3>
<p>根据脚本开头的逻辑，你在终端里输入命令时，应该长这样：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 格式: sh 脚本名 显卡数量 保存路径</span>
bash<span class="w"> </span>examples/sft/gsm8k/run_gemma_2b.sh<span class="w"> </span><span class="m">4</span><span class="w"> </span>./my_saved_model
</code></pre></div>

<p>这表示：用 <strong>4</strong> 张卡训练，结果保存在 <strong>./my_saved_model</strong> 文件夹里。</p>