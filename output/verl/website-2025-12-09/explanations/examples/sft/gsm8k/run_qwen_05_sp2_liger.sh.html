<h1>examples/sft/gsm8k/run_qwen_05_sp2_liger.sh</h1>
<p>这份文件（<code>run_qwen_05_sp2_liger.sh</code>）其实就是一个<strong>“AI 训练启动清单”</strong>。</p>
<p>把它想象成你在命令一群工人（GPU显卡）去教一个学生（Qwen模型）做数学题（GSM8K数据集）。你需要告诉工人们：用几张卡、教材在哪里、学完的笔记存哪里、以及用什么特殊的教学技巧。</p>
<p>为了让你听懂，我把这个脚本拆解成一个 <strong>6步的 Todo List (任务清单)</strong>，然后一步步解释。</p>
<h3>📋 任务清单 (Todo List)</h3>
<ol>
<li><strong>【安检】</strong>：检查有没有告诉我必须要填的信息（显卡数量、保存路径）。</li>
<li><strong>【分配资源】</strong>：确认用多少个 GPU 来干活。</li>
<li><strong>【指定教材】</strong>：告诉 AI 去哪里读题（训练数据）和考试（验证数据）。</li>
<li><strong>【选定学生】</strong>：指定我们要训练哪个基础模型（Qwen 2.5）。</li>
<li><strong>【设定教学大纲】</strong>：设置学习率、批次大小等参数。</li>
<li><strong>【开启黑科技】</strong>：启用特殊的加速技术（Liger Kernel 和 Ulysses 并行）来省显存、提速度。</li>
</ol>
<hr />
<h3>🚀 逐步讲解 (Step-by-Step)</h3>
<h4>第一步：【安检】检查输入参数</h4>
<p>代码对应：</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$#</span><span class="s2">&quot;</span><span class="w"> </span>-lt<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Usage: ...&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>
<span class="nv">nproc_per_node</span><span class="o">=</span><span class="nv">$1</span>
<span class="nv">save_path</span><span class="o">=</span><span class="nv">$2</span>
<span class="nb">shift</span><span class="w"> </span><span class="m">2</span>
</code></pre></div>

<ul>
<li><strong>讲人话</strong>：脚本在说：“喂，如果不告诉我你要用几张显卡（<code>$1</code>）以及模型训练完存哪（<code>$2</code>），我就不干了（<code>exit 1</code>）。”</li>
<li><strong>操作</strong>：它把第一个参数记作“显卡数”，第二个记作“保存路径”，然后把这两个参数拿走（<code>shift 2</code>），准备处理剩下的参数。</li>
</ul>
<h4>第二步：【分配资源】召唤工头 (Torchrun)</h4>
<p>代码对应：</p>
<div class="codehilite"><pre><span></span><code>torchrun<span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="nv">$nproc_per_node</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-m<span class="w"> </span>verl.trainer.fsdp_sft_trainer<span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>讲人话</strong>：这是启动命令的核心。<ul>
<li><code>torchrun</code>: 这是一个“包工头”，负责管理所有的显卡。</li>
<li><code>--nproc_per_node</code>: 你刚才填的显卡数量（比如 8 张）。</li>
<li><code>-m verl.trainer.fsdp_sft_trainer</code>: 告诉电脑，我们要运行的 Python 程序是 <code>verl</code> 库里的 <code>fsdp_sft_trainer</code>。<ul>
<li><strong>SFT</strong> (Supervised Fine-Tuning): 意思是“有监督微调”，就是拿着标准答案教 AI。</li>
<li><strong>FSDP</strong>: 一种省显存的技术（把模型切碎了放在不同显卡里）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4>第三步：【指定教材】数据哪里来</h4>
<p>代码对应：</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>data.train_files<span class="o">=</span><span class="nv">$HOME</span>/data/gsm8k/train.parquet<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.val_files<span class="o">=</span><span class="nv">$HOME</span>/data/gsm8k/test.parquet<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.prompt_key<span class="o">=</span>extra_info<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.response_key<span class="o">=</span>extra_info<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>...
</code></pre></div>

<ul>
<li><strong>讲人话</strong>：<ul>
<li><strong>教材 (gsm8k)</strong>: GSM8K 是一个经典的小学数学应用题数据集。</li>
<li><strong>Key (键名)</strong>: 告诉程序，数据文件里的哪一列是“题目”，哪一列是“答案”。这里看起来数据处理比较特殊，都在 <code>extra_info</code> 里，或者通过后面的字典键值对来映射。</li>
</ul>
</li>
</ul>
<h4>第三步补丁：具体教什么</h4>
<p>代码对应：</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>data.prompt_dict_keys<span class="o">=[</span><span class="s1">&#39;question&#39;</span><span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>+data.response_dict_keys<span class="o">=[</span><span class="s1">&#39;answer&#39;</span><span class="o">]</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>讲人话</strong>：明确指出，输入给 AI 的是 <code>question</code>（问题），希望 AI 输出的是 <code>answer</code>（答案）。</li>
</ul>
<h4>第四步：【选定学生】模型是谁</h4>
<p>代码对应：</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>model.partial_pretrain<span class="o">=</span>Qwen/Qwen2.5-0.5B-Instruct<span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>讲人话</strong>：我们要训练的“底座模型”是 <strong>Qwen 2.5</strong> (通义千问)，大小是 <strong>0.5B</strong> (0.5亿参数)。这是一个非常小巧的模型，跑得快，适合做实验。</li>
</ul>
<h4>第五步：【设定教学大纲】怎么学</h4>
<p>代码对应：</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>optim.lr<span class="o">=</span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.micro_batch_size<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>trainer.default_local_dir<span class="o">=</span><span class="nv">$save_path</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>讲人话</strong>：<ul>
<li><code>lr=1e-4</code>: 学习率。学得太快容易“走火入魔”，学得太慢“没有效果”。</li>
<li><code>batch_size=4</code>: 一次给它看 4 道题，看完再总结规律。</li>
<li><code>save_path</code>: 学完的模型存在哪（对应第一步你输入的路径）。</li>
</ul>
</li>
</ul>
<h4>第六步：【开启黑科技】重点来了！</h4>
<p>这部分是这个脚本标题里 <code>sp2</code> 和 <code>liger</code> 的来源，也是最难懂的部分。</p>
<p><strong>1. Liger Kernel (里格尔内核)</strong></p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span>model.use_liger<span class="o">=</span>True<span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>解释</strong>：<code>Liger</code> 是一个专门优化过的底层计算库（Kernel）。</li>
<li><strong>比喻</strong>：本来 AI 算数是用“小学竖式”慢慢算，开启 <code>Liger</code> 就像给了它一个计算器，算得更快，而且更省草稿纸（显存）。</li>
</ul>
<p><strong>2. Ulysses Sequence Parallelism (尤利西斯序列并行)</strong></p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">ulysses_sequence_parallel_size</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<ul>
<li><strong>解释</strong>：这是文件名里 <code>sp2</code> 的意思 (Sequence Parallel = 2)。</li>
<li><strong>比喻</strong>：假设有一篇<strong>超级长</strong>的文章（比如 10万字）要 AI 读。单个显卡脑子（显存）装不下这么长的文章。<ul>
<li><code>sp2</code> 的意思是：把这篇文章<strong>切成 2 段</strong>。</li>
<li>显卡 A 读前半段，显卡 B 读后半段。</li>
<li>它们之间通过高速互联（NVLink）交换信息，假装自己读了全文。</li>
<li>这允许你训练<strong>超长上下文</strong>的模型。</li>
</ul>
</li>
</ul>
<p><strong>3. 去除填充 (Remove Padding)</strong></p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">use_remove_padding</span><span class="o">=</span><span class="nb">true</span>
</code></pre></div>

<ul>
<li><strong>解释</strong>：训练数据长短不一，通常短的句子后面要补 0 (Padding) 凑长度。这个选项是把这些没用的 0 去掉，不让显卡浪费时间算 0。</li>
</ul>
<h3>总结</h3>
<p>这个脚本在做一件事：</p>
<blockquote>
<p>使用 <strong>Verl 框架</strong>，开启 <strong>Liger 优化</strong> 和 <strong>2路序列并行 (SP2)</strong>，在一台机器上利用多张显卡，对 <strong>Qwen 2.5 (0.5B)</strong> 这个小模型进行 <strong>GSM8K 数学题</strong> 的 <strong>微调训练 (SFT)</strong>。</p>
</blockquote>
<p>如果你要运行它，假设你有 8 张卡，想存到 <code>/tmp/model</code>，你就输入：
<code>bash examples/sft/gsm8k/run_qwen_05_sp2_liger.sh 8 /tmp/model</code></p>