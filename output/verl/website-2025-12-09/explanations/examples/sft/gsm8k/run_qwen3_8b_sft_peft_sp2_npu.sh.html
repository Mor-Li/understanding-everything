<h1>examples/sft/gsm8k/run_qwen3_8b_sft_peft_sp2_npu.sh</h1>
<p>没问题，看到这种脚本文件（Shell Script）确实容易让人头大，因为它混合了Linux命令、Python启动命令以及一大堆深度学习的专业参数。</p>
<p>这就好比你看一张<strong>极其复杂的做菜配方</strong>，上面写满了“300度火候”、“分子料理机设置”等术语。</p>
<p>我们把这个文件拆解成一个 <strong>5步走的 ToDo List</strong>，通过完成这5个任务，你就能完全理解它在干什么。</p>
<hr />
<h3>任务清单 (ToDo List)</h3>
<ol>
<li><strong>任务一：搞清楚我们在干什么（宏观目标）</strong></li>
<li><strong>任务二：看懂脚本的“开场白”（基础设置）</strong></li>
<li><strong>任务三：准备“食材”（数据配置）</strong></li>
<li><strong>任务四：设定“烹饪方法”（模型与训练策略）</strong></li>
<li><strong>任务五：搞懂“黑科技”（硬件加速与并行）</strong></li>
</ol>
<hr />
<h3>详细拆解</h3>
<h4>✅ 任务一：搞清楚我们在干什么（宏观目标）</h4>
<p>在看代码前，先看文件名：<code>run_qwen3_8b_sft_peft_sp2_npu.sh</code>。这其实已经剧透了所有内容：</p>
<ul>
<li><strong>Qwen3-8B</strong>: 我们要训练的模型是通义千问第三代（80亿参数版本）。</li>
<li><strong>SFT (Supervised Fine-Tuning)</strong>: 我们在做“有监督微调”。简单说，就是给模型看一堆“问题+标准答案”，让它学会照着做。</li>
<li><strong>PEFT</strong>: 我们不训练整个大模型（太贵太慢），而是用“参数高效微调”技术，只训练模型的一小部分。</li>
<li><strong>NPU</strong>: 我们用的不是英伟达显卡（GPU），而是华为昇腾芯片（NPU）。</li>
</ul>
<p><strong>结论</strong>：这个脚本是用来启动一个训练任务的，目的是教Qwen3-8B模型做数学题（GSM8K是数学数据集）。</p>
<hr />
<h4>✅ 任务二：看懂脚本的“开场白”（基础设置）</h4>
<p>脚本的前几行是标准的Linux操作，目的是接收你输入的命令参数。</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 开启调试模式，执行时会打印每一行命令</span>
<span class="nb">set</span><span class="w"> </span>-x

<span class="c1"># 检查你有没有输入足够的参数。如果少于2个，就报错告诉你怎么用</span>
<span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$#</span><span class="s2">&quot;</span><span class="w"> </span>-lt<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Usage: run_qwen3_8b_sft_peft_sp2_npu.sh &lt;nproc_per_node&gt; &lt;save_path&gt; [other_configs...]&quot;</span>
<span class="w">    </span><span class="nb">exit</span><span class="w"> </span><span class="m">1</span>
<span class="k">fi</span>

<span class="c1"># 获取第1个参数：每台机器用几张卡（比如 8）</span>
<span class="nv">nproc_per_node</span><span class="o">=</span><span class="nv">$1</span>
<span class="c1"># 获取第2个参数：训练好的模型保存到哪里</span>
<span class="nv">save_path</span><span class="o">=</span><span class="nv">$2</span>

<span class="c1"># 把前两个参数“移走”，剩下的参数留给后面用</span>
<span class="nb">shift</span><span class="w"> </span><span class="m">2</span>
</code></pre></div>

<p><strong>人话翻译</strong>：
当你运行这个脚本时，你必须告诉它：“我要用几张卡”和“结果存哪儿”。</p>
<hr />
<h4>✅ 任务三：准备“食材”（数据配置）</h4>
<p>接下来是核心命令 <code>torchrun</code>，它负责启动Python程序。我们先看关于<strong>数据</strong>的部分：</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="c1"># 训练数据文件（Parquet格式，一种高效表格文件）</span>
<span class="w">    </span>data.train_files<span class="o">=</span><span class="nv">$HOME</span>/data/gsm8k/train.parquet<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># 验证数据文件（考试题）</span>
<span class="w">    </span>data.val_files<span class="o">=</span><span class="nv">$HOME</span>/data/gsm8k/test.parquet<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># 告诉程序，数据里的“问题”藏在哪个列里</span>
<span class="w">    </span>data.prompt_key<span class="o">=</span>extra_info<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>data.prompt_dict_keys<span class="o">=[</span><span class="s1">&#39;question&#39;</span><span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># 告诉程序，数据里的“答案”藏在哪个列里</span>
<span class="w">    </span>data.response_key<span class="o">=</span>extra_info<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>+data.response_dict_keys<span class="o">=[</span><span class="s1">&#39;answer&#39;</span><span class="o">]</span><span class="w"> </span><span class="se">\</span>
</code></pre></div>

<p><strong>人话翻译</strong>：
*   <strong>食材来源</strong>：去 <code>$HOME/data/gsm8k/</code> 目录下拿数据。
*   <strong>切菜方式</strong>：数据里有一列叫 <code>extra_info</code>，里面是个字典，请把 <code>question</code> 拿出来当题目，把 <code>answer</code> 拿出来当标准答案。</p>
<hr />
<h4>✅ 任务四：设定“烹饪方法”（模型与训练策略）</h4>
<p>这里定义了我们如何训练模型，以及怎么省钱（PEFT/LoRA）。</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="c1"># 使用的基础模型是 Qwen3-8B</span>
<span class="w">    </span>model.partial_pretrain<span class="o">=</span>Qwen/Qwen3-8B<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># 学习率：学得太快容易忘，学得太慢如果不动。这里是 0.0001</span>
<span class="w">    </span>optim.lr<span class="o">=</span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># 训练几轮？把所有书看2遍</span>
<span class="w">    </span>trainer.total_epochs<span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># ------ 下面是省钱秘籍 (LoRA) ------</span>
<span class="w">    </span><span class="c1"># LoRA的秩（Rank）：数字越小，训练参数越少，越省显存</span>
<span class="w">    </span>model.lora_rank<span class="o">=</span><span class="m">32</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># LoRA的缩放系数</span>
<span class="w">    </span>model.lora_alpha<span class="o">=</span><span class="m">16</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># 对模型里所有的线性层都进行微调（效果通常更好）</span>
<span class="w">    </span>model.target_modules<span class="o">=</span>all-linear<span class="w"> </span><span class="se">\</span>
</code></pre></div>

<p><strong>人话翻译</strong>：
我们不重新造一个大脑，而是拿现成的 <code>Qwen3-8B</code> 大脑，在上面贴一些“便利贴”（LoRA模块）。我们只修改这些便利贴上的数字，让它学会做数学题。这样做显存占用极小，速度快。</p>
<hr />
<h4>✅ 任务五：搞懂“黑科技”（硬件加速与并行）</h4>
<p>这是最难懂的部分，也是为了让大模型能跑起来的关键技术。</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="c1"># 每次给每张卡喂多少数据</span>
<span class="w">    </span>data.micro_batch_size_per_gpu<span class="o">=</span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># 策略：FSDP (Fully Sharded Data Parallel)</span>
<span class="w">    </span><span class="c1"># 意思把模型切碎了放在不同卡上，防止单张卡显存不够</span>
<span class="w">    </span>model.strategy<span class="o">=</span>fsdp<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># ------ 序列并行 (Sequence Parallelism) ------</span>
<span class="w">    </span><span class="c1"># 如果一句话特别长，一张卡处理不过来，就切成2段，2张卡合作处理</span>
<span class="w">    </span><span class="nv">ulysses_sequence_parallel_size</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># 去除填充：一种加速技巧，不计算无效的空格</span>
<span class="w">    </span><span class="nv">use_remove_padding</span><span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="c1"># 关键点：指定设备类型为 NPU（华为昇腾）</span>
<span class="w">    </span>trainer.device<span class="o">=</span>npu
</code></pre></div>

<p><strong>人话翻译</strong>：
*   <strong>FSDP</strong>: 就像拼图一样，模型太大了，一张卡放不下，所以把模型拆散，每张卡只拿一部分，计算的时候再凑在一起。
*   <strong>Ulysses (尤利西斯并行)</strong>: 处理超长文本的技术。比如一本书太厚，一个人读太慢，这里设置 <code>size=2</code>，表示把长文本切两半，两个人同时读。
*   <strong>NPU</strong>: 明确告诉程序，别找NVIDIA的驱动，去调用华为的CANN库。</p>
<hr />
<h3>总结</h3>
<p>现在你再看这个文件，它的逻辑就是：</p>
<ol>
<li><strong>检查命令</strong>：你告诉我用几张卡了吗？</li>
<li><strong>启动程序</strong>：用 <code>torchrun</code> 叫醒所有 NPU 芯片。</li>
<li><strong>加载数据</strong>：去读 GSM8K 数学题。</li>
<li><strong>加载模型</strong>：读入 Qwen3-8B，并挂载 LoRA 插件（省资源模式）。</li>
<li><strong>开启加速</strong>：打开 FSDP（切分模型）和 Ulysses（切分长文本），在华为 NPU 上飞快地跑起来。</li>
</ol>
<p>这就是这个脚本的全部含义！它是指挥整个训练舰队的“作战指令书”。</p>