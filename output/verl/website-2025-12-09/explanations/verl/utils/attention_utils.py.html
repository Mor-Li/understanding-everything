<h1>verl/utils/attention_utils.py</h1>
<p>è¿™ä»½ä»£ç ä¹ä¸€çœ‹å…¨æ˜¯å‡½æ•°å®šä¹‰å’Œå¥‡æ€ªçš„å¯¼å…¥ï¼Œä½†å®é™…ä¸Šå®ƒçš„é€»è¾‘éå¸¸ç®€å•ã€‚å®ƒå°±åƒæ˜¯ä¸€ä¸ª<strong>â€œä¸‡èƒ½è½¬æ¥å¤´â€</strong>ã€‚</p>
<p>ä¸ºäº†è®©ä½ å½»åº•ç†è§£ï¼Œæˆ‘åˆ¶å®šäº†ä¸€ä¸ª <strong>4æ­¥èµ°çš„ Task Listï¼ˆä»»åŠ¡æ¸…å•ï¼‰</strong>ã€‚æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥æ‹†è§£ï¼š</p>
<h3>ğŸ“ Task 1: ç†è§£èƒŒæ™¯â€”â€”ä¸ºä»€ä¹ˆè¦å†™è¿™ä¸ªæ–‡ä»¶ï¼Ÿ</h3>
<p><strong>èƒŒæ™¯ï¼š</strong>
ç°åœ¨çš„AIæ¨¡å‹éœ€è¦åœ¨ä¸åŒçš„ç¡¬ä»¶ä¸Šè¿è¡Œã€‚
1.  <strong>NVIDIA æ˜¾å¡ (CUDA):</strong> æœ€å¸¸ç”¨çš„ï¼Œç”Ÿæ€æœ€æˆç†Ÿã€‚
2.  <strong>åä¸º æ˜‡è…¾èŠ¯ç‰‡ (NPU):</strong> å›½äº§ç®—åŠ›ï¼Œç”Ÿæ€æ­£åœ¨å»ºè®¾ä¸­ã€‚</p>
<p><strong>ç—›ç‚¹ï¼š</strong>
è¿™ä¸¤å®¶ç¡¬ä»¶çš„åº•å±‚ä»£ç åº“æ˜¯ä¸ä¸€æ ·çš„ã€‚
*   åœ¨ NVIDIA ä¸Šï¼Œä½ æƒ³å¤„ç† Attentionï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰çš„æ•°æ®ï¼Œä½ å¾—è°ƒç”¨ <code>flash_attn</code> è¿™ä¸ªåº“ã€‚
*   åœ¨ åä¸º NPU ä¸Šï¼Œä½ å¾—è°ƒç”¨ <code>transformers</code> æˆ– <code>verl</code> è‡ªå·±å°è£…çš„ NPU ä¸“ç”¨åº“ã€‚</p>
<p><strong>å¦‚æœä¸å†™è¿™ä¸ªæ–‡ä»¶ï¼š</strong>
ä½ çš„ä¸»ä»£ç é‡Œå°±ä¼šå……æ»¡è¿™ç§ä¸‘é™‹çš„åˆ¤æ–­ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="n">is_cuda</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">flash_attn</span>
    <span class="n">flash_attn</span><span class="o">.</span><span class="n">pad_input</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">is_npu</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">npu_utils</span>
    <span class="n">npu_utils</span><span class="o">.</span><span class="n">pad_input</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</code></pre></div>

<p>è¿™å¤ªéº»çƒ¦äº†ã€‚</p>
<p><strong>âœ… Task 1 ç»“è®ºï¼š</strong> è¿™ä¸ªæ–‡ä»¶çš„ç›®çš„å°±æ˜¯<strong>å±è”½ç¡¬ä»¶å·®å¼‚</strong>ï¼Œè®©å¤–é¢çš„ä»£ç ä¸éœ€è¦å…³å¿ƒç°åœ¨åˆ°åº•æ˜¯ç”¨ NVIDIA è¿˜æ˜¯åä¸º NPUã€‚</p>
<hr />
<h3>ğŸ“ Task 2: ç†è§£æ ¸å¿ƒåŠŸèƒ½â€”â€”è¿™äº›å‡½æ•°æ˜¯å¹²å˜›çš„ï¼Ÿ</h3>
<p>æ–‡ä»¶ä¸­åå¤å‡ºç°äº†å››ä¸ªåå­—ï¼š
1.  <code>pad_input</code> (å¡«å……è¾“å…¥)
2.  <code>unpad_input</code> (å»é™¤å¡«å……)
3.  <code>index_first_axis</code> (ç´¢å¼•ç¬¬ä¸€ç»´åº¦)
4.  <code>rearrange</code> (é‡æ’æ•°æ®)</p>
<p><strong>è§£é‡Šï¼š</strong>
è¿™äº›éƒ½æ˜¯ä¸ºäº†é…åˆ <strong>Flash Attention</strong>ï¼ˆä¸€ç§åŠ é€Ÿè®¡ç®—çš„æŠ€æœ¯ï¼‰ä½¿ç”¨çš„è¾…åŠ©å·¥å…·ã€‚
*   æ™®é€šçš„ Attention è®¡ç®—å¯ä»¥æ¥å—é•¿çŸ­ä¸ä¸€çš„å¥å­è¡¥é›¶ï¼ˆPaddingï¼‰åçš„çŸ©é˜µã€‚
*   <strong>Flash Attention</strong> ä¸ºäº†æè‡´çš„é€Ÿåº¦ï¼Œé€šå¸¸éœ€è¦æŠŠè¿™äº›è¡¥é›¶å»æ‰ï¼ˆUnpadï¼‰ï¼ŒæŠŠæ‰€æœ‰æœ‰æ•ˆæ•°æ®æ‹¼æˆä¸€æ¡é•¿é¾™æ¥è®¡ç®—ï¼Œç®—å®Œå†æ‹¼å›å»ï¼ˆPadï¼‰ã€‚</p>
<p><strong>âœ… Task 2 ç»“è®ºï¼š</strong> è¿™å››ä¸ªå‡½æ•°æ˜¯åš Flash Attention åŠ é€Ÿæ—¶å¿…ä¸å¯å°‘çš„<strong>æ•°æ®é¢„å¤„ç†/åå¤„ç†å·¥å…·</strong>ã€‚</p>
<hr />
<h3>ğŸ“ Task 3: æ‹†è§£ä»£ç é€»è¾‘â€”â€”å®ƒæ˜¯æ€ä¹ˆâ€œè‡ªåŠ¨åˆ‡æ¢â€çš„ï¼Ÿ</h3>
<p>ç°åœ¨æˆ‘ä»¬çœ‹ä»£ç çš„æ ¸å¿ƒéƒ¨åˆ† <code>_get_attention_functions</code> å‡½æ•°ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">_get_attention_functions</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]:</span>
    <span class="c1"># 1. æ£€æŸ¥å½“å‰æ˜¯ä»€ä¹ˆç¡¬ä»¶</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">verl.utils.device</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_cuda_available</span><span class="p">,</span> <span class="n">is_npu_available</span>

    <span class="c1"># ... (çœç•¥éƒ¨åˆ†ä»£ç )</span>

    <span class="c1"># 2. å¦‚æœæ˜¯ NVIDIA (CUDA)</span>
    <span class="k">if</span> <span class="n">is_cuda_available</span><span class="p">:</span>
        <span class="c1"># ä» flash_attn åº“é‡Œå¯¼å…¥è¿™äº›åŠŸèƒ½</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">flash_attn.bert_padding</span><span class="w"> </span><span class="kn">import</span> <span class="n">index_first_axis</span><span class="p">,</span> <span class="n">pad_input</span><span class="p">,</span> <span class="n">rearrange</span><span class="p">,</span> <span class="n">unpad_input</span>

    <span class="c1"># 3. å¦‚æœæ˜¯ åä¸º (NPU)</span>
    <span class="k">elif</span> <span class="n">is_npu_available</span><span class="p">:</span>
        <span class="c1"># ä» NPU ä¸“ç”¨å·¥å…·é‡Œå¯¼å…¥è¿™äº›åŠŸèƒ½</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">verl.utils.npu_flash_attn_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">index_first_axis</span><span class="p">,</span> <span class="n">pad_input</span><span class="p">,</span> <span class="n">rearrange</span><span class="p">,</span> <span class="n">unpad_input</span>

    <span class="c1"># 4. è¿”å›æ‰¾åˆ°çš„å‡½æ•°</span>
    <span class="k">return</span> <span class="n">_index_first_axis</span><span class="p">,</span> <span class="n">_pad_input</span><span class="p">,</span> <span class="n">_rearrange</span><span class="p">,</span> <span class="n">_unpad_input</span>
</code></pre></div>

<p><strong>é€»è¾‘åˆ†æï¼š</strong>
è¿™å°±å¥½æ¯”ä¸€ä¸ª<strong>å‰å°æ¥å¾…å‘˜</strong>ã€‚å½“ä½ éœ€è¦â€œæ‰“æ‰«æˆ¿é—´â€ï¼ˆè°ƒç”¨å‡½æ•°ï¼‰æ—¶ï¼Œæ¥å¾…å‘˜å…ˆçœ‹ä½ åœ¨å“ªä¸ªé…’åº—ï¼ˆæ£€æŸ¥ç¡¬ä»¶ï¼‰ï¼š
*   å¦‚æœåœ¨å¸Œå°”é¡¿ï¼ˆCUDAï¼‰ï¼Œå°±å«å¸Œå°”é¡¿çš„ä¿æ´é˜¿å§¨ã€‚
*   å¦‚æœåœ¨å¦‚å®¶ï¼ˆNPUï¼‰ï¼Œå°±å«å¦‚å®¶çš„ä¿æ´é˜¿å§¨ã€‚</p>
<p><strong>âœ… Task 3 ç»“è®ºï¼š</strong> ä»£ç ä½¿ç”¨äº†<strong>åŠ¨æ€å¯¼å…¥ï¼ˆDynamic Importï¼‰</strong>ã€‚ç¨‹åºè¿è¡Œæ—¶æ‰å†³å®šå»å“ªé‡Œæ‰¾è¿™äº›å‡½æ•°ï¼Œè€Œä¸æ˜¯å†™æ­»çš„ã€‚</p>
<hr />
<h3>ğŸ“ Task 4: çœ‹æ‡‚å¤–éƒ¨æ¥å£â€”â€”å¤–é¢çš„äººæ€ä¹ˆç”¨ï¼Ÿ</h3>
<p>æœ€åçœ‹æ–‡ä»¶åº•éƒ¨çš„è¿™å‡ ä¸ªå‡½æ•°ï¼ˆä»¥ <code>pad_input</code> ä¸ºä¾‹ï¼‰ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">pad_input</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unified entry point... (ç»Ÿä¸€çš„å…¥å£ç‚¹)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 1. é—®æ¥å¾…å‘˜è¦çœŸæ­£çš„æ‰§è¡Œå‡½æ•° (func)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">_get_attention_functions</span><span class="p">()</span>

    <span class="c1"># 2. æ‰§è¡Œé‚£ä¸ªå‡½æ•°ï¼Œå¹¶æŠŠå‚æ•°ä¼ è¿‡å»</span>
    <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>

<p><strong>é€»è¾‘åˆ†æï¼š</strong>
è¿™å°±æ˜¯åœ¨è¿™ä¸ªæ–‡ä»¶é‡Œæš´éœ²ç»™ç”¨æˆ·çš„â€œå‡â€å‡½æ•°ï¼ˆä»£ç†ï¼‰ã€‚
å¤–éƒ¨çš„å¼€å‘è€…åªéœ€è¦å†™ï¼š
<code>verl.utils.attention_utils.pad_input(data)</code></p>
<p>ä»–<strong>ä¸éœ€è¦çŸ¥é“</strong>åº•å±‚åˆ°åº•è°ƒç”¨äº† <code>flash_attn.bert_padding.pad_input</code> è¿˜æ˜¯ <code>npu_flash_attn.pad_input</code>ã€‚è¿™ä¸ªâ€œå‡â€å‡½æ•°ä¼šè‡ªåŠ¨å¸®ä»–æ‰¾åˆ°æ­£ç¡®çš„â€œçœŸâ€å‡½æ•°å¹¶æ‰§è¡Œã€‚</p>
<p><strong>âœ… Task 4 ç»“è®ºï¼š</strong> è¿™æ˜¯ä¸€ä¸ª<strong>å°è£…å±‚ï¼ˆWrapperï¼‰</strong>ã€‚å®ƒç»Ÿä¸€äº†æ¥å£ï¼Œè®©å¤–éƒ¨è°ƒç”¨å˜å¾—æå…¶ç®€å•ã€‚</p>
<hr />
<h3>ğŸ’¡ æ€»ç»“ï¼ˆHuman Translationï¼‰</h3>
<p>å¦‚æœè®©æˆ‘ç”¨ä¸€å¥è¯è®²å®Œè¿™ä¸ªæ–‡ä»¶ï¼š</p>
<blockquote>
<p><strong>è¿™æ˜¯ä¸€ä¸ªâ€œé€‚é…å™¨â€ï¼Œå®ƒèƒ½è‡ªåŠ¨æ£€æµ‹ä½ æ˜¯åœ¨ç”¨ NVIDIA æ˜¾å¡è¿˜æ˜¯åä¸º NPUï¼Œç„¶åè‡ªåŠ¨å¸®ä½ åŠ è½½å¯¹åº”çš„ Flash Attention æ•°æ®å¤„ç†å·¥å…·ï¼Œè¿™æ ·ä½ åœ¨å†™ä¸»ç¨‹åºæ—¶å°±ä¸ç”¨æ“å¿ƒç¡¬ä»¶å…¼å®¹æ€§é—®é¢˜äº†ã€‚</strong></p>
</blockquote>