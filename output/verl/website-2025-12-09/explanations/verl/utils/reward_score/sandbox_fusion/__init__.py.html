<h1>verl/utils/reward_score/sandbox_fusion/<strong>init</strong>.py</h1>
<p>这份代码其实就是一个<strong>“全自动的代码阅卷老师”</strong>。</p>
<p>它的核心任务只有一件事：<strong>拿着模型写出来的代码，去跑一遍测试用例，然后给这代码打个分（0到1分）。</strong></p>
<p>为了让你彻底看懂，我把它拆解成一个<strong>包含5个步骤的 Task Todo List</strong>。我们假装你是这个“阅卷老师”，你需要按顺序执行以下任务：</p>
<hr />
<h3>📝 阅卷任务清单 (Task Todo List)</h3>
<h4>✅ Task 1: 把“废话”去掉，只留下核心代码 (代码提取)</h4>
<p><strong>代码位置:</strong> <code>solution = completion ...</code> 到 <code>return 0.0</code> 之前
<strong>发生了什么:</strong>
模型生成的答案（<code>completion</code>）通常包含很多解释，比如“这是你要的代码...”。
*   <strong>你的动作:</strong> 寻找 Markdown 格式的标记（<code>```python</code> 或 <code>```</code>）。
*   <strong>目的:</strong> 把包裹在标记里的纯代码抠出来。如果找不到代码块，直接判 0 分，因为没法运行。</p>
<h4>✅ Task 2: 检查“考题”是否完整 (解析测试用例)</h4>
<p><strong>代码位置:</strong> <code>try: ... if not isinstance(test_cases, dict): ...</code>
<strong>发生了什么:</strong>
你需要用来测试代码的题目（<code>test_cases</code>）传进来可能是个字符串。
*   <strong>你的动作:</strong>
    1.  把它转换成字典（JSON对象）。
    2.  检查里面有没有 <code>inputs</code> (输入) 和 <code>outputs</code> (预期输出)。
    3.  有时候题目格式是 <code>assert_case</code> (断言测试)，也要把它转化成标准格式。
*   <strong>目的:</strong> 确保你有合法的题目去考这段代码，如果题目格式错了，没法考，报错并给 0 分。</p>
<h4>✅ Task 3: 把代码扔进“沙盒”里跑一遍 (安全执行)</h4>
<p><strong>代码位置:</strong> <code>res_list, metadata_list = check_correctness(...)</code>
<strong>发生了什么:</strong>
你不能在自己的电脑上直接跑别人的代码（万一有病毒或者死循环怎么办？）。
*   <strong>你的动作:</strong> 把<strong>提取出的代码</strong>和<strong>测试题目</strong>，打包发送给一个远程的“沙盒服务”（<code>sandbox_fusion_url</code>）。
*   <strong>目的:</strong> 让那个远程服务去跑代码，然后告诉你每一道题是 <code>True</code> (通过) 还是 <code>False</code> (没通过/报错)。</p>
<h4>✅ Task 4: 算出最终分数 (计算得分)</h4>
<p><strong>代码位置:</strong> <code>if continuous: ... else: ...</code>
<strong>发生了什么:</strong>
拿到远程沙盒返回的结果列表（比如 <code>[True, True, False, True]</code>）后，你要算分。
这里有两种算分模式：
1.  <strong>Continuous (连续模式):</strong> 只看前 N 个（比如前10个）例子，算出通过率。
2.  <strong>Normal (普通模式):</strong> 看所有测试用例，算出 <code>通过数量 / 总数量</code>。
*   <strong>目的:</strong> 得出一个 <code>0.0</code> 到 <code>1.0</code> 之间的浮点数，这就是给模型的“奖励分”。</p>
<h4>✅ Task 5: 应对突发状况 (错误处理)</h4>
<p><strong>代码位置:</strong> <code>except Exception as e: ...</code>
<strong>发生了什么:</strong>
整个过程中可能会出 bug（比如网络断了、JSON解析失败、沙盒挂了）。
*   <strong>你的动作:</strong> 如果中间任何一步崩了，不要让整个程序死机。捕获错误，打印日志 (<code>logger.error</code>)，然后无奈地返回 <code>0.0</code> 分。
*   <strong>目的:</strong> 保证训练流程的稳定性，就算阅卷失败，也要优雅地结束。</p>
<hr />
<h3>总结一下</h3>
<p>这个文件的逻辑就是：</p>
<ol>
<li><strong>输入</strong>: 模型写的文本 (<code>completion</code>) + 考题 (<code>test_cases</code>)。</li>
<li><strong>清洗</strong>: 只要 Python 代码。</li>
<li><strong>外包</strong>: 扔给 <code>check_correctness</code> 去远程跑代码。</li>
<li><strong>打分</strong>: 跑对了几个，就给几分。</li>
<li><strong>输出</strong>: 分数 (<code>score</code>) + 详细日志 (<code>metadata</code>)。</li>
</ol>
<p>这在强化学习（RL）中非常重要，因为模型需要这个“分数”来知道自己写代码写得对不对，从而不断进化。</p>