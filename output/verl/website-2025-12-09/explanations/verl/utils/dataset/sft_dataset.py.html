<h1>verl/utils/dataset/sft_dataset.py</h1>
<p>这份代码是一个用于 <strong>大模型有监督微调（SFT, Supervised Fine-Tuning）</strong> 的数据处理脚本。</p>
<p>简单来说，它的作用是：<strong>把原始的问答表格文件（Parquet格式），转换成模型能看懂的数字张量（Tensor），并告诉模型“哪部分是你需要学习的答案，哪部分是题目”。</strong></p>
<p>为了让你听懂，我把这个脚本看作一个<strong>“流水线工人”</strong>，下面是这位工人的 <strong>任务清单 (To-Do List)</strong>。</p>
<hr />
<h3>📋 SFT 数据处理工人的 To-Do List</h3>
<h4>第一阶段：准备原材料（初始化与读取）</h4>
<ol>
<li><strong>接到任务单（Config）</strong>：搞清楚我要读哪个文件？最大长度限制是多少？要不要打乱数据？</li>
<li><strong>把原材料搬回家（Download）</strong>：把 Parquet 数据文件从远程（如 HDFS）下载到本地。</li>
<li><strong>拆箱验货（Read &amp; Filter）</strong>：<ul>
<li>用 Pandas 读取文件。</li>
<li>如果数据太多，根据设置只取一部分（<code>max_samples</code>）。</li>
<li>把数据里的“问题（Prompt）”和“答案（Response）”两列分别提取出来，存进内存。</li>
</ul>
</li>
</ol>
<h4>第二阶段：按需加工（<code>__getitem__</code> 核心逻辑）</h4>
<p><em>当训练程序说：“给我第 N 条数据”时，工人开始实时执行以下操作：</em></p>
<ol>
<li><strong>包装修饰（Apply Chat Template）</strong>：给原始问题加上对话格式（比如加上 <code>&lt;|user|&gt;</code> 标签），让模型知道这是用户说的话。</li>
<li><strong>翻译成密码（Tokenize）</strong>：把文字（问题和答案）分别转换成模型能读懂的数字 ID（Token IDs）。</li>
<li><strong>拼接组装（Concat）</strong>：把 <code>[问题 IDs]</code> 和 <code>[答案 IDs]</code> 拼成一条长长的序列。</li>
<li><strong>统一规格（Pad or Truncate）</strong>：<ul>
<li>如果太短：后面补 0（Padding），凑够最大长度。</li>
<li>如果太长：根据设置切掉多余的（截断）。</li>
</ul>
</li>
<li><strong>画重点（Masking - 最关键的一步）</strong>：<ul>
<li>告诉模型：<strong>“只许背答案，不许背题目！”</strong>（计算 Loss 时屏蔽掉 Prompt 部分）。</li>
</ul>
</li>
</ol>
<hr />
<h3>🔍 逐步代码详解（中英文对照）</h3>
<p>现在我们对照代码，一步步看它是怎么完成上面这个 List 的。</p>
<h4>1. 准备原材料 (<code>__init__</code> 和 <code>_read_files_and_tokenize</code>)</h4>
<p>这部分代码在类初始化时运行一次，目的是把数据加载到内存里。</p>
<ul>
<li><strong>设定配置</strong>：
    <code>python
    self.prompt_key = config.get("prompt_key", "prompt") # 哪一列是问题？
    self.response_key = config.get("response_key", "response") # 哪一列是答案？</code></li>
<li>
<p><strong>读取与抽样</strong>：
    ```python
    # 读取 Parquet 文件
    dataframe = pd.read_parquet(parquet_file)</p>
<h1>如果需要打乱（Shuffle）或者只取前 N 条</h1>
<p>if self.max_samples &gt; 0 ...:
    indices = rng.choice(...) # 随机选出索引
    self.dataframe = self.dataframe.iloc[indices.tolist()] # 只要选中的这些
<code>``
*   **提取文本**：
代码里有一段看起来很复杂的</code>series_to_item<code>和</code>apply`，其实就是为了防止数据格式嵌套太深（比如列表套列表），它的核心目的就是：<strong>把 Prompt 和 Response 变成纯文本列表</strong>。</p>
</li>
</ul>
<h4>2. 加工单条数据 (<code>__getitem__</code>) —— <strong>这是最核心的部分</strong></h4>
<p>每当训练循环需要一条数据，就会调用这个方法。</p>
<p><strong>Step A: 格式化对话 (Chat Template)</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 把纯文本变成对话格式，例如：[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;}]</span>
<span class="n">prompt_chat</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}]</span>

<span class="c1"># 转换成字符串，例如：&quot;&lt;|user|&gt;你好&lt;|end|&gt;&lt;|assistant|&gt;&quot;</span>
<span class="n">prompt_chat_str</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> 

<span class="c1"># 答案后面加上结束符 EOS</span>
<span class="n">response_chat_str</span> <span class="o">=</span> <span class="n">response</span> <span class="o">+</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</code></pre></div>

<p><strong>Step B: 数字化 (Tokenize)</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 把问题变成数字，例如 [101, 2045, 332]</span>
<span class="n">prompt_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt_chat_str</span><span class="p">,</span> <span class="o">...</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># 把答案变成数字</span>
<span class="n">response_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">response_chat_str</span><span class="p">,</span> <span class="o">...</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>

<p><strong>Step C: 拼接 (Concatenation)</strong>
SFT 的原理是让模型看完了问题，接着生成答案。所以在输入层，它们是连在一起的。</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 最终输入 = 问题 + 答案</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">prompt_ids</span><span class="p">,</span> <span class="n">response_ids</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<p><strong>Step D: 统一长度 (Padding)</strong>
模型训练需要矩阵整齐，所以长度必须固定（比如 1024）。</p>
<div class="codehilite"><pre><span></span><code><span class="k">if</span> <span class="n">sequence_length</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">:</span>
    <span class="c1"># 补齐操作：生成一堆 pad_token_id 填在后面</span>
    <span class="n">padded_input_ids</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">padded_input_ids</span><span class="p">))</span>
</code></pre></div>

<p><strong>Step E: 制作掩码 (Masking) —— 这里的逻辑很重要！</strong></p>
<p>我们训练模型时，<strong>不希望模型计算“问题”部分的 Loss</strong>。因为问题是已知的，模型只需要学会根据问题生成“答案”。</p>
<div class="codehilite"><pre><span></span><code><span class="n">loss_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="k">if</span> <span class="n">prompt_length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># 把属于 prompt 的部分的 loss_mask 设为 0</span>
    <span class="c1"># 意思是：这部分错了也不扣分，不用学</span>
    <span class="n">loss_mask</span><span class="p">[:</span> <span class="nb">min</span><span class="p">(</span><span class="n">prompt_length</span><span class="p">,</span> <span class="n">loss_mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div>

<ul>
<li><code>attention_mask</code>: 告诉模型哪些是真字，哪些是补位的 0。</li>
<li><code>loss_mask</code>: 告诉模型哪些字是需要预测的（即 Response 部分）。</li>
</ul>
<h3>总结</h3>
<p>这个文件的逻辑就是：
1.  <strong>读表</strong>（Pandas读Parquet）。
2.  <strong>取词</strong>（拿Prompt和Response）。
3.  <strong>拼词</strong>（Prompt + Response）。
4.  <strong>转码</strong>（Tokenizer转ID）。
5.  <strong>对齐</strong>（Padding补0）。
6.  <strong>遮盖</strong>（Loss Mask 把 Prompt 盖住，只算 Response 的梯度）。</p>
<p>最终输出给模型的是一个字典，包含了 <code>input_ids</code>（输入内容）, <code>attention_mask</code>（长度掩码）, 和 <code>loss_mask</code>（训练目标掩码）。</p>