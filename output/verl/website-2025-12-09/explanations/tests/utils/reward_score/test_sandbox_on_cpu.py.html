<h1>tests/utils/reward_score/test_sandbox_on_cpu.py</h1>
<p>这份代码其实是一个<strong>测试文件</strong>（Unit Test）。</p>
<p>想象一下，你开发了一个“<strong>自动阅卷系统</strong>”（Reward Model），这个系统的功能是给AI生成的答案打分。
*   如果是数学题，它要看答案对不对。
*   如果是代码题，它要把代码跑一遍，看能不能通过测试用例。</p>
<p>这个文件的作用，就是<strong>用来检查这个“自动阅卷系统”本身有没有坏掉</strong>。</p>
<p>为了让你读懂，我制定了一个<strong>学习任务清单 (Task Todo List)</strong>，我们一步步来勾选完成。</p>
<hr />
<h3>✅ Task 1：搞懂“考卷”和“标准答案”是什么 (数据准备)</h3>
<p>在代码的最上面，定义了一堆变量，这些就是用来测试的“样题”。</p>
<p><strong>1.1 数学题部分 (<code>prime_math_...</code>)</strong>
*   <strong><code>prime_math_answers</code></strong>: 这是模拟AI生成的数学答案。比如矩阵 <code>\begin{bmatrix}...</code> 或者公式。
*   <strong><code>prime_math_gts</code> (Ground Truths)</strong>: 这是标准答案。
*   <strong>逻辑</strong>: 阅卷系统需要比对这两个，虽然格式可能微调（比如 <code>bmatrix</code> vs <code>pmatrix</code>），但意思一样得给满分。</p>
<p><strong>1.2 代码题部分 (<code>prime_code_...</code>)</strong>
*   <strong><code>prime_code_answers</code></strong>: 这是一个Python代码字符串。你看那个 <code>def main(): ...</code>，它是一个解决迷宫/路径问题的算法。
*   <strong><code>prime_code_gts</code></strong>: 这是代码的标准答案，但它不是代码，而是一堆 <strong>JSON 格式的输入输出 (Input/Output)</strong>。
    *   比如：输入 <code>5 7 6 11...</code>，期望输出 <code>4</code>。
    *   <strong>逻辑</strong>: 阅卷系统必须把上面的代码放到一个“沙盒”（Sandbox）里运行，输入这些数据，看输出对不对。
*   <strong><code>prime_code_scores</code></strong>: 预设的期望得分。<code>[1.0, 0.9]</code> 意思是第一题全对给1分，第二题部分对给0.9分。</p>
<hr />
<h3>✅ Task 2：检查“数学阅卷”功能 (Basic Function)</h3>
<p>找到函数：<code>def test_prime_math():</code></p>
<ul>
<li><strong>目标</strong>: 测试数学打分逻辑是否正常。</li>
<li><strong>步骤</strong>:<ol>
<li>拿到 <code>completion</code> (AI答案) 和 <code>ground_truth</code> (标准答案)。</li>
<li>调用 <code>default_compute_score(...)</code> 这个核心阅卷函数。</li>
<li><strong>断言 (<code>assert</code>)</strong>: 检查算出来的分数是不是 <code>1.0</code>。</li>
</ol>
</li>
<li><strong>结论</strong>: 如果这个测试通过，说明你的系统能看懂数学公式。</li>
</ul>
<hr />
<h3>✅ Task 3：检查“代码阅卷”功能 (Sandbox Function)</h3>
<p>找到函数：<code>def test_prime_code():</code></p>
<ul>
<li><strong>目标</strong>: 测试代码打分逻辑（通常需要在本地安装特定环境）。</li>
<li><strong>步骤</strong>:<ol>
<li>拿到代码和测试用例。</li>
<li>调用阅卷函数。</li>
<li>对比算出来的分数和 <code>prime_code_scores</code> 里预设的分数（比如0.9）是否一致。</li>
</ol>
</li>
<li><strong>注意</strong>: 这里有个 <code>@pytest.mark.skip</code>，意思是“跳过这个测试”。作者写了备注：<code>pyext not compatible with python 3.12</code>。说明在Python 3.12环境下，本地跑代码沙盒的库不兼容，所以暂时不测这个。</li>
</ul>
<hr />
<h3>✅ Task 4：检查“批量阅卷”速度 (Parallelism)</h3>
<p>找到函数：<code>def test_parallelism():</code></p>
<ul>
<li><strong>目标</strong>: 如果我有1000份考卷，一份份改太慢了。这个测试是检查能不能<strong>并行（多进程）</strong>阅卷。</li>
<li><strong>步骤</strong>:<ol>
<li><strong>造数据</strong>: 用 <code>while</code> 循环把上面的数学题和代码题复制了很多遍，凑够32个题目。</li>
<li><strong>异步运行</strong>: <code>asyncio.run(parallel_compute_score_async(..., num_processes=16))</code>。<ul>
<li>意思是：启动16个阅卷老师（进程）同时干活。</li>
</ul>
</li>
<li><strong>打印</strong>: 输出分数。</li>
</ol>
</li>
<li><strong>结论</strong>: 如果这步不报错且速度快，说明大规模评估时系统撑得住。</li>
</ul>
<hr />
<h3>✅ Task 5：检查“远程沙盒”功能 (Remote Sandbox)</h3>
<p>这里是重点，针对代码题，本地跑不安全或环境很难配，通常会发给一个远程服务器（Sandbox Fusion）去跑。</p>
<p><strong>5.1 <code>test_prime_code_sandbox_fusion</code></strong>
*   <strong>前提</strong>: <code>@pytest.mark.skipif(...)</code>。只有当你设置了环境变量 <code>SANDBOX_FUSION_URL</code>（远程沙盒的网址）时，才跑这个测试。
*   <strong>逻辑</strong>:
    1.  读取远程沙盒的URL。
    2.  调用 <code>default_compute_score</code>，但这次带上了 <code>extra_info={"sandbox_fusion_url": ...}</code>。
    3.  系统内部发现有URL，就会把代码发给远程服务器跑，而不是在本地跑。
    4.  检查分数对不对。</p>
<p><strong>5.2 <code>test_continuous_score_consistency</code></strong>
*   <strong>目标</strong>: 这是一个<strong>一致性测试</strong>。
*   <strong>背景</strong>: 代码题有时候不是非0即1，可能通过了90%的测试用例。
*   <strong>逻辑</strong>:
    1.  用方法A（本地逻辑或旧逻辑 <code>prime_code</code>）算一次分。
    2.  用方法B（远程沙盒逻辑 <code>sandbox_fusion</code>）算一次分。
    3.  <strong>比对</strong>: 确保两个方法算出来的分数（比如都是0.9）是一模一样的。
    4.  防止系统升级后，远程算的0.9，本地算的0.0，那就出bug了。</p>
<hr />
<h3>📝 总结 (Summary)</h3>
<p>这个文件就是一个<strong>质检员</strong>，它在说：</p>
<ol>
<li><strong>准备数据</strong>：我准备了几道数学题和代码题（含答案）。</li>
<li><strong>测并行</strong>：我看能不能同时改32份卷子。</li>
<li><strong>测数学</strong>：我看数学题能不能改对。</li>
<li><strong>测代码</strong>：<ul>
<li>如果只在本地改，能不能改对？（但现在Python 3.12跳过了）</li>
<li>如果发给远程服务器改，能不能改对？</li>
<li>远程改和本地改，给出的“步骤分”是不是一样？</li>
</ul>
</li>
</ol>
<p>你只要看懂了它是<strong>“拿预设的题目和答案，去跑打分函数，验证分数是否符合预期”</strong>，你就完全看懂了这个文件。</p>