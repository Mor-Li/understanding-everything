<h1>docs/algo/baseline.md</h1>
<p>这份文件乍一看确实全是枯燥的数据和表格，但它其实是这个开源项目（看起来是一个叫 <code>verl</code> 的强化学习训练框架）的<strong>“成绩单”</strong>和<strong>“参考答案”</strong>。</p>
<p>简单来说，它的作用是告诉你：<strong>“如果你用我们的代码和设置，去训练这些模型，你应该能达到这样的分数。如果达不到，说明你哪里弄错了。”</strong></p>
<p>为了让你看懂，我把阅读这份文件拆解成一个 <strong>6步走的 Task List（任务清单）</strong>，带你一步步通关。</p>
<hr />
<h3>✅ Task 1: 搞清楚“我们在干什么？”（宏观概念）</h3>
<ul>
<li><strong>背景知识</strong>：这是一个关于<strong>LLM（大语言模型）强化学习（RL）</strong>的项目。</li>
<li><strong>文件核心</strong>：Baseline（基线）意味着“标准成绩”。开发者跑了很多实验，记录了不同模型在不同算法下的表现。</li>
<li><strong>你的任务</strong>：认识到这份文件是用来<strong>“对齐预期”</strong>的。<ul>
<li>比如：你想用 PPO 算法训练 Qwen-0.5B 模型，训练完不知道效果好不好？来查这张表，官方跑了 <strong>56.7</strong> 分，你如果只有 20 分，说明有问题；如果你跑了 57 分，说明复现成功。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 2: 准备工作（文件开头的代码）</h3>
<ul>
<li><strong>关注点</strong>：文件最上面那一小段代码。
    <code>bash
    python3 examples/data_preprocess/*.py</code></li>
<li><strong>解读</strong>：这是<strong>“入场券”</strong>。</li>
<li><strong>你的任务</strong>：<ul>
<li>理解在看表格之前，必须先处理数据。官方告诉你，所有表格里的成绩，都是基于运行过这个数据预处理脚本之后得到的。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 3: 怎么读懂那张巨大的表格？（核心技能）</h3>
<p>让我们拿 <strong>GSM8k</strong>（一个经典的小学数学应用题数据集）那张大表里的几行来举例拆解。</p>
<p><strong>请看这一行：</strong>
<code>| NVIDIA GPU | google/gemma-2-2b-it | SFT | 52.06 | ... |</code></p>
<ul>
<li><strong>Hardware (硬件)</strong>：用什么显卡跑的（NVIDIA 还是 AMD）。</li>
<li><strong>Model (模型)</strong>：用的哪个基础模型（这里是 Google 的 Gemma-2-2b）。</li>
<li><strong>Method (方法)</strong>：这是重点！<ul>
<li><code>hf checkpoint</code>: 原始模型直接跑，没经过这个库训练。</li>
<li><code>SFT</code>: 监督微调（由老师教一遍）。</li>
<li><code>PPO</code> / <code>GRPO</code> / <code>ReMax</code>: 各种强化学习算法（让模型自己练习、奖惩）。</li>
</ul>
</li>
<li><strong>Test score (分数)</strong>：在 GSM8k 数学题上的准确率。</li>
</ul>
<p><strong>你的任务（对比观察）：</strong>
*   找同一款模型，看不同方法的区别。
    *   Gemma-2-2b 原始模型：<strong>23.9 分</strong>。
    *   Gemma-2-2b 经过 SFT（微调）：<strong>52.06 分</strong>（变强了！）。
    *   Gemma-2-2b 经过 SFT + PPO（强化学习）：<strong>64.02 分</strong>（更强了！）。
*   <strong>结论</strong>：这个表格证明了<strong>这个库的强化学习算法（PPO）能显著提升模型做数学题的能力。</strong></p>
<hr />
<h3>✅ Task 4: 了解业界的“黑话”算法（进阶理解）</h3>
<p>表格里有很多奇怪的缩写，你不需要懂原理，但要知道它们代表了<strong>“不同的训练流派”</strong>：</p>
<ul>
<li><strong>PPO</strong>: 最经典的强化学习算法（ChatGPT 用的就是这个思路）。</li>
<li><strong>GRPO</strong>: 最近很火的一种优化算法（DeepSeek-R1 背后那种思路，省显存，效果好）。<ul>
<li>你会发现表格里 Qwen2.5-3B 用 GRPO 跑到了 <strong>86.1</strong> 的高分。</li>
</ul>
</li>
<li><strong>ReMax / SPPO / PRIME</strong>: 其他变种算法。</li>
<li><strong>LoRA</strong>: 省资源的微调方式。</li>
</ul>
<p><strong>你的任务</strong>：
*   看到 <code>GRPO</code> 或 <code>Megatron</code>（一种加速框架）时，知道这是在展示这个库支持<strong>很多高级、前沿的训练技术</strong>，而不仅仅是基础的 PPO。</p>
<hr />
<h3>✅ Task 5: 看看别的赛道（其他数据集）</h3>
<p>除了 GSM8k（简单数学），文件下面还列了：</p>
<ol>
<li><strong>DAPO math-17k</strong>: 更难的数学竞赛题（AIME）。<ul>
<li>你看分数只有 36.3 或 40.0，说明题目很难。</li>
</ul>
</li>
<li><strong>Coding (Leetcode)</strong>: 写代码的能力。<ul>
<li>展示了用 PRIME 算法训练写代码模型的效果。</li>
</ul>
</li>
</ol>
<p><strong>你的任务</strong>：
*   知道这个库不仅能训数学模型，也能训写代码的模型。</p>
<hr />
<h3>✅ Task 6: 如何“抄作业”？（Details 列）</h3>
<p>这是最实用的一列：<strong>Details（详情）</strong>。</p>
<ul>
<li><strong>内容</strong>：全是链接。</li>
<li><strong>解读</strong>：<ul>
<li><code>[script]</code>: 官方给出了运行脚本。你想复现 97 分的效果？点这个链接，复制它的命令去跑。</li>
<li><code>[log]</code> / <code>[wandb]</code>: 训练过程的日志。你可以点进去看训练时的曲线图（Loss 怎么降的，奖励怎么升的）。</li>
</ul>
</li>
</ul>
<p><strong>你的任务</strong>：
*   如果你想自己跑，<strong>不要瞎猜参数</strong>。直接点击表格最右边的链接，照着官方的脚本（Script）和参数设置去跑。</p>
<hr />
<h3>总结（Takeaway）</h3>
<p><strong>这个文件在讲什么？</strong></p>
<blockquote>
<p>“嘿，兄弟，我们测试了 Qwen、Gemma、Deepseek 等各种模型。我们发现，用我们的 PPO 或 GRPO 算法训练后，数学能力都暴涨了（看分数对比）。如果你也想达到这个效果，请点击表格右边的脚本链接直接运行！”</p>
</blockquote>
<p>现在，你再看那张表，是不是觉得它就是一张<strong>“配置推荐单”</strong>和<strong>“实力证明书”</strong>了？</p>