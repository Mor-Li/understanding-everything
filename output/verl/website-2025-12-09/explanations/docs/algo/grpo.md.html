<h1>docs/algo/grpo.md</h1>
<p>完全没问题。这份文档主要介绍了一种名为 <strong>GRPO (Group Relative Policy Optimization)</strong> 的强化学习算法，以及如何在代码中配置它。</p>
<p>因为这是深度学习/强化学习（RL）的专业内容，直接看确实容易晕。我们可以把它想象成一个<strong>“学习任务清单” (To-Do List)</strong>。</p>
<p>你可以把这当成一个闯关游戏，我们一关一关地来解锁文中的知识点。</p>
<hr />
<h3>📝 任务清单：从零读懂 GRPO</h3>
<h4>✅ Task 1: 理解背景 —— 为什么要发明 GRPO？</h4>
<p><strong>文中的核心观点：</strong> 传统的算法（比如 PPO）太贵了，我们要省钱省力。</p>
<ul>
<li><strong>传统 PPO 的做法：</strong> 就像一个学生（Actor）在考试，旁边必须坐一个专门的老师（Critic）来打分，告诉学生这题做得好不好。<ul>
<li><em>坏处：</em> 训练这个“老师”模型非常消耗计算资源（显存、时间）。</li>
</ul>
</li>
<li><strong>GRPO 的做法：</strong> 把“老师”辞退了。<ul>
<li><em>好处：</em> 不需要训练额外的 Critic 模型，计算效率更高。</li>
<li><em>来源：</em> 这个方法来自 DeepSeekMath 的论文。</li>
</ul>
</li>
</ul>
<h4>✅ Task 2: 掌握核心机制 —— 没有“老师”，怎么打分？</h4>
<p><strong>文中的核心观点：</strong> 既然没有老师打分，那就搞“题海战术”和“内部竞争”。</p>
<p>GRPO 的工作流程（文中的 4 个步骤）：
1.  <strong>Group Sampling (分组采样)：</strong> 给模型一道题，让它一口气生成好几个不同的答案（比如生成 4 个）。这就像让学生对同一道题写 4 种解法。
2.  <strong>Reward Assignment (奖励分配)：</strong> 用一个规则（比如答案对不对）给这 4 个答案打分。
3.  <strong>Baseline Calculation (基准计算)：</strong> 算出这 4 个答案的<strong>平均分</strong>。
4.  <strong>Policy Update (策略更新)：</strong>
    *   如果某个答案的分数 <strong>高于</strong> 平均分，就鼓励模型下次多这么写。
    *   如果某个答案的分数 <strong>低于</strong> 平均分，就惩罚模型下次少这么写。
    *   <em>总结：</em> 这就是 <strong>"Group Relative" (组内相对)</strong> 的含义——我不跟标准答案比，我跟自己生成的这一组答案的平均水平比。</p>
<h4>✅ Task 3: 搞定配置 —— 怎么修改代码参数？</h4>
<p><strong>文中的核心观点：</strong> 虽然大部分参数和 PPO 一样，但有几个关键参数必须改，否则就不是 GRPO 了。</p>
<p>你需要关注以下几个关键配置（对应文中的 Configuration 部分）：</p>
<ol>
<li>
<p><strong><code>actor_rollout.ref.rollout.n</code> (采样数量)</strong></p>
<ul>
<li><strong>解释：</strong> 就是 Task 2 里说的“一口气生成几个答案”。</li>
<li><strong>操作：</strong> 默认是 1，但在 GRPO 里<strong>必须大于 1</strong>（否则就没有“组”的概念了，没法算平均分）。</li>
</ul>
</li>
<li>
<p><strong><code>algorithm.adv_estimator</code> (优势估计器)</strong></p>
<ul>
<li><strong>操作：</strong> 必须设置为 <code>grpo</code>。</li>
</ul>
</li>
<li>
<p><strong><code>actor_rollout_ref.actor.use_kl_loss</code> (KL 散度损失)</strong></p>
<ul>
<li><strong>背景：</strong> 在强化学习中，我们要防止模型学得太“飘”，偏离原始模型太远，通常会用 KL 散度来约束。</li>
<li><strong>区别：</strong> 以前 PPO 通常把这个约束放在“奖励”里扣分。</li>
<li><strong>操作：</strong> GRPO 建议把它直接放在 <strong>Loss（损失函数）</strong> 里。所以这里要设为 <code>True</code>。</li>
</ul>
</li>
<li>
<p><strong><code>actor_rollout_ref.actor.loss_agg_mode</code> (损失聚合模式)</strong></p>
<ul>
<li><strong>解释：</strong> 怎么算整体的误差。</li>
<li><strong>操作：</strong> 文中建议使用 <code>"token-mean"</code>（按 Token 取平均），这比原论文的方法在长思维链（Long CoT）场景下更稳定。</li>
</ul>
</li>
</ol>
<h4>✅ Task 4: 避坑指南 —— 进阶变体 DrGRPO</h4>
<p><strong>文中的核心观点：</strong> 原版 GRPO 有个小毛病，有人提出了改进版 DrGRPO。</p>
<ul>
<li><strong>问题 (Bug)：</strong> 有篇新论文（R1-Zero-Like Training）发现，GRPO 有时候会为了拿高分故意把答案写得<strong>特别长</strong>（哪怕是错的）。这叫“优化偏差”或“长度偏差”。</li>
<li><strong>解决 (Fix)：</strong> DrGRPO。</li>
<li><strong>如何切换到 DrGRPO？</strong><ol>
<li>把损失聚合模式 (<code>loss_agg_mode</code>) 改为 <code>"seq-mean-token-sum-norm"</code>。</li>
<li>关掉 KL Loss (<code>use_kl_loss</code> 设为 <code>False</code>)。</li>
<li>关掉标准差归一化 (<code>norm_adv_by_std_in_grpo</code> 设为 <code>False</code>)。</li>
</ol>
</li>
</ul>
<hr />
<h3>🚀 总结 (Summary)</h3>
<p>如果老板问你这文档讲了啥，你可以这样汇报：</p>
<ol>
<li><strong>是什么：</strong> GRPO 是一个<strong>不需要 Critic 模型</strong>的强化学习算法，比 PPO 更省资源。</li>
<li><strong>怎么做：</strong> 它通过<strong>一次生成一组答案</strong>，算出<strong>平均分</strong>，然后让优于平均分的答案获得奖励。</li>
<li><strong>怎么配：</strong> 重点要把采样数 (<code>rollout.n</code>) 设大，把 KL 散度放到 Loss 里计算。</li>
<li><strong>注意点：</strong> 如果发现模型废话太多（长度偏差），可以尝试文档里提到的 <strong>DrGRPO</strong> 配置来修正。</li>
</ol>
<p>这样看，是不是清晰多了？</p>