<h1>docs/algo/entropy.md</h1>
<p>这篇文章确实是一篇比较硬核的学术论文介绍（关于大模型强化学习的）。看不懂很正常，因为它涉及了很多数学和强化学习（RL）的术语。</p>
<p>为了帮你理解，我把它想象成<strong>“你在指导一个学生（AI模型）学数学”</strong>的过程，并列了一个<strong>Todo List（任务清单）</strong>。</p>
<p>你可以把阅读这篇文章的过程，看作是完成以下这 5 个任务：</p>
<hr />
<h3>📋 任务清单：理解“熵机制”论文</h3>
<h4>✅ Task 1: 搞清楚背景（我们在干什么？）</h4>
<ul>
<li><strong>背景</strong>：现在大模型（LLM）通过“强化学习”（RL）来提升推理能力（比如做数学题）很火（类似 DeepSeek-R1 的思路）。</li>
<li><strong>目标</strong>：想让模型通过不断的尝试和反馈，变得更聪明，能解难题。</li>
<li><strong>现状</strong>：大家发现训练过程中有个大坑，这篇论文就是来填这个坑的。</li>
</ul>
<h4>✅ Task 2: 发现问题（什么是“熵坍塌”？）</h4>
<ul>
<li><strong>核心概念</strong>：<strong>“熵”（Entropy）</strong>。在这里你可以简单理解为<strong>“多样性”</strong>或者<strong>“探索欲”</strong>。<ul>
<li>熵高 = 模型愿意尝试各种不同的解题方法（好奇宝宝）。</li>
<li>熵低 = 模型死守一种套路，非常自信（哪怕是错的也不改）。</li>
</ul>
</li>
<li><strong>遇到的Bug</strong>：<strong>“熵坍塌”（Entropy Collapse）</strong>。<ul>
<li>作者发现，在训练开始没多久，模型的“熵”就掉得太快了。</li>
<li><strong>后果</strong>：模型变得盲目自信，觉得自己什么都会了，不再尝试新思路。结果就是成绩（Performance）上不去，卡在瓶颈了。</li>
<li><strong>文中公式</strong>：$R = -a \exp(H) + b$。意思就是：<strong>熵（H）只要一没了，成绩（R）也就到头了。</strong></li>
</ul>
</li>
</ul>
<h4>✅ Task 3: 寻找病根（为什么会坍塌？）</h4>
<ul>
<li><strong>理论分析</strong>：作者去挖了数学原理（协方差那一块）。</li>
<li><strong>原因</strong>：在现有的训练方法（比如 Policy Gradient）中，如果模型发现某个动作“收益很高”（Advantage高）且“概率很大”（本来就爱这么干），它就会疯狂加强这个动作。</li>
<li><strong>通俗解释</strong>：模型尝到了一点甜头，就疯狂重复这一招，把其他可能更好的路全堵死了。这种“急功近利”的更新方式，导致多样性迅速消失。</li>
</ul>
<h4>✅ Task 4: 提出解决方案（怎么修？）</h4>
<ul>
<li><strong>策略</strong>：既然模型容易“急功近利”，那我们就要给它<strong>“踩刹车”</strong>。</li>
<li><strong>新方法</strong>：作者提出了两个补丁（算法）：<ol>
<li><strong><code>Clip-Cov</code></strong>：截断协方差。简单说，就是当模型想对某些高风险导致熵降低的行为进行大幅更新时，强制限制它的更新幅度。</li>
<li><strong><code>KL-Cov</code></strong>：利用KL散度来约束。也是一种数学手段，防止模型在这个导致熵降低的方向上跑得太偏。</li>
</ol>
</li>
<li><strong>目的</strong>：<strong>强行保留模型的“探索欲”（熵）</strong>，让它在训练后期依然愿意尝试新解法。</li>
</ul>
<h4>✅ Task 5: 验收成果（效果咋样？）</h4>
<ul>
<li><strong>图表证据</strong>：文中展示了图表，使用了新方法后，模型的“熵”一直维持在较高水平（是基线的10倍以上）。</li>
<li><strong>行为变化</strong>：模型的回答长度变长了（说明在认真思考推理，而不是瞎猜）。</li>
<li><strong>考试成绩</strong>：<ul>
<li>在 <strong>AIME</strong>（美国数学邀请赛）等很难的数学评测集上，成绩显著提升。</li>
<li>特别是在 <strong>32B</strong>（大参数）模型上，比原来的方法（GRPO）强了 <strong>15%</strong> 左右。</li>
</ul>
</li>
</ul>
<hr />
<h3>💡 总结（一句话看懂）</h3>
<p>这篇论文说：<strong>大模型在学数学时容易“过早自信”，导致不再探索新解法（熵坍塌）；作者发明了一种“防沉迷/防自大”的机制（Clip-Cov/KL-Cov），强制模型保持好奇心，结果模型数学成绩大大提高了。</strong></p>