<h1>docs/perf/device_tuning.rst</h1>
<p>这份文档其实就是一份<strong>“显卡配置推荐清单”</strong>（类似于玩游戏的“最低配置要求”）。</p>
<p>因为它全是技术术语和表格，所以看着晕。我把它拆解成一个 <strong>5步 Task List（任务清单）</strong>，带你一步步看懂它：</p>
<hr />
<h3>✅ Task 1: 理解核心背景 (Why?)</h3>
<p><strong>观点</strong>：强化学习（RL）训练比普通微调（SFT）更吃显卡资源，很难凭空猜出需要多少显卡。
*   <strong>通俗解释</strong>：你想训练一个AI模型，但不知道手里的显卡够不够用。如果显卡不够，程序跑起来就会报错（OOM）。
*   <strong>文档作用</strong>：作者做了一堆实验，把“跑通不同模型所需的<strong>最低</strong>硬件要求”列了出来，供你参考。</p>
<h3>✅ Task 2: 区分两种配置标准 (Definitions)</h3>
<p>文档里提到了两种脚本配置标准，你需要知道它们的区别：
1.  <strong>Tag: MIN (最低配置)</strong>：
    *   <strong>含义</strong>：能跑通就行。用了很多省显存的技术（比如offload），速度可能不快，但能让你用较少的显卡跑起来。
    *   <strong>现状</strong>：目前文档里列出的全是这种。
2.  <strong>Tag: RECOMMENDED (推荐配置)</strong>：
    *   <strong>含义</strong>：跑得快。不为了省钱而牺牲速度，追求效率。</p>
<h3>✅ Task 3: 学会查阅“配置表” (How to read)</h3>
<p>这是文档的主体部分。作者按模型大小（0.5B, 7B, 70B...）分了类。
<strong>举个例子（以 7B 模型为例）：</strong></p>
<ul>
<li><strong>Model (模型)</strong>: <code>Qwen2.5-7B</code> (通义千问70亿参数版本)</li>
<li><strong>Task (任务)</strong>: <code>GRPO-LoRA</code> (一种省显存的强化学习训练方法)</li>
<li><strong>Resource (硬件需求)</strong>: <code>1*H100</code><ul>
<li><strong>解读</strong>：如果你想训练这个7B模型，你<strong>至少</strong>需要一张英伟达 <strong>H100</strong> 显卡。</li>
</ul>
</li>
<li><strong>Resource (另一个案例)</strong>: <code>2*H800</code><ul>
<li><strong>解读</strong>：或者你需要两张 <strong>H800</strong> 显卡。</li>
</ul>
</li>
<li><strong>MaxBatch</strong>: <code>16</code> (一次能处理的数据量，显存越小这个数越小)</li>
<li><strong>Link</strong>: 点击那个链接就能直接拿到运行脚本。</li>
</ul>
<h3>✅ Task 4: 理解文件命名规则 (Naming)</h3>
<p>如果你以后想贡献代码，或者在文件夹里找文件，要看懂文件名的格式：
<code>[模型名]_[任务类型]_[显卡数量]_[设备型号]_[训练框架]_[推理框架].sh</code></p>
<ul>
<li><strong>例子</strong>：<code>qwen2-7b_grpo-lora_1_h100_fsdp_vllm.sh</code></li>
<li><strong>翻译</strong>：这是一个针对 <strong>Qwen2-7B</strong> 模型的脚本，跑 <strong>GRPO-LoRA</strong> 任务，用 <strong>1张 H100</strong> 显卡，训练用 <strong>fsdp</strong> 框架，推理用 <strong>vllm</strong> 框架。</li>
</ul>
<h3>✅ Task 5: 总结与行动 (Action)</h3>
<ul>
<li><strong>如果你是用户</strong>：<ol>
<li>看看你想训练多大的模型（比如 7B 还是 70B）。</li>
<li>去对应的标题下找表格。</li>
<li>看看 <code>Resource</code> 这一列，你的显卡达标了吗？</li>
<li>如果达标，点 <code>Link</code> 下载对应的脚本开始跑。</li>
</ol>
</li>
<li><strong>如果你是开发者</strong>：<ol>
<li>文档呼吁大家贡献更多测试数据。</li>
<li>如果你测出了新的配置，可以提交 PR（代码合并请求），并附上 Wandb 的截图证明能跑通。</li>
</ol>
</li>
</ul>
<hr />
<h3>一句话总结：</h3>
<p><strong>这就是个查阅手册。告诉你：“想训练 Qwen-70B？请准备 8张 H100 显卡；想训练 Qwen-7B？1张 H100 就够了。”</strong></p>