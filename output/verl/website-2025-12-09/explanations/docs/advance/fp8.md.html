<h1>docs/advance/fp8.md</h1>
<p>这份文档确实写得非常“硬核”，充满了术语。它主要是在讲如何在 <code>verl</code> 这个强化学习（RL）框架中，利用 <strong>FP8（8位浮点数）</strong> 技术来加速模型的 <strong>Rollout（推理/生成数据）</strong> 阶段。</p>
<p>为了帮你理解，我把你当作一个项目的负责人，列了一个 <strong>“FP8 加速升级任务清单 (Todo List)”</strong>。我们按照这个清单，一步步把文档拆解开来看。</p>
<hr />
<h3>📋 任务清单：理解与部署 FP8 Rollout</h3>
<h4>✅ Task 1: 搞懂目标——我们为什么要折腾这个？</h4>
<ul>
<li><strong>背景知识</strong>：在强化学习（RL）训练大模型时，有一个很耗时的步骤叫 <strong>Rollout</strong>（也就是让模型根据提示词生成很多回答，用来给后面的训练做素材）。</li>
<li><strong>痛点</strong>：通常我们用 BF16（16位精度）来做这件事，虽然准，但速度不够快，显存占用也大。</li>
<li><strong>文档的核心观点</strong>：这篇文档说，我们现在支持用 <strong>FP8</strong>（8位精度，数据更小、计算更快）来做 Rollout 了。</li>
<li><strong>预期收益</strong>：速度会变快（提升 12%~35%），显存占用变少，且准确率几乎不下降。</li>
</ul>
<h4>✅ Task 2: 了解技术方案——这是怎么实现的？</h4>
<ul>
<li><strong>核心手段</strong>：文档提到用了 <strong>"Monkey Patch"</strong>（一种代码补丁技术）。简单说，就是不需要改动 vLLM（一个推理库）的源码，而是在运行 <code>verl</code> 时，动态地把 vLLM 里的一些函数“偷梁换柱”了。</li>
<li><strong>具体做了啥</strong>：<ol>
<li><strong>即时量化 (Quantize weights)</strong>：模型加载时是高精度的，代码会自动把它转成 FP8 格式。</li>
<li><strong>权重处理 (Process weights)</strong>：修改了 vLLM 处理权重的逻辑，让它能吃得下这些 FP8 的数据。（注：如果是用 SGLang 这个库，则不需要这一步，因为它原生支持）。</li>
</ol>
</li>
<li><strong>技术标准</strong>：采用的是 Deepseek 使用的 <strong>Blockwise Quantization</strong>（分块量化），这是一种很先进的量化方法，能保证精度损失最小。</li>
</ul>
<h4>✅ Task 3: 检查兼容性——我的环境能用吗？</h4>
<p>文档列出了一个 <strong>Support Matrix（支持列表）</strong>，你需要核对你的环境：
*   <strong>模型类型</strong>：支持稠密模型（Dense，如Llama/Qwen-Base）和 混合专家模型（MoE，如Qwen-MoE）。
*   <strong>软件版本</strong>：vLLM (0.10.x 或 0.11) 或者 SGLang (0.5.5)。
*   <strong>训练后端</strong>：FSDP 或者 Megatron 都支持。
*   <strong>硬件</strong>：虽然没明说，但FP8通常需要 NVIDIA H100 或 H800 系列显卡支持最好。</p>
<h4>✅ Task 4: 评估风险与收益——效果真的好吗？（实验部分）</h4>
<p>文档做了两个实验，分别用 Qwen3-8B 和 Qwen3-30B 模型。</p>
<p><strong>实验 1：Qwen3-8B (Dense模型)</strong>
*   <strong>速度</strong>：用了 FP8 后，推理速度提升了 <strong>12% ~ 18%</strong>（取决于CUDA版本，越新越快）。
*   <strong>准确率 (Accuracy)</strong>：
    *   ⚠️ <strong>风险点</strong>：如果直接用 FP8，准确率会下降（文档里的图显示如果不加 TIS，acc 会掉）。
    *   ✅ <strong>解决方案</strong>：必须开启 <strong>TIS (Token-level Importance Sampling)</strong>。这是一种修正算法。文档结论是：<strong>只要开了 TIS，FP8 的效果和原本的 BF16 几乎一样好。</strong></p>
<p><strong>实验 2：Qwen3-30B (MoE模型)</strong>
*   <strong>速度</strong>：这个提升巨大！推理速度提升了 <strong>超过 35%</strong>。
*   <strong>准确率</strong>：同样结论，MoE 模型本身就容易出现分布不匹配，但只要开启 TIS，FP8 的表现就能对齐 BF16。</p>
<h4>✅ Task 5: 动手执行——怎么配置？(Usage)</h4>
<p>既然效果好且风险可控，最后一步就是怎么开启它。
文档给了两种方法：</p>
<ol>
<li>
<p><strong>方法 A：改配置文件 (<code>yaml</code>)</strong>
    找到 <code>verl/trainer/config/ppo_megatron_trainer.yaml</code>，加上：
    <code>yaml
    rollout:
      quantization: "fp8"  # 把这一行加上</code></p>
</li>
<li>
<p><strong>方法 B：命令行参数</strong>
    在运行脚本时直接加参数：
    <code>actor_rollout_ref.rollout.quantization=fp8</code></p>
</li>
</ol>
<hr />
<h3>💡 总结一下 (TL;DR)</h3>
<p>这篇文档就是告诉你：
1.  <strong>好消息</strong>：<code>verl</code> 现在支持用 FP8 跑推理了，<strong>速度快很多</strong>（尤其是 MoE 模型）。
2.  <strong>注意事项</strong>：为了不让模型变笨（掉精度），<strong>必须配合 TIS (Token-level Importance Sampling) 算法</strong>一起用。
3.  <strong>怎么做</strong>：改个配置参数 <code>quantization="fp8"</code> 就行。</p>