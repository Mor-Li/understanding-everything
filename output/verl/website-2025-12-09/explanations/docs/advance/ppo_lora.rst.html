<h1>docs/advance/ppo_lora.rst</h1>
<p>没问题。这份文档其实是在教你<strong>如何在一个叫 <code>verl</code> 的强化学习框架中，开启并配置 LoRA（一种省显存的微调技术）</strong>。</p>
<p>之所以你看得晕，是因为它堆砌了很多参数和配置项。我把它拆解成一个<strong>“学习与实操的任务清单 (To-Do List)”</strong>，你只需要按顺序一步步理解和操作即可。</p>
<hr />
<h3>📋 任务清单：在 RL 训练中启用 LoRA</h3>
<h4>✅ Task 1: 理解核心概念（我们在做什么？）</h4>
<p><strong>文档观点：</strong>
我们支持在 PPO、GRPO 等强化学习算法中使用 LoRA。
*   <strong>为什么要用？</strong> 全量微调（Full Fine-tuning）大模型太费显存了。LoRA 是一种“外挂”技术，只训练一小部分参数，就能达到类似的效果。
*   <strong>有什么好处？</strong>
    1.  <strong>省钱省卡</strong>：你可以在 8 张 80G 的显卡上训练 70B（700亿参数）的巨大模型。
    2.  <strong>速度更快</strong>：显存占用小了，Batch Size 就能开大。
    3.  <strong>部署方便</strong>：只需要保存那个小小的 LoRA 文件，不用保存整个大模型。</p>
<h4>✅ Task 2: 检查环境要求（能用的前提）</h4>
<p><strong>文档观点：</strong>
不是随便什么配置都能开 LoRA，必须满足特定的后端设置。
*   <strong>操作指南</strong>：
    1.  训练器必须是 <code>RayPPOTrainer</code>（这是代码里的类名）。
    2.  <strong>后端必须配合：</strong>
        *   训练策略 (<code>strategy</code>) 必须设为 <code>fsdp</code> 或 <code>fsdp2</code>（一种分布式训练技术）。
        *   推理/采样引擎 (<code>rollout.name</code>) 必须设为 <code>vllm</code>（目前最高效的推理库）。
        *   <em>注：sglang 的支持还在开发中。</em></p>
<h4>✅ Task 3: 配置必填参数（必须改的设置）</h4>
<p><strong>文档观点：</strong>
要开启 LoRA，你必须在配置文件里修改以下几项：
*   <strong><code>actor_rollout_ref.model.lora_rank</code></strong>: (整数) 比如 8, 16, 32, 64。这是 LoRA 的“秩”，决定了外挂脑容量的大小。
*   <strong><code>actor_rollout_ref.model.lora_alpha</code></strong>: (浮点数) LoRA 的缩放系数。
*   <strong><code>actor_rollout_ref.model.target_modules</code></strong>: (字符串) 通常设为 <code>all-linear</code>（对所有线性层生效）。
*   <strong>关键设置</strong>：
    *   <strong><code>actor_rollout_ref.rollout.load_format="safetensors"</code></strong>: <strong>必须设这个！</strong> 否则 vLLM 没法加载基础模型。</p>
<h4>✅ Task 4: 配置进阶/可选参数（想接着练怎么办？）</h4>
<p><strong>文档观点：</strong>
如果你有一个已经练了一半的 LoRA，或者想分阶段训练：
*   <strong><code>actor_rollout_ref.model.lora_adapter_path</code></strong>: 填入你之前保存 LoRA 的文件夹路径。
    *   文件夹里必须包含 <code>adapter_model.safetensors</code> 和 <code>adapter_config.json</code>。
    *   填了这个，程序就会加载旧的 LoRA 继续练，而不是新建一个。</p>
<h4>✅ Task 5: 优化性能与避坑（最佳实践）</h4>
<p><strong>文档观点：</strong>
这里是“老司机的经验之谈”，教你如何练得更好、不爆显存。</p>
<ol>
<li><strong>关于学习率 (Learning Rate)</strong>：<ul>
<li><strong>建议</strong>：把学习率调大！通常比全量微调大一个数量级（比如 10 倍）。因为 LoRA 训练参数少，胆子可以大一点。</li>
</ul>
</li>
<li><strong>关于 Rank 的大小</strong>：<ul>
<li><strong>警告</strong>：别设太小。太小了模型学不会。</li>
<li><strong>经验值</strong>：<ul>
<li>0.5B 的小模型：Rank &gt;= 32。</li>
<li>32B 的大模型：Rank &gt;= 128。</li>
<li><em>总结：Rank=32 或 128 的效果通常和全量微调差不多。</em></li>
</ul>
</li>
</ul>
</li>
<li><strong>显存优化技巧（防崩溃）</strong>：<ul>
<li><strong><code>use_shm=True</code></strong>: 把模型预加载到共享内存，加载速度变快。</li>
<li><strong><code>layered_summon=True</code></strong>: <strong>强烈推荐！</strong> 尤其是当你跑 70B 大模型或者显存小于 48G 时。它会让模型一层一层地同步参数，避免瞬间显存爆炸。</li>
</ul>
</li>
</ol>
<h4>✅ Task 6: 抄作业（参考配置与脚本）</h4>
<p><strong>文档观点：</strong>
如果你还是不会配，直接抄这个 72B 模型的配置，或者运行现成的脚本。</p>
<ul>
<li><strong>72B 模型参考配置</strong>（在 8卡 A100/H100 80G 上）：<ul>
<li>Rank 设为 32。</li>
<li>学习率 <code>lr</code> 设为 <code>3e-5</code>。</li>
<li>开启了 <code>param_offload</code> (参数卸载) 和 <code>layered_summon</code> 来省显存。</li>
</ul>
</li>
<li><strong>去哪里找运行脚本？</strong><ul>
<li>从头练：<code>examples/grpo_trainer/run_qwen2_5-3b_gsm8k_grpo_lora.sh</code></li>
<li>加载旧 LoRA 接着练：<code>examples/grpo_trainer/run_qwen2_5-3b_gsm8k_grpo_lora_from_adapter.sh</code></li>
</ul>
</li>
</ul>
<hr />
<h3>总结一下你需要做的事：</h3>
<ol>
<li><strong>确认</strong>你用的是 FSDP + vLLM。</li>
<li><strong>修改配置</strong>：加上 <code>lora_rank</code> (建议 32+), <code>lora_alpha</code>, <code>target_modules=all-linear</code>。</li>
<li><strong>重要</strong>：加上 <code>load_format="safetensors"</code>。</li>
<li><strong>调参</strong>：把学习率调大一点，开启 <code>layered_summon</code> 防止爆显存。</li>
<li><strong>运行</strong>提供的 <code>examples</code> 脚本开始跑。</li>
</ol>