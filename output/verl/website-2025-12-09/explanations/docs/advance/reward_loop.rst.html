<h1>docs/advance/reward_loop.rst</h1>
<p>这份文档主要是在介绍 <code>verl</code> 框架中<strong>“奖励循环”（Reward Loop）</strong>模块的设计和用法。简单来说，它是在讲<strong>“当大模型生成了一个答案后，系统如何给这个答案打分”</strong>。</p>
<p>这对于强化学习（RLHF）至关重要，因为模型需要根据分数来优化自己。</p>
<p>为了让你更容易理解，我为你列了一个 <strong>“学习任务清单 (To-Do List)”</strong>，我们将文档拆解为 5 个步骤，由浅入深地通过通俗的语言来讲解。</p>
<hr />
<h3>✅ Task 1: 理解核心概念 —— 谁在打分？</h3>
<p><strong>文档对应部分：</strong> <code>Reward Function Usage</code></p>
<p>首先，你需要知道系统支持哪几种打分方式（即奖励函数）。文档列出了四种：</p>
<ol>
<li><strong>Rule-based Reward (基于规则):</strong><ul>
<li><strong>通俗解释：</strong> 像做数学题或选择题。答案对就是对，错就是错。比如通过字符串匹配，看模型输出的答案是否和标准答案一模一样。</li>
<li><em>特点：简单、死板。</em></li>
</ul>
</li>
<li><strong>Discriminative Reward Model (DisRM, 判别式奖励模型):</strong><ul>
<li><strong>通俗解释：</strong> 像请了一位阅卷老师（专门训练过的模型）。把问题和答案给它，它直接吐出一个分数（比如 0.8 分）。</li>
<li><em>特点：常用，像传统的 Reward Model。</em></li>
</ul>
</li>
<li><strong>Generative Reward Model (GenRM, 生成式奖励模型):</strong><ul>
<li><strong>通俗解释：</strong> 像请了一位会写评语的老师（通常是强大的 LLM）。它不直接给分，而是写一段话分析你的答案，然后你得从这段话里提取出分数。</li>
<li><em>特点：灵活、更智能，但处理起来麻烦一点。</em></li>
</ul>
</li>
<li><strong>Hybrid (混合模式):</strong><ul>
<li><strong>通俗解释：</strong> 综合以上几种。比如先用规则筛一遍，再用模型打分。</li>
</ul>
</li>
</ol>
<hr />
<h3>✅ Task 2: 学习如何配置 —— 怎么让系统知道用哪种？</h3>
<p><strong>文档对应部分：</strong> <code>Rule-based Reward</code> &amp; <code>Discriminative Reward Model</code></p>
<p>这一步讲怎么用简单的配置来实现 Task 1 中的前两种：</p>
<ul>
<li><strong>如果你想用“规则打分”：</strong><ul>
<li>什么都不用做（默认就是这个），或者不提供自定义函数。系统会去 <code>verl/utils/reward_score/</code> 找默认规则。</li>
</ul>
</li>
<li><strong>如果你想用“判别式模型打分 (DisRM)”：</strong><ul>
<li>只需要在配置里告诉系统模型的路径：<code>--reward_model.model.path</code>。</li>
<li>系统会自动把问题和答案喂给模型，拿到分数。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 3: 进阶挑战 —— 手写生成式奖励 (GenRM)</h3>
<p><strong>文档对应部分：</strong> <code>Generative Reward Model (GenRM)</code></p>
<p>这是文档中最难懂、但最重要的部分。因为 GenRM 输出的是一段话，系统不知道怎么读，所以<strong>你需要自己写代码（Custom Reward Function）</strong>。</p>
<p>文档给了一个 Python 代码示例，逻辑如下：
1.  <strong>准备 Prompt：</strong> 把题目和模型生成的答案拼成一个提示词（比如：“请评价这个答案...”）。
2.  <strong>调用模型：</strong> 通过 HTTP 接口（<code>reward_router_address</code>）把提示词发给 GenRM 模型。
3.  <strong>解析结果：</strong> 拿到模型回复的一大段话，写代码从中把分数“抠”出来（比如用 <code>split</code> 或正则提取数字）。</p>
<p><strong>关键点：</strong> 如果你用 GenRM，必须同时提供模型路径和自定义函数。</p>
<hr />
<h3>✅ Task 4: 理解系统架构 —— 资源怎么分配？</h3>
<p><strong>文档对应部分：</strong> <code>Architecture Design</code></p>
<p>这部分讲的是计算资源（GPU）怎么安排。</p>
<ul>
<li><strong>Colocate Mode (合租模式):</strong><ul>
<li><strong>解释：</strong> 负责生成的模型（Actor）和负责打分的模型（Reward）共用同一堆 GPU。</li>
<li><strong>流程：</strong> Actor 先干活，干完了歇着，Reward 模型醒过来打分。</li>
<li><strong>优点：</strong> 省钱（省显存）。</li>
</ul>
</li>
<li><strong>Standalone Mode (独居模式):</strong><ul>
<li><strong>解释：</strong> 打分模型有自己专属的 GPU 资源池。</li>
<li><strong>流程：</strong> Actor 生成一个，Reward 模型立刻在另一边打分，互不干扰。</li>
<li><strong>优点：</strong> 速度快，效率高。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 5: 了解幕后黑手 —— 谁在干活？</h3>
<p><strong>文档对应部分：</strong> <code>RewardLoopWorker</code> &amp; <code>Manager</code></p>
<p>最后一部分是给开发者看的底层实现逻辑：</p>
<ol>
<li><strong>RewardLoopWorker:</strong><ul>
<li>它是具体的“打分工人”。它支持异步操作（Async），可以批量处理数据。</li>
<li>它的逻辑是：如果有自定义函数就用自定义的；如果没有，且开启了模型，就用 DisRM；否则用规则。</li>
</ul>
</li>
<li><strong>RewardLoopManager:</strong><ul>
<li>它是“工头”。负责启动 Reward 模型，并管理上面的 Worker。</li>
</ul>
</li>
<li><strong>RewardModelManager:</strong><ul>
<li>它是“路由器”。因为可能有多个打分模型在跑，它负责把请求分发给不同的服务器，并把结果收回来。这让用户写代码时只需要对着一个统一的地址发请求就行了。</li>
</ul>
</li>
</ol>
<hr />
<h3>总结 (Summary)</h3>
<p>这篇文档其实就在说三件事：
1.  <strong>What:</strong> 我们支持规则、判别模型、生成模型三种打分方式。
2.  <strong>How:</strong> 简单的直接配路径，复杂的（GenRM）需要你自己写个 Python 函数来解析分数。
3.  <strong>Deploy:</strong> 你可以让打分模型和生成模型挤在一起（省钱），也可以分开跑（求快），系统底层（Worker/Manager）都帮你封装好了。</p>