<h1>docs/README_vllm0.7.md</h1>
<p>这份文档主要是在讲 <strong>如何在 <code>verl</code>（一个大模型强化学习训练框架）里，正确地安装和配置 <code>vLLM 0.7</code> 版本作为推理引擎</strong>。</p>
<p>因为它包含了很多代码细节和版本特例，确实容易让人晕头转向。为了让你看懂，我把它拆解成一个 <strong>“任务清单 (To-Do List)”</strong>，你只需要按顺序确认每个阶段即可。</p>
<hr />
<h3>📋 任务清单：从安装到加速</h3>
<h4>🟢 第一步：决策（要不要读这篇文档？）</h4>
<p><strong>核心观点</strong>：文档开头第一句话就给了个“逃生通道”。
- <strong>任务</strong>：检查你是否必须用 <code>vLLM 0.7</code>？
- <strong>解释</strong>：文档说 <code>verl</code> 配合 <code>vLLM 0.8.3</code> 已经稳定了。如果你没有特殊理由非要用旧版 0.7，建议直接关掉这个文档，去看 <code>docs/README_vllm0.8.md</code>，那边的体验会更好。
- <strong>结论</strong>：如果你非要用 0.7，请继续往下看。</p>
<hr />
<h4>🔵 第二步：基础环境安装</h4>
<p><strong>核心观点</strong>：搭建最基本的运行环境。
- <strong>任务</strong>：
    1. 创建一个 Python 3.10 的环境。
    2. 安装 <code>verl</code> 框架。
    3. <strong>关键点</strong>：安装 <code>vLLM</code> 时，建议直接安装 <code>0.7.3</code> 版本（命令：<code>pip3 install vllm==0.7.3</code>）。
    4. 安装 <code>flash-attn</code>（加速库）。
- <strong>避坑指南</strong>：
    - 文档特别提到，如果你手欠安装了更老的 <code>0.7.0</code>, <code>0.7.1</code>, 或 <code>0.7.2</code>，你必须<strong>手动修改源代码</strong>（改 Python 文件）。
    - <strong>建议</strong>：直接装 <code>0.7.3</code>，就能跳过那些繁琐的“手动打补丁”环节（即文档中列出的那堆 <code>Remove the assertion</code> 代码修改）。</p>
<hr />
<h4>🟡 第三步：开启加速功能（CUDA Graph）</h4>
<p><strong>核心观点</strong>：默认配置跑得慢，改两个参数能变快。
- <strong>背景</strong>：<code>verl</code> 用 FSDP 训练，用 vLLM 做推理（生成文本）。默认情况下 vLLM 没开“显卡图计算（CUDA Graph）”，速度一般。
- <strong>任务</strong>：
    - 在你的启动脚本（bash script）里，找到配置项，修改/添加以下两行：
        <code>bash
        actor_rollout_ref.rollout.enforce_eager=False  # 关闭eager模式，开启图模式
        actor_rollout_ref.rollout.free_cache_engine=True # 开启显存释放优化</code>
- <strong>效果</strong>：生成文本的时间能从 85秒 缩短到 62秒。
- <strong>警告</strong>：如果你一次生成多条回复（SamplingParams 的 n &gt; 1），老版引擎可能会不稳定。</p>
<hr />
<h4>🔴 第四步：进阶折腾（使用 vLLM V1 引擎 - 选做）</h4>
<p><strong>核心观点</strong>：如果你想要极致速度（1.5倍提升）且不怕麻烦，可以升级引擎内核。
- <strong>背景</strong>：vLLM 有个新版内核叫 V1 Engine，比老的 V0 快且稳，但安装很麻烦。
- <strong>任务</strong>（仅限高阶玩家）：
    1. 卸载刚才装的 vLLM。
    2. 从 GitHub 下载 vLLM 源码。
    3. 切换到特定的代码版本（commit id: <code>2275784</code>）。
    4. 用 <code>sed</code> 命令修改一行源代码（关于并行计算大小的设置）。
    5. 重新编译安装。
    6. 设置环境变量 <code>export VLLM_USE_V1=1</code> 来激活它。
- <strong>收益</strong>：速度提升 1.5 倍，且解决了第三步中提到的“生成多条回复不稳定”的问题。</p>
<hr />
<h3>📝 总结（说人话版）</h3>
<ol>
<li><strong>首选</strong>：去看 vLLM 0.8 的文档，别折腾这个 0.7 了。</li>
<li><strong>次选</strong>（必须用0.7）：直接装 <code>vLLM 0.7.3</code>，然后在脚本里把 <code>enforce_eager</code> 设为 <code>False</code> 就能用了。</li>
<li><strong>不要做</strong>：别装 0.7.0 - 0.7.2，否则要手动改代码，很痛苦。</li>
<li><strong>高玩选做</strong>：自己编译源码开启 V1 引擎，获得最高性能。</li>
</ol>