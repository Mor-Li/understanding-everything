<h1>docs/faq/faq.rst</h1>
<p>这份文件是一个 <strong>FAQ（常见问题解答）</strong> 文档，主要针对一个名为 <code>verl</code>（看起来是一个基于 Ray 框架的大模型强化学习/训练库）的项目。</p>
<p>为了让你更容易理解，我将这份文档的内容转化为一份 <strong>“当你遇到问题时的排查清单 (Task To-Do List)”</strong>。你可以把它当作一个<strong>运维或开发手册</strong>来看。</p>
<p>我们将任务分为五个主要类别：</p>
<hr />
<h3>✅ 任务清单 1：解决 Ray 集群与分布式相关问题</h3>
<p><strong>场景：</strong> 当你在多台机器或集群（如 Slurm）上运行程序，底层框架 Ray 报错时。</p>
<ul>
<li><strong>[ ] 任务：如何在分布式 Ray 中打断点调试？</strong><ul>
<li><strong>行动：</strong> 不要直接用普通的 print 或 pdb，请参考 Ray 官方提供的调试指南链接。</li>
</ul>
</li>
<li><strong>[ ] 任务：遇到报错 "Unable to register worker with raylet"？</strong><ul>
<li><strong>原因：</strong> 通常是因为 Slurm 调度系统限制了 CPU 核心的可见性，导致 Ray 试图启动的 worker 数量超过了权限。</li>
<li><strong>行动：</strong> 修改配置项 <code>ray_init.num_cpus</code>，将其设置为系统允许的数值。</li>
</ul>
</li>
<li><strong>[ ] 任务：如何进行多节点（Multi-node）训练？</strong><ul>
<li><strong>行动：</strong><ol>
<li>先按 Ray 官方指南启动集群。</li>
<li>在配置文件中，将 <code>trainer.nnode</code> 设置为你的机器数量。</li>
</ol>
</li>
</ul>
</li>
<li><strong>[ ] 任务：如何在 Slurm 集群上运行 verl？</strong><ul>
<li><strong>行动步骤：</strong><ol>
<li><strong>环境准备：</strong> 如果集群支持 Apptainer/Singularity，将 verl 的 Docker 镜像转换为 Apptainer 镜像（文档提供了 pull 命令）；或者使用集群自带的包管理器。</li>
<li><strong>数据准备：</strong> 参考 GSM8K 的示例准备数据和模型权重。</li>
<li><strong>脚本修改：</strong> 修改 <code>examples/slurm/ray_on_slurm.slurm</code> 脚本，填入你集群的具体信息。</li>
<li><strong>提交任务：</strong> 使用 <code>sbatch</code> 命令提交。</li>
<li><em>提示：如果改了资源规格，记得更新环境变量。</em></li>
</ol>
</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ 任务清单 2：解决安装与编译报错</h3>
<p><strong>场景：</strong> 刚开始搭建环境，或者代码跑起来直接崩掉报 <code>Error</code> 时。</p>
<ul>
<li><strong>[ ] 任务：遇到 <code>NotImplementedError: TensorDict does not support membership checks...</code>？</strong><ul>
<li><strong>原因：</strong> 你可能在使用 linux-arm64 架构，且 <code>tensordict</code> 包的版本不兼容。</li>
<li><strong>行动（二选一）：</strong><ol>
<li><strong>方案一（推荐）：</strong> 卸载旧包，从 GitHub 源码编译安装 <code>tensordict</code> v0.6.2。</li>
<li><strong>方案二（临时）：</strong> 修改报错的那行代码，把 <code>key in tensordict_var</code> 改为 <code>key in tensordict_var.keys()</code>。</li>
</ol>
</li>
</ul>
</li>
<li><strong>[ ] 任务：遇到 Triton <code>compile_module_from_src</code> 编译错误？</strong><ul>
<li><strong>行动：</strong> 修改配置文件，设置 <code>use_torch_compile</code> 标志，禁用对融合算子的即时编译（JIT）。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ 任务清单 3：运行时错误与模型保存</h3>
<p><strong>场景：</strong> 模型正在训练或推理过程中出现的问题。</p>
<ul>
<li><strong>[ ] 任务：遇到 <code>CUDA error: an illegal memory access</code>？</strong><ul>
<li><strong>行动：</strong> 这通常是 vLLM（推理引擎）的问题。请去查阅你所使用的 vLLM 版本的官方文档来排查。</li>
</ul>
</li>
<li><strong>[ ] 任务：如何保存/转换模型 Checkpoint？</strong><ul>
<li><strong>行动：</strong> 如果你想把模型保存为 HuggingFace 的 <code>safetensor</code> 格式，请查看 <code>verl/model_merger</code> 相关的代码或文档。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ 任务清单 4：性能监控与网络设置</h3>
<p><strong>场景：</strong> 想要优化训练速度，或者解决网络连接问题。</p>
<ul>
<li><strong>[ ] 任务：搞不懂 Train / Mini / Micro Batch Size 的区别？</strong><ul>
<li><strong>行动：</strong> 文档提供了一个架构图（链接），用于解释这三者在训练循环中的层级关系。</li>
</ul>
</li>
<li><strong>[ ] 任务：如何分析训练任务的性能（生成 Ray Timeline）？</strong><ul>
<li><strong>行动：</strong><ol>
<li>设置配置项 <code>ray_init.timeline_json_file=/tmp/ray_timeline.json</code>（指定一个路径）。</li>
<li>任务结束后，下载该 JSON 文件。</li>
<li>使用 Chrome 浏览器的 <code>chrome://tracing</code> 或 Perfetto UI 打开该文件进行可视化分析。</li>
</ol>
</li>
</ul>
</li>
<li><strong>[ ] 任务：如何只给 WandB（日志工具）设置代理？</strong><ul>
<li><strong>原因：</strong> 有时你只需要日志走代理，而不想影响其他 HTTP 请求（如模型调用）。</li>
<li><strong>行动：</strong> 在启动脚本中添加参数 <code>+trainer.wandb_proxy=http://你的代理地址:端口</code>。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ 任务清单 5：高级训练精度问题（重要）</h3>
<p><strong>场景：</strong> 发现训练指标异常，比如 <code>actor/grad_norm</code>（梯度范数）一直变大，模型不收敛。</p>
<ul>
<li><strong>[ ] 任务：排查推理与训练的精度不匹配问题</strong><ul>
<li><strong>诊断步骤：</strong><ol>
<li>开启配置 <code>actor_rollout_ref.rollout.calculate_log_probs=True</code>。</li>
<li>观察指标 <code>training/rollout_probs_diff_mean</code>。</li>
<li><strong>判断标准：</strong> 正常应小于 0.005。如果大于 0.01，说明推理引擎（vLLM）和训练引擎精度不一致。</li>
</ol>
</li>
<li><strong>常见触发条件（同时满足时）：</strong><ol>
<li>使用非 Hopper 架构 GPU（如 A100, L20, B200）。</li>
<li>使用了特定版本的 vLLM（存在 bug）。</li>
<li>输入输出文本很长（如多轮对话、推理模型）。</li>
</ol>
</li>
<li><strong>解决方案：</strong><ul>
<li>添加配置：<code>+actor_rollout_ref.rollout.engine_kwargs.vllm.disable_cascade_attn=True</code>。</li>
<li><em>解释：这是为了绕过 vLLM 中 Flash Attention 的一个已知 Bug，直到 vLLM 发布新版本修复。</em></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<h3>总结</h3>
<p>这份文档就是一个<strong>“踩坑指南”</strong>。如果你是初次使用，重点关注 <strong>安装相关</strong> 和 <strong>Slurm 运行步骤</strong>；如果你是在调优模型，重点关注 <strong>Ray Timeline</strong> 和 <strong>精度不匹配（任务清单 5）</strong> 的部分。</p>