<h1>docs/preparation/reward_function.rst</h1>
<p>这份文档其实是在讲：<strong>在强化学习（RL）训练中，如果模型生成了一个回答，我们该如何给这个回答打分（Reward）？</strong></p>
<p>这就像是一个老师批改作业的指南。我把这份文档拆解成一个 <strong>4步走的 Task List</strong>，帮你一步步理清逻辑：</p>
<hr />
<h3>📋 Task 1: 理解核心概念（为什么要写这个？）</h3>
<p><strong>文档观点：</strong></p>
<blockquote>
<p>"For each dataset, we need to implement a reward function or utilize a reward model..."</p>
</blockquote>
<p><strong>你的任务：</strong>
你需要明白，强化学习（PPO）的核心是“奖惩机制”。
*   模型写了一个答案（Response）。
*   系统需要立刻判断这个答案好不好（Reward）。
*   <strong>这篇文档就是在教你：如何定义“什么是好答案”。</strong></p>
<p>目前系统主要支持两种打分方式：
1.  <strong>写代码规则打分（Reward Function）：</strong> 比如做数学题，答案对就是1分，错就是0分。（本文重点）
2.  <strong>用另一个AI打分（Reward Model）：</strong> 比如写作文，很难用规则判断，就用另一个模型来评分（针对RLHF或代码生成）。</p>
<hr />
<h3>📋 Task 2: 认识“阅卷官” (RewardManager)</h3>
<p><strong>文档观点：</strong></p>
<blockquote>
<p>"RewardManager... compute the score for each response."</p>
</blockquote>
<p><strong>你的任务：</strong>
了解系统内部是谁在干活。有一个叫 <code>RewardManager</code> 的类（在 <code>main_ppo.py</code> 里），它就是“阅卷官”。</p>
<p>它的工作流程是这样的（文档中提到的 DataProto）：
1.  <strong>接收试卷</strong>：拿到模型生成的 <code>responses</code>（回答）。
2.  <strong>接收标准答案</strong>：拿到数据集中预存的 <code>ground_truth</code>（正确答案）。
3.  <strong>接收科目</strong>：拿到 <code>data_source</code>（这是哪一种数据集，比如是数学还是代码）。
4.  <strong>开始批改</strong>：把回答解码成字符串，扔给具体的打分函数 <code>compute_score_fn</code>，最后得出一个分数。</p>
<hr />
<h3>📋 Task 3: 学习现有的“评分标准” (Pre-implemented)</h3>
<p><strong>文档观点：</strong></p>
<blockquote>
<p>"We already pre-implemented some reward functions... GSM8k and MATH"</p>
</blockquote>
<p><strong>你的任务：</strong>
看看别人是怎么制定评分规则的，以 <strong>GSM8k（小学数学题）</strong> 为例，文档里写了它的评分逻辑：</p>
<ul>
<li><strong>满分（Score 1.0）：</strong> 答案完全正确。</li>
<li><strong>辛苦分（Score 0.1）：</strong> 答案算错了，但是格式是对的（比如按照要求用了 <code>####</code> 符号引出答案）。</li>
<li><strong>零分（Score 0）：</strong> 格式也不对，答案也不对。</li>
</ul>
<p><em>这告诉你：奖励函数不仅可以看结果，还可以看格式规范。</em></p>
<hr />
<h3>📋 Task 4: 动手制定你自己的“评分标准” (Customized)</h3>
<p><strong>文档观点：</strong></p>
<blockquote>
<p>"You can implement customized reward functions..."</p>
</blockquote>
<p><strong>你的任务：</strong>
如果你用的是自己的数据集，现有的数学评分标准不适用，你需要自己写一个 Python 函数。</p>
<p><strong>怎么做？</strong>
1.  <strong>新建一个 Python 文件</strong>（比如 <code>my_reward.py</code>）。
2.  <strong>写一个函数</strong>，必须包含以下参数：
    ```python
    def my_reward_fn(data_source, solution_str, ground_truth, extra_info=None):
        # solution_str 是模型生成的回答
        # ground_truth 是正确答案</p>
<div class="codehilite"><pre><span></span><code>    # 举例：如果回答的长度越长，分数越高（仅仅是举例）
    return len(solution_str) / 100
```
</code></pre></div>

<ol>
<li><strong>告诉系统去哪找这个函数</strong>：
    在配置文件里修改：<ul>
<li><code>custom_reward_function.path</code>: 填你的文件路径。</li>
<li><code>custom_reward_function.name</code>: 填你的函数名（比如 <code>my_reward_fn</code>）。</li>
</ul>
</li>
</ol>
<hr />
<h3>💡 总结一下</h3>
<p>这篇文档就是告诉你：
<strong>要想训练模型，你得先告诉系统怎么给模型打分。你可以直接用现成的数学题打分逻辑，也可以自己写一个 Python 函数，根据标准答案给模型的输出算出一个数字（分数）。</strong></p>