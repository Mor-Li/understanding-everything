<h1>docs/ascend_tutorial/ascend_quick_start.rst</h1>
<p>这份文档其实是一份<strong>技术操作手册</strong>。</p>
<p>简单来说，它的核心目的是：<strong>教你怎么在华为昇腾（Ascend）的国产显卡（NPU）上，运行 <code>verl</code> 这个强化学习训练框架。</strong></p>
<p>通常这些AI框架都是默认在英伟达（NVIDIA）显卡上跑的，要在华为显卡上跑，需要安装特殊的驱动和软件。</p>
<p>为了让你看懂，我把它拆解成一个<strong>任务清单（To-Do List）</strong>，你可以把它想象成“把大象装进冰箱”的步骤，我们一步步来：</p>
<hr />
<h3>📋 任务清单 (To-Do List)</h3>
<ol>
<li><strong>【硬件确认】</strong>：检查你有没有华为的特定型号显卡。</li>
<li><strong>【基础环境搭建】</strong>：安装驱动和最底层的软件（类似给电脑装系统和显卡驱动）。</li>
<li><strong>【核心组件安装】</strong>：安装让AI模型能跑起来的加速引擎（vLLM）和训练框架（verl）。</li>
<li><strong>【可选组件安装】</strong>：如果你要跑超大模型，才需要装这个（MindSpeed）。</li>
<li><strong>【实战测试】</strong>：下载数据，跑一个测试命令，看能不能跑通。</li>
</ol>
<hr />
<h3>🚀 逐步详解 (Step-by-Step)</h3>
<p>下面我把文中的技术术语翻译成人话：</p>
<h4>第一步：硬件确认 (Hardware Support)</h4>
<p><strong>文中观点</strong>：不是所有华为卡都能跑，我只测试了这几种。
*   <strong>你需要做</strong>：确认你的服务器是不是以下型号之一：
    *   Atlas 200T A2 Box16
    *   Atlas 900 A2 PODc
    *   Atlas 800T A3
    *   <em>如果不是，后面的步骤可能跑不通。</em></p>
<h4>第二步：基础环境搭建 (Installation - Basic)</h4>
<p><strong>文中观点</strong>：软件版本必须严格对应，差一点都不行。
*   <strong>你需要做</strong>：
    1.  <strong>Docker（懒人方案）</strong>：文中推荐如果不想自己一步步装，可以直接用 Docker 镜像（类似直接下载一个装好软件的系统盘）。
    2.  <strong>手动安装（硬核方案）</strong>：
        *   <strong>Python</strong>：要在 3.10 到 3.12 之间。
        *   <strong>CANN (华为的驱动包)</strong>：必须是 8.3.RC1 版本。
        *   <strong>PyTorch (AI计算框架)</strong>：必须是 2.7.1。
        *   <strong>torch_npu (华为的插件)</strong>：这是让 PyTorch 认识华为显卡的关键插件，也要 2.7.1。
        *   <strong>其他依赖</strong>：安装 torchvision, triton-ascend 等辅助工具。</p>
<h4>第三步：安装推理加速引擎 (Install vLLM)</h4>
<p><strong>文中观点</strong>：训练过程中模型需要不断生成文本（Rollout），这步很慢，所以需要高性能的推理引擎。
*   <strong>你需要做</strong>：
    *   激活华为的环境变量（让系统找到驱动）。
    *   下载并安装 <strong>vLLM</strong> 和 <strong>vllm-ascend</strong>。
    *   <em>注意</em>：这是专门适配华为显卡的版本，不能直接 <code>pip install vllm</code>，得按文中的命令从源码编译安装。</p>
<h4>第四步：安装 verl 框架 (Install verl)</h4>
<p><strong>文中观点</strong>：这是主角，用来做强化学习（RL）训练的库。
*   <strong>你需要做</strong>：
    *   从 GitHub 下载 <code>verl</code> 的代码。
    *   安装针对 NPU（华为芯片）的依赖表 <code>requirements-npu.txt</code>。
    *   <strong>特别提醒</strong>：文中提到华为目前<strong>不支持</strong> <code>flash_attn</code> 和 <code>liger-kernel</code> 这两个常用的加速库，所以不要强行开启它们。</p>
<h4>第五步：(可选) 安装 MindSpeed</h4>
<p><strong>文中观点</strong>：如果你要用 Megatron（一种超大模型训练技术），你需要装这个。
*   <strong>你需要做</strong>：
    *   如果是普通训练，<strong>跳过此步</strong>。
    *   如果是大模型分布式训练，按文中指令安装 MindSpeed 和 Megatron-LM。</p>
<h4>第六步：快速开始/实战测试 (Quick Start)</h4>
<p><strong>文中观点</strong>：装了这么多，到底能不能用？跑个 Qwen2.5 (0.5B参数) 的小模型验证一下。
*   <strong>你需要做</strong>：
    1.  <strong>准备数据</strong>：运行文中提供的 <code>gsm8k.py</code> 脚本，下载并处理数据集。
    2.  <strong>执行训练命令</strong>：
        *   复制文中那一大段 <code>python3 -m verl.trainer.main_ppo ...</code> 的命令。
        *   这段命令配置了模型路径、显卡数量（8卡）、算法（GRPO）等参数。
        *   如果在终端里看到训练进度条在动，说明你成功了！</p>
<hr />
<h3>💡 总结文中的核心观点</h3>
<ol>
<li><strong>华为生态正在完善</strong>：虽然比英伟达麻烦点（需要源码编译、版本卡得死），但 <code>verl</code> 已经支持在华为 Ascend 芯片上跑 PPO、GRPO 等强化学习算法了。</li>
<li><strong>有局限性</strong>：明确告诉了你哪些加速库（如 flash attention）目前还用不了，不要踩坑。</li>
<li><strong>支持的模型</strong>：文末的表格列出了他们测试过的模型（Qwen系列为主），如果你用这些模型，成功率很高。</li>
</ol>
<p><strong>给你的建议</strong>：
如果你是开发者，先看有没有现成的 <strong>Docker 镜像</strong>，直接拉取镜像最省事。如果要手动装，严格按照它列出的 <code>version</code> 版本号来，不要随意升级。</p>