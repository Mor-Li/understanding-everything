<h1>fla/modules/feature_map.py</h1>
<p>完全理解你的感受。这段代码是深度学习中非常硬核的“线性注意力机制（Linear Attention）”的底层实现，里面混合了数学公式的近似、张量操作和神经网络层。如果不了解背景，看这代码就像看天书。</p>
<p>为了让你读懂，我们不直接读代码，而是设定一个<strong>学习任务清单 (Todo List)</strong>。我们把这个文件想象成一个<strong>兵器库</strong>，里面的每一个类都是一种为了让模型“跑得更快”而制造的特殊武器。</p>
<p>以下是你的任务清单，请按顺序阅读：</p>
<hr />
<h3>✅ Task 1: 搞懂“为什么”要写这个文件 (背景知识)</h3>
<ul>
<li><strong>痛点：</strong> 标准的 Transformer（比如 ChatGPT 用的）计算注意力时，要算 $Q \times K^T$。如果序列很长，这个计算量是爆炸的（平方级复杂度）。</li>
<li><strong>解决思路：</strong> 有一种叫“线性注意力”的技术，试图把计算量降下来。它的核心技巧是：<strong>不要直接算 $Q \times K$，而是先把 $Q$ 和 $K$ 分别通过一个函数 $\phi(\cdot)$ 映射一下</strong>，然后再算。</li>
<li><strong>这个文件的作用：</strong> 这个文件里定义的所有类（<code>...FeatureMap</code>），全都是那个函数 <strong>$\phi(\cdot)$</strong>。</li>
<li><strong>一句话总结：</strong> 这个文件就是一堆<strong>特征映射函数</strong>，目的是把输入的向量变形，以便后续能快速计算注意力。</li>
</ul>
<hr />
<h3>✅ Task 2: 攻克数学工具 (辅助函数)</h3>
<p>在看类之前，先看开头的两个函数，它们是后面很多类的基础工具。</p>
<ul>
<li><strong><code>flatten_diag_outer_product(x, y)</code></strong><ul>
<li><strong>直觉：</strong> 它是算两个向量的“外积”（Outer Product）。如果 $x=[a, b]$，$y=[c, d]$，外积就是 $\begin{bmatrix} ac &amp; ad \ bc &amp; bd \end{bmatrix}$。</li>
<li><strong>代码做了啥：</strong> 它算出这个矩阵，然后<strong>把它拉平</strong>，只取右上角的部分（因为很多时候矩阵是对称的，存一半就够了）。</li>
<li><strong>目的：</strong> 用来制造“高阶特征”（比如 $x^2$ 这种项），这是泰勒展开（Taylor Expansion）需要的。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 3: 理解最简单的“激活派” (基础映射)</h3>
<p>这部分最简单，就是把输入向量通过一个激活函数。</p>
<ul>
<li><strong>涉及类：</strong> <code>ReLUFeatureMap</code>, <code>GELUFeatureMap</code>, <code>SwishFeatureMap</code>, <code>SigmoidFeatureMap</code>。</li>
<li><strong>代码逻辑：</strong>
    <code>python
    def forward(self, x):
        return F.relu(x) # 或者 gelu, sigmoid...</code></li>
<li><strong>原理：</strong> 传统的 Softmax 出来的数值都是正的。为了模拟它，线性注意力通常要求 $\phi(x)$ 也是非负的。所以最简单的办法就是：<strong>直接套一个 ReLU 激活函数</strong>，把负数滤掉，既简单又保证非负。</li>
</ul>
<hr />
<h3>✅ Task 4: 理解“可学习派” (神经网络映射)</h3>
<p>这部分不再用固定的数学公式，而是用神经网络层（Linear）来自己学习如何映射。</p>
<ul>
<li><strong>涉及类：</strong> <code>T2RFeatureMap</code>, <code>HedgehogFeatureMap</code>。</li>
<li><strong><code>T2RFeatureMap</code> (Trainable to RNN):</strong><ul>
<li><strong>逻辑：</strong> 就是一个全连接层 (<code>nn.Linear</code>) 加上一个 <code>ReLU</code>。</li>
<li><strong>观点：</strong> 既然我们不知道什么样的映射最好，干脆让模型自己学一个投影矩阵。</li>
</ul>
</li>
<li><strong><code>HedgehogFeatureMap</code> (刺猬映射):</strong><ul>
<li><strong>名字来源：</strong> 论文名字叫 "The Hedgehog &amp; the Porcupine"（刺猬与豪猪）。</li>
<li><strong>逻辑：</strong> <code>cat([2*x, -2*x]).softmax(-1)</code>。它通过拼接正负项并做 Softmax，试图在保持线性的同时，模仿标准 Attention 中 Softmax 的分布特性。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 5: 理解“泰勒展开派” (高阶数学映射)</h3>
<p>这是最难懂的部分，也是代码里最复杂的数学实现。</p>
<ul>
<li><strong>涉及类：</strong> <code>TaylorFeatureMap</code>, <code>RebasedFeatureMap</code>。</li>
<li><strong>原理：</strong> 标准 Attention 用的是 $e^{q \cdot k}$。数学上，$e^x \approx 1 + x + \frac{x^2}{2}$。</li>
<li><strong>代码逻辑：</strong><ul>
<li>这些类试图构造出 $1$ (常数), $x$ (一阶项), $x^2$ (二阶项)。</li>
<li>还记得 Task 2 的 <code>flatten_diag_outer_product</code> 吗？它就是用来算 $x^2$ (二阶项) 的。</li>
<li><strong><code>TaylorFeatureMap</code></strong>：严格按照泰勒公式的系数（$\frac{1}{\sqrt{2}}$ 等）来拼接这些项。</li>
<li><strong><code>RebasedFeatureMap</code></strong>：思路一样，但它认为泰勒公式的系数不一定最好，所以它引入了可学习的参数（<code>gamma</code>, <code>beta</code>）来调整这些项的权重。</li>
</ul>
</li>
</ul>
<hr />
<h3>✅ Task 6: 理解“花式拼接派” (其他变体)</h3>
<p>剩下的几个类是一些特殊的论文提出的奇技淫巧。</p>
<ul>
<li><strong><code>DPFPFeatureMap</code>:</strong><ul>
<li><strong>逻辑：</strong> 把向量复制多份，每一份向后滚动（Roll）几位，然后乘起来。</li>
<li><strong>目的：</strong> 不增加参数，通过这种“错位重组”产生更丰富的数据特征。</li>
</ul>
</li>
<li><strong><code>HadamardFeatureMap</code>:</strong><ul>
<li><strong>逻辑：</strong> 两个线性层的输出直接对应位置相乘（Hadamard积）。</li>
</ul>
</li>
<li><strong><code>LearnablePolySketch...</code>:</strong><ul>
<li><strong>逻辑：</strong> 用“草图（Sketching）”算法的思想，通过降维再升维的方式来近似多项式特征。这是为了在特征维度很高时节省显存。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结 (你的通关秘籍)</h3>
<p>如果你要使用或修改这个文件，只需要记住这三点：</p>
<ol>
<li><strong>输入输出：</strong> 所有类的输入都是 <code>(Batch, Head, Length, Dim)</code>，输出是映射后的特征。</li>
<li><strong>核心目的：</strong> 它们都是为了把 $Q$ 和 $K$ 变成一种适合做“线性点积”的形式，通常要求<strong>非负</strong>且<strong>包含高阶信息</strong>。</li>
<li><strong>怎么选：</strong><ul>
<li>想要快且省显存？选 <code>ReLUFeatureMap</code> 或 <code>T2RFeatureMap</code>。</li>
<li>想要效果好、接近标准 Transformer？选 <code>HedgehogFeatureMap</code> 或 <code>RebasedFeatureMap</code>（因为它们更灵活或数学近似更准）。</li>
</ul>
</li>
</ol>
<p>现在你再回头看代码，把注意力放在 <code>forward</code> 函数里，就会发现它们其实只是在做简单的 <code>Linear</code> 变换或者 <code>cat</code> 拼接操作而已。</p>