<h1>legacy/training/README.md</h1>
<p>这份文档是一个名为 <strong>Flame</strong> 的项目的操作指南。简单来说，它是用来<strong>训练</strong>一种名为 FLA（Flash Linear Attention）的大语言模型的工具库。</p>
<p>文档开头有一个<strong>非常重要</strong>的提示：<strong>这个项目已经搬家了</strong>，现在的代码是“旧版本（Legacy）”，不再更新。如果你要用最新的功能，应该去新的仓库。</p>
<p>不过，如果你仍需使用这份旧代码，以下是根据文档内容整理的一份 <strong>Step-by-Step 任务清单（To-Do List）</strong>，带你一步步看懂它在讲什么：</p>
<h3>⚠️ 任务 0：确认版本（重要）</h3>
<ul>
<li><strong>检查点</strong>：文档第一段就警告说，<code>flame</code> 项目已经迁移到了基于 <code>torchtitan</code> 的新架构。</li>
<li><strong>行动</strong>：如果你是新开始的项目，建议先去点击文档里的 <a href="https://github.com/fla-org/flame">new repository</a> 链接看看新版。如果你确定要用这个旧版，请继续往下看。</li>
</ul>
<hr />
<h3>✅ 任务 1：环境搭建 (Setup)</h3>
<ul>
<li><strong>目标</strong>：把代码跑起来所需的基础软件装好。</li>
<li><strong>步骤</strong>：<ol>
<li>克隆（下载）代码仓库。</li>
<li>安装依赖包（<code>pip install .</code> 和 <code>pip install accelerate</code>）。</li>
<li><strong>避坑指南</strong>：文档特别提到 HuggingFace 的 <code>tokenizers</code> 库在处理长文本时有内存泄漏问题，<strong>必须</strong>安装 <code>0.20.4</code> 或以上版本。</li>
</ol>
</li>
</ul>
<hr />
<h3>✅ 任务 2：数据预处理 (Preprocessing)</h3>
<ul>
<li><strong>目标</strong>：在开始训练前，先把数据下载下来并转换成模型能读懂的格式（Tokenization）。</li>
<li><strong>步骤</strong>：<ol>
<li>使用作者提供的脚本 <code>preprocess.py</code>。</li>
<li><strong>示例 A</strong>：如果你用 <code>fineweb-edu</code> 数据集，文档给出了具体的命令，它会自动下载并把处理好的数据存在 <code>data/</code> 目录下。</li>
<li><strong>示例 B</strong>：如果你用 <code>SlimPajama</code> 数据集（通常用于 GLA 模型），因为数据太大，建议先用 <code>git lfs</code> 下载数据，再运行处理脚本。</li>
</ol>
</li>
</ul>
<hr />
<h3>✅ 任务 3：从头开始训练 (Training from Scratch)</h3>
<ul>
<li><strong>目标</strong>：不依赖现有模型，从零训练一个 340M（3.4亿参数）的小模型。</li>
<li><strong>步骤</strong>：<ol>
<li>运行 <code>bash train.sh</code> 脚本。</li>
<li><strong>理解参数</strong>：你需要配置很多参数，文档列出了表格解释：<ul>
<li><code>lr</code>: 学习率。</li>
<li><code>batch</code>: 每张显卡一次处理多少数据。</li>
<li><code>context</code>: 上下文长度（比如 2048）。</li>
<li><code>gpus</code> / <code>nodes</code>: 用多少张卡、多少台机器。</li>
</ul>
</li>
<li><strong>数学计算</strong>：文档教你如何计算“全局 Batch Size”（总共一次喂给模型多少数据）。<ul>
<li>公式：<code>batch_size</code> × <code>梯度累积步数</code> × <code>上下文长度</code> × <code>GPU数量</code> × <code>节点数量</code>。</li>
<li><em>警告</em>：修改参数时要小心，这会影响训练的总步数和预热步数。</li>
</ul>
</li>
<li><strong>断点续训</strong>：如果训练中断了，可以通过加一个 <code>checkpoint=路径</code> 参数，从上次断掉的地方继续练。</li>
<li><strong>监控</strong>：支持使用 <code>wandb</code> 来画图监控训练过程。</li>
</ol>
</li>
</ul>
<hr />
<h3>✅ 任务 4：继续预训练/微调 (Continual Pretraining)</h3>
<ul>
<li><strong>目标</strong>：不想从头练，想拿一个已经厉害的模型（比如 Mistral-7B）改成 FLA 架构继续练。</li>
<li><strong>步骤</strong>：<ol>
<li><strong>转换权重</strong>：先用 <code>convert_from_llama.py</code> 脚本，把 Mistral-7B 的权重“搬运”并转换成 GLA 模型的格式。</li>
<li><strong>开始微调</strong>：拿着转换好的模型路径，运行 <code>train.sh</code>。</li>
<li><strong>注意</strong>：对于 7B 这种大模型，单机训练可能不够，文档建议使用多节点（Multi-node）训练。</li>
</ol>
</li>
</ul>
<hr />
<h3>总结</h3>
<p>这篇文档的核心逻辑就是：
<strong>装环境 -&gt; 下数据并处理 -&gt; (选择 A: 从零训练 OR 选择 B: 拿别人的模型改一下继续练)</strong>。</p>