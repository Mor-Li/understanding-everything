<h1>CHANGELOG.md</h1>
<p>这份文件是 <strong>NVIDIA Megatron Core</strong> 的更新日志（Changelog）。</p>
<p><strong>简单来说，Megatron Core 是英伟达开发的一个“超大模型训练工具箱”。</strong> 它是专门给那些需要训练像 GPT-4、DeepSeek、Qwen 这种千亿参数大模型的工程师用的。</p>
<p>因为内容非常硬核，充满了底层优化的术语，看不懂很正常。为了让你理解，我把最新的 <strong>0.15.0 版本</strong>（也是文件中最长、最重要的部分）拆解成一个 <strong>“项目经理发给开发团队的 To-Do List”</strong>。</p>
<p>我们可以把这次更新想象成我们在<strong>升级一个超级工厂的流水线</strong>。以下是我们在这次更新中完成的任务清单：</p>
<hr />
<h3>✅ 任务清单 (基于 0.15.0 版本)</h3>
<h4>任务一：让流水线跑得更快、更省地皮 (Performance / 性能优化)</h4>
<p><strong>目标：</strong> 减少训练大模型的时间，节省显存。
1.  <strong>[完成] 预处理加速：</strong> 我们优化了位置编码（RoPE）的计算方式。
    *   <em>人话解释：</em> 以前每次都要现场算位置信息，现在我们把结果存好直接用，这块处理速度快了3倍，整体训练快了10-14%。
2.  <strong>[完成] 显存不够 CPU 来凑：</strong> 增加了 CPU activation offloading。
    *   <em>人话解释：</em> 显卡内存（显存）太贵太小，我们现在允许把一些暂时不用的数据临时搬到 CPU 内存里，虽然慢点，但能训练更大的模型了。
3.  <strong>[完成] 新型优化器：</strong> 支持了 Muon 优化器。
    *   <em>人话解释：</em> 引入了一种新的“炼丹炉火候控制算法”，可能让模型收敛得更好。</p>
<h4>任务二：升级“混合专家”系统 (MoE / Mixture of Experts)</h4>
<p><strong>背景：</strong> MoE 是现在的热门技术（比如 DeepSeek V3 就用了），意思是模型里有很多“专家”小脑瓜，遇到不同问题派不同专家。
<strong>目标：</strong> 让这种复杂的架构跑得更顺。
1.  <strong>[完成] 支持更低精度的计算 (FP8/FP4)：</strong>
    *   <em>人话解释：</em> 以前专家们算数用小数点后很多位（FP16/BF16），现在我们教他们用“估算”（8位甚至4位精度）来处理部分数据。算得稍微粗糙点，但速度极快，显存占用极低。
2.  <strong>[完成] 优化专家调度逻辑：</strong>
    *   <em>人话解释：</em> 调整了数据分配给专家的顺序（比如在分配前先做一些计算），减少排队等待时间。</p>
<h4>任务三：支持更多新奇的模型 (Model Support / 模型适配)</h4>
<p><strong>目标：</strong> 别家出了新架构，我们要能支持。
1.  <strong>[完成] 适配 Qwen3 和 GPT-OSS：</strong>
    *   <em>人话解释：</em> 增加了对这些特定模型架构的参数支持，想训练 Qwen3 的人可以直接用了。
2.  <strong>[完成] 语音和多模态支持：</strong>
    *   <em>人话解释：</em> 现在的模型不仅要看字，还要听声音。我们增加了处理语音数据的能力（Audio semantic reasoning）。</p>
<h4>任务四：让模型回答得更快 (Inference / 推理)</h4>
<p><strong>目标：</strong> 训练完的模型，拿来跟用户对话时要反应快。
1.  <strong>[完成] CUDA Graph 缓存优化：</strong>
    *   <em>人话解释：</em> 以前模型每走一步都要问显卡“下一步怎么走”，现在我们把地图（Graph）画好存起来，直接按地图跑。速度提升了 2 倍。
2.  <strong>[完成] 动态音频长度支持：</strong>
    *   <em>人话解释：</em> 处理长短不一的音频输入时，吞吐量提升了 2.5 倍。</p>
<h4>任务五：修补漏洞 (Bug Fixes / 捉虫)</h4>
<p><strong>目标：</strong> 修复之前版本会让程序崩溃或算错数的问题。
1.  <strong>[修复]</strong> 修复了检查点（Checkpoint）保存时丢失元数据的问题。（防止存了个寂寞）
2.  <strong>[修复]</strong> 修复了某些情况下梯度计算错误的问题。（防止模型越学越傻）
3.  <strong>[修复]</strong> 修复了在没有安装 CUDA 的机器上导入报错的问题。（提升易用性）</p>
<hr />
<h3>总结：这篇文档到底讲了啥？</h3>
<p>这篇文档的核心是在说：<strong>Megatron Core 0.15.0 版本是一个大更新。</strong></p>
<ol>
<li><strong>它极大地提升了效率</strong>（通过缓存优化、FP8低精度计算）。</li>
<li><strong>它紧跟潮流</strong>（重点优化了 MoE 架构，这是目前最火的大模型架构）。</li>
<li><strong>它开始兼顾多模态</strong>（不仅是文字，开始搞音频了）。</li>
</ol>
<p>如果你不是专门写代码训练大模型的工程师，你只需要知道：<strong>英伟达又把铲子磨快了，以后训练超级AI模型会更快、更便宜（显存利用率高）、支持的模型种类更多。</strong></p>