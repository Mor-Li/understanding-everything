<h1>tasks/data_utils.py</h1>
<p>这份代码其实就是一个<strong>“数据加工流水线”</strong>。</p>
<p>在自然语言处理（NLP，比如 BERT、GPT）模型中，你不能直接把一句话（比如 "Hello World"）扔给模型。你需要把这句话变成模型能看懂的<strong>数字格式</strong>，并且还要符合特定的<strong>长度规则</strong>。</p>
<p>这份文件 <code>tasks/data_utils.py</code> 的作用就是把原始文本（Raw Text）一步步加工成模型能吃的“压缩饼干”。</p>
<p>为了让你好理解，我把这个过程拆解成一个 <strong>5步走的 Task List（待办清单）</strong>。</p>
<hr />
<h3>📝 数据加工 Task List</h3>
<ol>
<li><strong>Task 1: 打扫卫生 (Cleaning)</strong><ul>
<li>把文本里的换行符、多余的空格清理掉，让句子变干净。</li>
</ul>
</li>
<li><strong>Task 2: 翻译成密码 (Tokenization)</strong><ul>
<li>把中文/英文单词转换成数字 ID（比如 "Hello" -&gt; 101, "World" -&gt; 202）。</li>
</ul>
</li>
<li><strong>Task 3: 加上“交通标志” (Special Tokens)</strong><ul>
<li>告诉模型哪里是句子的开始，哪里是句子的结束（加上 <code>[CLS]</code> 和 <code>[SEP]</code> 标志）。</li>
</ul>
</li>
<li><strong>Task 4: 削足适履 (Truncation &amp; Padding)</strong><ul>
<li><strong>太长了？</strong> 砍掉多余的。</li>
<li><strong>太短了？</strong> 补零凑齐长度。（这是最关键的一步逻辑）。</li>
</ul>
</li>
<li><strong>Task 5: 打包装箱 (Build Sample)</strong><ul>
<li>把处理好的所有数据转换成 Numpy 数组，打包成一个字典，准备发货给模型。</li>
</ul>
</li>
</ol>
<hr />
<h3>🔍 逐步详细讲解</h3>
<p>下面我结合代码，一步步讲这 5 个 Task 是怎么实现的。</p>
<h4>Task 1: 打扫卫生 (Cleaning)</h4>
<p><strong>对应函数：</strong> <code>clean_text(text)</code></p>
<ul>
<li><strong>观点：</strong> 原始数据很脏，有乱七八糟的换行和空格，会干扰模型。</li>
<li><strong>代码逻辑：</strong><ul>
<li><code>text.replace("\n", " ")</code>: 把换行符变成空格。</li>
<li><code>re.sub(r'\s+', ' ', text)</code>: 如果有连续好几个空格，合并成一个。</li>
<li><code>text.replace(' . ', '. ')</code>: 修正句号前后的空格。</li>
</ul>
</li>
</ul>
<h4>Task 2: 翻译成密码 (Tokenization)</h4>
<p><strong>对应函数：</strong> <code>build_tokens_types_paddings_from_text</code></p>
<ul>
<li><strong>观点：</strong> 计算机不认识字，只认识数字。</li>
<li><strong>代码逻辑：</strong><ul>
<li>它接收 <code>text_a</code>（第一句话）和 <code>text_b</code>（第二句话，可选）。</li>
<li>调用 <code>tokenizer.tokenize(text_a)</code>：把文本切分成 token（词元），比如把 "I love AI" 切分成 <code>['I', 'love', 'AI']</code>，然后变成对应的数字 ID 列表。</li>
</ul>
</li>
</ul>
<h4>Task 3: 加上“交通标志” (Special Tokens)</h4>
<p><strong>对应函数：</strong> <code>build_tokens_types_paddings_from_ids</code> (前半部分)</p>
<ul>
<li><strong>观点：</strong> BERT 类型的模型需要特殊的结构来区分句子。标准结构是：
    <code>[CLS] 句子A [SEP] 句子B [SEP]</code></li>
<li><strong>代码逻辑：</strong><ul>
<li><code>ids.append(cls_id)</code>: 在开头加一个开始标志。</li>
<li><code>ids.extend(text_a_ids)</code>: 放第一句话。</li>
<li><code>ids.append(sep_id)</code>: 加一个分隔符。</li>
<li>如果 <code>text_b</code> 存在，后面再放第二句话，最后再加个 <code>sep_id</code>。</li>
</ul>
</li>
</ul>
<h4>Task 4: 削足适履 (Truncation &amp; Padding) —— <strong>最核心部分</strong></h4>
<p><strong>对应函数：</strong> <code>build_tokens_types_paddings_from_ids</code> (后半部分)</p>
<ul>
<li>
<p><strong>观点：</strong> 模型的输入长度是固定的（比如 <code>max_seq_length</code> = 512）。</p>
<ul>
<li>如果你的句子只有 10 个字，剩下 502 个位置不能空着，要填满（Padding）。</li>
<li>如果你的句子有 1000 个字，塞不进去，必须砍掉（Truncation）。</li>
</ul>
</li>
<li>
<p><strong>代码逻辑：</strong></p>
<ol>
<li><strong>砍掉 (Trim):</strong>
    <code>python
    if len(ids) &gt;= max_seq_length:
         # 超过长度了，砍掉多余的，留出位置给最后的 [SEP]
         ids = ids[0:max_seq_length_m1]</code></li>
<li><strong>补齐 (Pad):</strong>
    <code>python
    padding_length = max_seq_length - len(ids)
    if padding_length &gt; 0:
        # 缺多少补多少个 0 (pad_id)
        ids.extend([pad_id] * padding_length)</code></li>
<li><strong>重点概念解释：</strong><ul>
<li><strong><code>ids</code></strong>: 真正的词 ID。</li>
<li><strong><code>types</code></strong>: 用来区分是第一句还是第二句。第一句全是 0，第二句全是 1，补齐的空位用 <code>pad_id</code>。</li>
<li><strong><code>paddings</code> (Mask)</strong>: 这是一个掩码（Mask）。<strong>1 代表“这是真内容”，0 代表“这是补位的假内容”</strong>。模型看到 0 就知道不用读了。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4>Task 5: 打包装箱 (Build Sample)</h4>
<p><strong>对应函数：</strong> <code>build_sample</code></p>
<ul>
<li><strong>观点：</strong> Python 的 List 列表处理速度慢，深度学习框架（如 PyTorch/TensorFlow）喜欢用 Numpy 数组。</li>
<li><strong>代码逻辑：</strong><ul>
<li><code>np.array(ids, dtype=np.int64)</code>: 把列表转成 Numpy 数组。</li>
<li>最后返回一个字典 <code>sample</code>，里面包含了模型训练需要的所有要素：<ul>
<li><code>text</code>: 词的 ID。</li>
<li><code>types</code>: 句子区分标记 (Segment ID)。</li>
<li><code>padding_mask</code>: 哪些是真词，哪些是补位的。</li>
<li><code>label</code>: 这条数据的标签（比如分类任务的答案）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：这段代码到底生成了什么？</h3>
<p>假设输入句子 A 是 "你好"，句子 B 是 "世界"，最大长度是 10。
这段代码最终会生成类似这样的数据（简化版）：</p>
<ol>
<li>
<p><strong><code>ids</code> (内容):</strong>
    <code>[CLS, 你, 好, SEP, 世, 界, SEP, PAD, PAD, PAD]</code>
    <em>(对应数字可能像: [101, 23, 45, 102, 67, 89, 102, 0, 0, 0])</em></p>
</li>
<li>
<p><strong><code>types</code> (谁是谁):</strong>
    <code>[0, 0, 0, 0, 1, 1, 1, 0, 0, 0]</code>
    <em>(0代表句子A，1代表句子B)</em></p>
</li>
<li>
<p><strong><code>paddings</code> (谁是真的):</strong>
    <code>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0]</code>
    <em>(1代表有用信息，0代表那是填充的废话)</em></p>
</li>
</ol>
<p>现在你看这段代码，是不是觉得它其实就是一个严谨的<strong>打包工</strong>？</p>