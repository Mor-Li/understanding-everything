<h1>megatron/core/models/multimodal/llava_spec.py</h1>
<p>è¿™ä»½ä»£ç ä¹ä¸€çœ‹ç¡®å®å…¨æ˜¯æœ¯è¯­ï¼Œä½†å…¶å®å®ƒ<strong>ä¸æ˜¯åœ¨å†™ç®—æ³•é€»è¾‘</strong>ï¼ˆæ¯”å¦‚æ€ä¹ˆåšåŠ å‡ä¹˜é™¤ï¼‰ï¼Œè€Œæ˜¯åœ¨<strong>å†™â€œé…ç½®å•â€</strong>ã€‚</p>
<p>ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆæˆ‘ä»¬åœ¨ç»„è£…ä¸€å°ç”µè„‘ã€‚ä½ ä¸éœ€è¦è‡ªå·±åˆ¶é€ CPUï¼Œä½ åªéœ€è¦å†™ä¸€å¼ å•å­ï¼šCPUç”¨Intelçš„ï¼Œæ˜¾å¡ç”¨NVIDIAçš„ï¼Œå†…å­˜ç”¨é‡‘å£«é¡¿çš„ã€‚</p>
<p>è¿™ä»½æ–‡ä»¶å°±æ˜¯ç»™ <strong>LLaVA</strong> æ¨¡å‹ï¼ˆä¸€ä¸ªèƒ½çœ‹æ‡‚å›¾ç‰‡çš„AIï¼‰çš„è§£ç å™¨ï¼ˆDecoderï¼‰å†™çš„ä¸€å¼ <strong>é›¶ä»¶ç»„è£…å•</strong>ã€‚</p>
<p>ä¸ºäº†è®©ä½ å½»åº•ææ‡‚ï¼Œæˆ‘ä¸ºä½ åˆ¶å®šäº†ä¸€ä¸ª <strong>5æ­¥èµ°çš„ Task Listï¼ˆå­¦ä¹ ä»»åŠ¡æ¸…å•ï¼‰</strong>ï¼š</p>
<hr />
<h3>ğŸ“‹ å­¦ä¹ ä»»åŠ¡æ¸…å• (Task List)</h3>
<ol>
<li><strong>Task 1ï¼šææ‡‚èƒŒæ™¯</strong> â€”â€” ä»€ä¹ˆæ˜¯ Megatron å’Œ ModuleSpecï¼Ÿï¼ˆè¿™æ˜¯æ ¸å¿ƒæ¦‚å¿µï¼‰</li>
<li><strong>Task 2ï¼šç†è§£â€œä¸¤å¥—æ–¹æ¡ˆâ€</strong> â€”â€” ä¸ºä»€ä¹ˆä»£ç é‡Œæœ‰ä¸¤ä¸ªé•¿å¾—å¾ˆåƒçš„å‡½æ•°ï¼Ÿ</li>
<li><strong>Task 3ï¼šæ‹†è§£æ–¹æ¡ˆ A (TEç‰ˆ)</strong> â€”â€” ä¹Ÿå°±æ˜¯ <code>decoder_model_with_transformer_engine_default_spec</code>ã€‚</li>
<li><strong>Task 4ï¼šæ‹†è§£æ–¹æ¡ˆ B (Localç‰ˆ)</strong> â€”â€” ä¹Ÿå°±æ˜¯ <code>decoder_model_with_local_default_spec</code>ã€‚</li>
<li><strong>Task 5ï¼šç†è§£å¼€å¤´çš„é‚£å † import</strong> â€”â€” ä¸»è¦æ˜¯ Apex æ˜¯å¹²å˜›çš„ã€‚</li>
</ol>
<hr />
<h3>ğŸš€ é€æ­¥è®²è§£</h3>
<h4>âœ… Task 1ï¼šææ‡‚èƒŒæ™¯ â€”â€” ä»€ä¹ˆæ˜¯ ModuleSpecï¼Ÿ</h4>
<p>åœ¨ Megatron-Coreï¼ˆè¿™æ˜¯ NVIDIA å¼€å‘çš„è®­ç»ƒè¶…å¤§æ¨¡å‹çš„åº“ï¼‰ä¸­ï¼Œä¸ºäº†çµæ´»ï¼Œå®ƒæŠŠæ¨¡å‹çš„<strong>ç»“æ„</strong>å’Œå…·ä½“çš„<strong>å®ç°ç±»</strong>åˆ†å¼€äº†ã€‚</p>
<ul>
<li><strong>é—®é¢˜</strong>ï¼šTransformer å±‚é€šå¸¸ç”± Attentionï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€MLPï¼ˆå‰é¦ˆç½‘ç»œï¼‰ã€LayerNormï¼ˆå½’ä¸€åŒ–ï¼‰ç»„æˆã€‚ä½†æ˜¯ï¼Œå®ç°è¿™äº›ç»„ä»¶çš„æ–¹å¼æœ‰å¾ˆå¤šç§ï¼ˆæ¯”å¦‚æ™®é€šçš„ PyTorch å®ç°ï¼Œæˆ–è€… NVIDIA ä¼˜åŒ–è¿‡çš„ Transformer Engine å®ç°ï¼‰ã€‚</li>
<li><strong>è§£å†³</strong>ï¼š<code>ModuleSpec</code> å°±æ˜¯ä¸€ä¸ªâ€œèœå•â€ã€‚å®ƒå‘Šè¯‰ç¨‹åºï¼šâ€œå½“ä½ è¦åˆ›å»ºä¸€ä¸ª Transformer å±‚æ—¶ï¼Œè¯·æŠŠ Attention éƒ¨åˆ†ç”¨ <code>Class A</code>ï¼ŒMLP éƒ¨åˆ†ç”¨ <code>Class B</code>â€ã€‚</li>
</ul>
<p><strong>ç»“è®ºï¼š</strong> è¿™ä¸ªæ–‡ä»¶çš„ä½œç”¨å°±æ˜¯<strong>å®šä¹‰ LLaVA æ¨¡å‹æ¯ä¸€å±‚è¯¥ç”¨å“ªäº›å…·ä½“çš„ Python ç±»æ¥æ„å»º</strong>ã€‚</p>
<h4>âœ… Task 2ï¼šç†è§£â€œä¸¤å¥—æ–¹æ¡ˆâ€</h4>
<p>ä½ ä¼šå‘ç°ä»£ç é‡Œå®šä¹‰äº†ä¸¤ä¸ªä¸»è¦çš„å‡½æ•°ï¼š
1.  <code>decoder_model_with_transformer_engine_default_spec</code>
2.  <code>decoder_model_with_local_default_spec</code></p>
<p><strong>ä¸ºä»€ä¹ˆè¦ä¸¤ä¸ªï¼Ÿ</strong>
*   <strong>æ–¹æ¡ˆ A (Transformer Engine / TE)</strong>ï¼šè¿™æ˜¯<strong>é«˜æ€§èƒ½ç‰ˆ</strong>ã€‚å®ƒä½¿ç”¨äº† NVIDIA ä¸“é—¨ä¸º H100/A100 æ˜¾å¡ä¼˜åŒ–çš„åº“ï¼ˆTransformer Engineï¼‰ï¼Œé€Ÿåº¦æå¿«ï¼Œæ”¯æŒ FP8 ç­‰é»‘ç§‘æŠ€ã€‚
*   <strong>æ–¹æ¡ˆ B (Local)</strong>ï¼šè¿™æ˜¯<strong>é€šç”¨å…¼å®¹ç‰ˆ</strong>ã€‚å¦‚æœä½ æ²¡æœ‰å®‰è£… TE åº“ï¼Œæˆ–è€…æƒ³ç”¨çº¯ PyTorch çš„åŸç”Ÿå®ç°è°ƒè¯•ï¼Œå°±ç”¨è¿™ä¸ªã€‚</p>
<h4>âœ… Task 3ï¼šæ‹†è§£æ–¹æ¡ˆ A (TEç‰ˆ)</h4>
<p>è®©æˆ‘ä»¬çœ‹ <code>decoder_model_with_transformer_engine_default_spec</code> è¿™ä¸ªå‡½æ•°ã€‚è¿™æ˜¯ NVIDIA æ¨èçš„é…ç½®ã€‚</p>
<p><strong>ä»£ç ç¿»è¯‘ï¼š</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># è¿”å›ä¸€ä¸ª ModuleSpecï¼ˆç»„è£…å•ï¼‰</span>
<span class="k">return</span> <span class="n">ModuleSpec</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">TransformerLayer</span><span class="p">,</span> <span class="c1"># ä¸»æ¡†æ¶æ˜¯ TransformerLayer</span>
    <span class="n">submodules</span><span class="o">=</span><span class="n">TransformerLayerSubmodules</span><span class="p">(</span> <span class="c1"># å…·ä½“çš„å­é›¶ä»¶å¦‚ä¸‹ï¼š</span>

        <span class="c1"># 1. è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Self Attention) çš„é…ç½®</span>
        <span class="n">self_attention</span><span class="o">=</span><span class="n">ModuleSpec</span><span class="p">(</span>
            <span class="n">module</span><span class="o">=</span><span class="n">SelfAttention</span><span class="p">,</span>
            <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;attn_mask_type&quot;</span><span class="p">:</span> <span class="n">AttnMaskType</span><span class="o">.</span><span class="n">causal</span><span class="p">},</span> <span class="c1"># å› æœæ©ç ï¼ˆå› ä¸ºæ˜¯ç”Ÿæˆæ¨¡å‹ï¼Œä¸èƒ½çœ‹åé¢çš„è¯ï¼‰</span>
            <span class="n">submodules</span><span class="o">=</span><span class="n">SelfAttentionSubmodules</span><span class="p">(</span>
                <span class="c1"># QKV æŠ•å½±å±‚ï¼šä½¿ç”¨ TE ä¼˜åŒ–çš„ã€å¸¦ LayerNorm çš„ã€åˆ—å¹¶è¡Œçº¿æ€§å±‚</span>
                <span class="n">linear_qkv</span><span class="o">=</span><span class="n">TELayerNormColumnParallelLinear</span><span class="p">,</span> 
                <span class="c1"># æ ¸å¿ƒæ³¨æ„åŠ›è®¡ç®—ï¼šä½¿ç”¨ TE ä¼˜åŒ–çš„ç‚¹ç§¯æ³¨æ„åŠ›</span>
                <span class="n">core_attention</span><span class="o">=</span><span class="n">TEDotProductAttention</span><span class="p">,</span> 
                <span class="c1"># è¾“å‡ºæŠ•å½±å±‚ï¼šä½¿ç”¨ TE ä¼˜åŒ–çš„è¡Œå¹¶è¡Œçº¿æ€§å±‚</span>
                <span class="n">linear_proj</span><span class="o">=</span><span class="n">TERowParallelLinear</span><span class="p">,</span>
                <span class="c1"># QK LayerNormï¼šè¿™æ˜¯ LLaVA/ViT æœ‰æ—¶ä¼šç”¨åˆ°çš„ç‰¹æ®Šå½’ä¸€åŒ–ï¼Œå¦‚æœæ²¡å¼€å¯å°±ç”¨ IdentityOp (ç©ºæ“ä½œ)</span>
                <span class="n">q_layernorm</span><span class="o">=</span><span class="n">TENorm</span> <span class="k">if</span> <span class="n">qk_layernorm</span> <span class="k">else</span> <span class="n">IdentityOp</span><span class="p">,</span>
                <span class="n">k_layernorm</span><span class="o">=</span><span class="n">TENorm</span> <span class="k">if</span> <span class="n">qk_layernorm</span> <span class="k">else</span> <span class="n">IdentityOp</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">),</span>

        <span class="c1"># 2. èåˆæ“ä½œï¼šæŠŠ Bias(åç½®) + Dropout + Add(æ®‹å·®è¿æ¥) åˆå¹¶æˆä¸€æ­¥ç®—ï¼Œä¸ºäº†å¿«</span>
        <span class="n">self_attn_bda</span><span class="o">=</span><span class="n">get_bias_dropout_add</span><span class="p">,</span>

        <span class="c1"># 3. MLP (å‰é¦ˆç½‘ç»œ) çš„é…ç½® (ä»å¤–éƒ¨å‡½æ•°è·å–)</span>
        <span class="n">mlp</span><span class="o">=</span><span class="n">mlp</span><span class="p">,</span>

        <span class="c1"># 4. MLP éƒ¨åˆ†çš„èåˆæ“ä½œ</span>
        <span class="n">mlp_bda</span><span class="o">=</span><span class="n">get_bias_dropout_add</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">)</span>
</code></pre></div>

<p><strong>å…³é”®ç‚¹ï¼š</strong> æ³¨æ„æ‰€æœ‰å¸¦ <code>TE</code> å‰ç¼€çš„ç±»ï¼ˆå¦‚ <code>TELayerNormColumnParallelLinear</code>ï¼‰ï¼Œè¿™ä»£è¡¨å®ƒä»¬æ˜¯ç¡¬ä»¶åŠ é€Ÿç‰ˆæœ¬ã€‚</p>
<h4>âœ… Task 4ï¼šæ‹†è§£æ–¹æ¡ˆ B (Localç‰ˆ)</h4>
<p>å†çœ‹ <code>decoder_model_with_local_default_spec</code>ã€‚</p>
<p><strong>ä»£ç ç¿»è¯‘ï¼š</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">return</span> <span class="n">ModuleSpec</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">TransformerLayer</span><span class="p">,</span>
    <span class="n">submodules</span><span class="o">=</span><span class="n">TransformerLayerSubmodules</span><span class="p">(</span>
        <span class="c1"># 1. è¾“å…¥å½’ä¸€åŒ–ï¼šè¿™é‡Œæ˜¾å¼æŒ‡å®šäº† LayerNorm çš„å®ç°ç±» (LNImpl)</span>
        <span class="n">input_layernorm</span><span class="o">=</span><span class="n">LNImpl</span><span class="p">,</span> 

        <span class="n">self_attention</span><span class="o">=</span><span class="n">ModuleSpec</span><span class="p">(</span>
            <span class="n">module</span><span class="o">=</span><span class="n">SelfAttention</span><span class="p">,</span>
            <span class="n">submodules</span><span class="o">=</span><span class="n">SelfAttentionSubmodules</span><span class="p">(</span>
                <span class="c1"># æ³¨æ„è¿™é‡Œæ²¡æœ‰ TE å‰ç¼€ï¼Œç”¨çš„æ˜¯æ ‡å‡†çš„å¹¶è¡Œçº¿æ€§å±‚</span>
                <span class="n">linear_qkv</span><span class="o">=</span><span class="n">ColumnParallelLinear</span><span class="p">,</span> 
                <span class="c1"># æ ‡å‡†çš„ç‚¹ç§¯æ³¨æ„åŠ›</span>
                <span class="n">core_attention</span><span class="o">=</span><span class="n">DotProductAttention</span><span class="p">,</span> 
                <span class="n">linear_proj</span><span class="o">=</span><span class="n">RowParallelLinear</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="c1"># ... å…¶ä»–éƒ¨åˆ†ç±»ä¼¼</span>
    <span class="p">),</span>
<span class="p">)</span>
</code></pre></div>

<p><strong>å…³é”®ç‚¹ï¼š</strong> è¿™é‡Œç”¨çš„éƒ½æ˜¯ <code>megatron.core.tensor_parallel</code> ä¸‹çš„æ ‡å‡†å±‚ï¼Œä¸ä¾èµ– Transformer Engine åº“ã€‚</p>
<h4>âœ… Task 5ï¼šç†è§£å¼€å¤´çš„é‚£å † import (Apex å¤„ç†)</h4>
<p>æ–‡ä»¶æœ€å¼€å¤´æœ‰ä¸€æ®µ <code>try...except</code> ä»£ç ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">apex</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">megatron.core.fusions.fused_layer_norm</span><span class="w"> </span><span class="kn">import</span> <span class="n">FusedLayerNorm</span>
    <span class="n">HAVE_APEX</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">LNImpl</span> <span class="o">=</span> <span class="n">FusedLayerNorm</span> <span class="c1"># å¦‚æœè£…äº† Apexï¼Œå½’ä¸€åŒ–å°±ç”¨ Apex çš„åŠ é€Ÿç‰ˆ</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># å¦‚æœæ²¡è£… Apexï¼Œå°±ç”¨ PyTorch åŸç”Ÿçš„å½’ä¸€åŒ–</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">megatron.core.transformer.torch_norm</span><span class="w"> </span><span class="kn">import</span> <span class="n">WrappedTorchNorm</span>
    <span class="n">LNImpl</span> <span class="o">=</span> <span class="n">WrappedTorchNorm</span>
    <span class="n">HAVE_APEX</span> <span class="o">=</span> <span class="kc">False</span>
</code></pre></div>

<p><strong>è§£é‡Šï¼š</strong>
Apex æ˜¯ NVIDIA æ—©æœŸçš„ä¸€ä¸ªåŠ é€Ÿæ‰©å±•åº“ã€‚è¿™æ®µä»£ç æ˜¯åœ¨åš<strong>ç¯å¢ƒæ£€æŸ¥</strong>ï¼š
*   å¦‚æœä½ çš„ç¯å¢ƒé‡Œè£…äº† <code>apex</code>ï¼Œé‚£ä¹ˆ <code>LNImpl</code> (LayerNorm Implementation) å°±ç”¨ <code>FusedLayerNorm</code>ï¼ˆæ›´å¿«ï¼‰ã€‚
*   å¦‚æœä½ æ²¡è£…ï¼Œä¸ºäº†é˜²æ­¢ç¨‹åºæŠ¥é”™å´©æºƒï¼Œå®ƒä¼šè‡ªåŠ¨é™çº§ä½¿ç”¨ <code>WrappedTorchNorm</code>ï¼ˆæ…¢ä¸€ç‚¹ï¼Œä½†èƒ½è·‘ï¼‰ã€‚</p>
<hr />
<h3>ğŸ’¡ æ€»ç»“ (Takeaway)</h3>
<p>è¿™ä¸ªæ–‡ä»¶ <strong><code>llava_spec.py</code></strong> å®é™…ä¸Šæ˜¯åœ¨å›ç­”ä¸€ä¸ªé—®é¢˜ï¼š</p>
<blockquote>
<p><strong>â€œå½“æˆ‘è¦è®­ç»ƒ LLaVA æ¨¡å‹æ—¶ï¼Œæˆ‘è¯¥ç”¨å“ªäº›ç§¯æœ¨å—æ¥æ­æ¨¡å‹ï¼Ÿâ€</strong></p>
</blockquote>
<ul>
<li>å¦‚æœä½ æœ‰é«˜çº§æ˜¾å¡ä¸”ç¯å¢ƒé…ç½®å®Œç¾ï¼Œç³»ç»Ÿä¼šè°ƒç”¨ <code>decoder_model_with_transformer_engine_default_spec</code>ï¼Œç”¨å…¨å¥— <strong>TE (Transformer Engine)</strong> ç§¯æœ¨ï¼Œè·‘å¾—é£å¿«ã€‚</li>
<li>å¦‚æœä½ åªæ˜¯åœ¨æ™®é€šç¯å¢ƒè·‘ï¼Œæˆ–è€…æƒ³åšåŸºç¡€ç ”ç©¶ï¼Œç³»ç»Ÿå¯èƒ½ä¼šè°ƒç”¨ <code>decoder_model_with_local_default_spec</code>ï¼Œç”¨æ™®é€šç§¯æœ¨ã€‚</li>
</ul>
<p>å®ƒä¸åŒ…å«å¤æ‚çš„æ•°å­¦æ¨å¯¼ï¼Œçº¯ç²¹æ˜¯<strong>ä¾èµ–æ³¨å…¥ï¼ˆDependency Injectionï¼‰</strong>çš„ä¸€ç§é…ç½®å†™æ³•ã€‚</p>