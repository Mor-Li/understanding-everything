<h1>megatron/core/models/bert/bert_layer_specs.py</h1>
<p>è¿™ä»½ä»£ç ç¡®å®å……æ»¡äº†æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆMegatron-Coreï¼‰ç‰¹æœ‰çš„â€œé»‘è¯â€å’Œè®¾è®¡æ¨¡å¼ã€‚å¦‚æœä½ ä¸ç†Ÿæ‚‰Megatronçš„æ¶æ„ï¼Œçœ‹ä¸æ‡‚æ˜¯å¾ˆæ­£å¸¸çš„ã€‚</p>
<p>ç®€å•æ¥è¯´ï¼Œ<strong>è¿™ä¸ªæ–‡ä»¶ä¸æ˜¯åœ¨å†™ç®—æ³•é€»è¾‘ï¼Œè€Œæ˜¯åœ¨å†™â€œé…ç½®èœå•â€</strong>ã€‚å®ƒå®šä¹‰äº†å¦‚ä½•ç”¨ä¸åŒçš„é›¶ä»¶ï¼ˆæ¨¡å—ï¼‰ç»„è£…å‡ºä¸€ä¸ª BERT æ¨¡å‹çš„ä¸€å±‚ï¼ˆLayerï¼‰ã€‚</p>
<p>ä¸ºäº†å¸®ä½ ç†è§£ï¼Œæˆ‘åˆ¶å®šäº†ä¸€ä¸ª <strong>5æ­¥å­¦ä¹ ä»»åŠ¡æ¸…å• (To-Do List)</strong>ï¼Œæˆ‘ä»¬ä¸€æ­¥æ­¥æ¥æ‹†è§£è¿™ä¸ªæ–‡ä»¶ã€‚</p>
<hr />
<h3>ğŸ“‹ å­¦ä¹ ä»»åŠ¡æ¸…å• (To-Do List)</h3>
<ol>
<li><strong>Task 1: ç†è§£æ ¸å¿ƒæ¦‚å¿µ â€”â€” ä»€ä¹ˆæ˜¯ <code>ModuleSpec</code>ï¼Ÿï¼ˆç»„è£…è¯´æ˜ä¹¦ï¼‰</strong></li>
<li><strong>Task 2: æ£€æŸ¥å·¥å…·ç®± â€”â€” å¯¼å…¥ä¸ä¾èµ–æ£€æŸ¥ (Imports &amp; Checks)</strong></li>
<li><strong>Task 3: è§£æâ€œé¡¶é…ç‰ˆâ€é…ç½® â€”â€” Transformer Engine (TE) æ¨¡å¼</strong></li>
<li><strong>Task 4: è§£æâ€œæ ‡å‡†ç‰ˆâ€é…ç½® â€”â€” Local Spec æ¨¡å¼</strong></li>
<li><strong>Task 5: æ€»ç»“ â€”â€” è¿™ä¸ªæ–‡ä»¶åˆ°åº•åœ¨å¹²å˜›ï¼Ÿ</strong></li>
</ol>
<hr />
<h3>ğŸŸ¢ Task 1: ç†è§£æ ¸å¿ƒæ¦‚å¿µ â€”â€” ä»€ä¹ˆæ˜¯ <code>ModuleSpec</code>ï¼Ÿ</h3>
<p>åœ¨çœ‹ä»£ç å‰ï¼Œå…ˆå»ºç«‹ä¸€ä¸ªæ¦‚å¿µã€‚Megatron ä¸ºäº†è®©ä»£ç çµæ´»ï¼Œä¸ç›´æ¥åœ¨æ¨¡å‹é‡Œå†™æ­»ç”¨å“ªä¸ª Layerï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ç§â€œä¾èµ–æ³¨å…¥â€çš„æ¨¡å¼ã€‚</p>
<ul>
<li><strong>æ¯”å–»</strong>ï¼šä½ è¦ç»„è£…ä¸€å°ç”µè„‘ï¼ˆBERT Layerï¼‰ã€‚</li>
<li><strong>ä¼ ç»Ÿå†™æ³•</strong>ï¼šä»£ç é‡Œå†™æ­»â€œå¿…é¡»ç”¨ä¸‰æ˜Ÿå†…å­˜æ¡â€ã€‚</li>
<li><strong>Megatronå†™æ³•ï¼ˆModuleSpecï¼‰</strong>ï¼šä»£ç é‡Œç•™ä¸ªç©ºä½ï¼Œè¯´â€œè¿™é‡Œéœ€è¦æ’ä¸€ä¸ªå†…å­˜æ¡â€ã€‚ç„¶åé€šè¿‡è¿™ä¸ªæ–‡ä»¶ï¼ˆSpecï¼‰å‘Šè¯‰ç³»ç»Ÿï¼šâ€œè¿™æ¬¡è¯·ç»™æˆ‘ç”¨ä¸‰æ˜Ÿçš„ï¼Œä¸‹æ¬¡è¯·ç»™æˆ‘ç”¨é‡‘å£«é¡¿çš„â€ã€‚</li>
</ul>
<p><strong>ç»“è®º</strong>ï¼šè¿™ä¸ªæ–‡ä»¶é‡Œçš„ <code>ModuleSpec</code> å°±æ˜¯ä¸€å¼ å¼ <strong>é…ç½®å•</strong>ï¼Œå‘Šè¯‰ç³»ç»Ÿæ¯ä¸€å±‚å…·ä½“è¯¥ç”¨å“ªä¸ª Python Class å»å®ä¾‹åŒ–ã€‚</p>
<hr />
<h3>ğŸŸ¢ Task 2: æ£€æŸ¥å·¥å…·ç®± â€”â€” å¯¼å…¥ä¸ä¾èµ–æ£€æŸ¥</h3>
<p>ä»£ç å¼€å¤´çš„ä¸€å¤§æ®µ <code>try...except</code> æ˜¯åœ¨æ£€æŸ¥ä½ çš„ç¯å¢ƒé‡Œè£…äº†ä»€ä¹ˆåŠ é€ŸåŒ…ã€‚</p>
<div class="codehilite"><pre><span></span><code><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">transformer_engine</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">te</span>
    <span class="c1"># ... å¯¼å…¥ TE ç›¸å…³çš„ç‰¹åˆ¶å±‚ ...</span>
    <span class="n">HAVE_TE</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># æ ‡è®°ï¼šæˆ‘æœ‰ TE å¼•æ“</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">HAVE_TE</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># æ ‡è®°ï¼šæˆ‘æ²¡æœ‰ TE å¼•æ“</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">apex</span>
    <span class="c1"># ... å¯¼å…¥ Apex çš„ FusedLayerNorm ...</span>
    <span class="n">HAVE_APEX</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">LNImpl</span> <span class="o">=</span> <span class="n">FusedLayerNorm</span> <span class="c1"># å¦‚æœæœ‰ Apexï¼Œå½’ä¸€åŒ–å±‚å°±ç”¨ Apex çš„ï¼ˆæ›´å¿«ï¼‰</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># ... æ²¡ Apex å°±ç”¨ PyTorch è‡ªå¸¦çš„ ...</span>
    <span class="n">LNImpl</span> <span class="o">=</span> <span class="n">WrappedTorchNorm</span> <span class="c1"># é™çº§æ–¹æ¡ˆ</span>
</code></pre></div>

<p><strong>è§£è¯»ï¼š</strong>
*   <strong>Transformer Engine (TE)</strong>: NVIDIA ä¸“é—¨ä¸º H100/A100 æ˜¾å¡å¼€å‘çš„åŠ é€Ÿåº“ï¼Œæ”¯æŒ FP8 è®­ç»ƒã€‚å¦‚æœæœ‰è¿™ä¸ªåº“ï¼Œå°±èƒ½ç”¨æ›´å¿«çš„å±‚ã€‚
*   <strong>Apex</strong>: ä¹Ÿæ˜¯ NVIDIA çš„ä¸€ä¸ªåŠ é€Ÿåº“ã€‚å¦‚æœæœ‰ï¼ŒLayerNormï¼ˆå±‚å½’ä¸€åŒ–ï¼‰å°±ç”¨ Apex ç‰ˆçš„ï¼›æ²¡æœ‰å°±ç”¨ PyTorch åŸç”Ÿçš„ã€‚</p>
<hr />
<h3>ğŸŸ¢ Task 3: è§£æâ€œé¡¶é…ç‰ˆâ€é…ç½® â€”â€” Transformer Engine (TE) æ¨¡å¼</h3>
<p>æ¥ä¸‹æ¥çœ‹ç¬¬ä¸€ä¸ªå‡½æ•° <code>get_bert_layer_with_transformer_engine_spec</code>ã€‚è¿™æ˜¯ä¸ºäº†è¿½æ±‚æè‡´æ€§èƒ½çš„é…ç½®ã€‚</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_bert_layer_with_transformer_engine_spec</span><span class="p">():</span>
    <span class="c1"># ... æ£€æŸ¥æœ‰æ²¡æœ‰å®‰è£… TEï¼Œæ²¡è£…å°±æŠ¥é”™ ...</span>

    <span class="k">return</span> <span class="n">ModuleSpec</span><span class="p">(</span>
        <span class="n">module</span><span class="o">=</span><span class="n">TransformerLayer</span><span class="p">,</span> <span class="c1"># ä¸»æ¡†æ¶ï¼šè¿™æ˜¯ä¸€ä¸ª Transformer å±‚</span>
        <span class="n">submodules</span><span class="o">=</span><span class="n">TransformerLayerSubmodules</span><span class="p">(</span> <span class="c1"># å­æ¨¡å—ç»†èŠ‚ï¼š</span>
            <span class="n">self_attention</span><span class="o">=</span><span class="n">ModuleSpec</span><span class="p">(</span>
                <span class="n">module</span><span class="o">=</span><span class="n">SelfAttention</span><span class="p">,</span>
                <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;attn_mask_type&quot;</span><span class="p">:</span> <span class="n">AttnMaskType</span><span class="o">.</span><span class="n">padding</span><span class="p">},</span> <span class="c1"># BERT ç‰¹æ€§ï¼šèƒ½çœ‹åˆ°ä¸Šä¸‹æ–‡ï¼Œæ‰€ä»¥åª mask æ‰ padding</span>
                <span class="n">submodules</span><span class="o">=</span><span class="n">SelfAttentionSubmodules</span><span class="p">(</span>
                    <span class="c1"># é‡ç‚¹æ¥äº†ï¼è¿™é‡Œç”¨çš„å…¨æ˜¯ TE å¼€å¤´çš„ç±»</span>
                    <span class="n">linear_qkv</span><span class="o">=</span><span class="n">TELayerNormColumnParallelLinear</span><span class="p">,</span> <span class="c1"># èåˆäº† LayerNorm çš„çº¿æ€§å±‚</span>
                    <span class="n">core_attention</span><span class="o">=</span><span class="n">TEDotProductAttention</span><span class="p">,</span>       <span class="c1"># TE ä¼˜åŒ–çš„æ³¨æ„åŠ›è®¡ç®—</span>
                    <span class="n">linear_proj</span><span class="o">=</span><span class="n">TERowParallelLinear</span><span class="p">,</span>            <span class="c1"># TE ä¼˜åŒ–çš„è¡Œå¹¶è¡Œçº¿æ€§å±‚</span>
                    <span class="n">q_layernorm</span><span class="o">=</span><span class="n">IdentityOp</span><span class="p">,</span> <span class="c1"># å› ä¸ºä¸Šé¢ linear_qkv å·²ç»èåˆäº† Normï¼Œè¿™é‡Œå°±è®¾ä¸ºç©ºæ“ä½œ(Identity)</span>
                    <span class="n">k_layernorm</span><span class="o">=</span><span class="n">IdentityOp</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">),</span>
            <span class="c1"># ... MLP éƒ¨åˆ†ä¹Ÿç”¨ TE çš„å±‚ ...</span>
            <span class="n">mlp</span><span class="o">=</span><span class="n">ModuleSpec</span><span class="p">(</span>
                <span class="n">module</span><span class="o">=</span><span class="n">MLP</span><span class="p">,</span>
                <span class="n">submodules</span><span class="o">=</span><span class="n">MLPSubmodules</span><span class="p">(</span>
                    <span class="n">linear_fc1</span><span class="o">=</span><span class="n">TELayerNormColumnParallelLinear</span><span class="p">,</span> 
                    <span class="n">linear_fc2</span><span class="o">=</span><span class="n">TERowParallelLinear</span>
                <span class="p">),</span>
            <span class="p">),</span>
        <span class="p">),</span>
    <span class="p">)</span>
</code></pre></div>

<p><strong>è§£è¯»ï¼š</strong>
*   è¿™ä¸ªé…ç½®å•æ˜¯ç»™<strong>é«˜æ€§èƒ½æ˜¾å¡</strong>ç”¨çš„ã€‚
*   å®ƒæŠŠæ ‡å‡†çš„ Linear å±‚æ›¿æ¢æˆäº† <code>TELayerNormColumnParallelLinear</code>ã€‚è¿™æ˜¯ä¸€ç§<strong>ç®—å­èåˆ</strong>æŠ€æœ¯ï¼ŒæŠŠâ€œå½’ä¸€åŒ–â€å’Œâ€œçŸ©é˜µä¹˜æ³•â€ä¸¤ä¸ªåŠ¨ä½œåˆæˆä¸€ä¸ªåŠ¨ä½œåšï¼Œé€Ÿåº¦æå¿«ã€‚
*   å®ƒæŒ‡å®šäº† Attention ç±»å‹ä¸º <code>AttnMaskType.padding</code>ï¼Œè¿™æ˜¯ BERT å’Œ GPT çš„åŒºåˆ«ï¼ˆBERT æ˜¯åŒå‘çœ‹ï¼ŒGPT æ˜¯ä»å·¦å¾€å³çœ‹ï¼‰ã€‚</p>
<hr />
<h3>ğŸŸ¢ Task 4: è§£æâ€œæ ‡å‡†ç‰ˆâ€é…ç½® â€”â€” Local Spec æ¨¡å¼</h3>
<p>ä»£ç æœ€åå®šä¹‰çš„ <code>bert_layer_local_spec</code> æ˜¯é€šç”¨é…ç½®ï¼Œä¸ä¾èµ– Transformer Engineã€‚</p>
<div class="codehilite"><pre><span></span><code><span class="n">bert_layer_local_spec</span> <span class="o">=</span> <span class="n">ModuleSpec</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">TransformerLayer</span><span class="p">,</span>
    <span class="n">submodules</span><span class="o">=</span><span class="n">TransformerLayerSubmodules</span><span class="p">(</span>
        <span class="n">input_layernorm</span><span class="o">=</span><span class="n">LNImpl</span><span class="p">,</span> <span class="c1"># ä½¿ç”¨ Task 2 ä¸­å†³å®šçš„ Norm (Apex æˆ– Torch)</span>
        <span class="n">self_attention</span><span class="o">=</span><span class="n">ModuleSpec</span><span class="p">(</span>
            <span class="n">module</span><span class="o">=</span><span class="n">SelfAttention</span><span class="p">,</span>
            <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;attn_mask_type&quot;</span><span class="p">:</span> <span class="n">AttnMaskType</span><span class="o">.</span><span class="n">padding</span><span class="p">},</span>
            <span class="n">submodules</span><span class="o">=</span><span class="n">SelfAttentionSubmodules</span><span class="p">(</span>
                <span class="c1"># é‡ç‚¹ï¼šè¿™é‡Œç”¨çš„æ˜¯æ ‡å‡†çš„ Megatron Core å¹¶è¡Œå±‚ï¼Œæ²¡æœ‰ TE å‰ç¼€</span>
                <span class="n">linear_qkv</span><span class="o">=</span><span class="n">ColumnParallelLinear</span><span class="p">,</span> 
                <span class="n">core_attention</span><span class="o">=</span><span class="n">DotProductAttention</span><span class="p">,</span>
                <span class="n">linear_proj</span><span class="o">=</span><span class="n">RowParallelLinear</span><span class="p">,</span>
                <span class="c1"># ...</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="c1"># ...</span>
        <span class="n">mlp</span><span class="o">=</span><span class="n">ModuleSpec</span><span class="p">(</span>
            <span class="n">module</span><span class="o">=</span><span class="n">MLP</span><span class="p">,</span>
            <span class="n">submodules</span><span class="o">=</span><span class="n">MLPSubmodules</span><span class="p">(</span>
                <span class="n">linear_fc1</span><span class="o">=</span><span class="n">ColumnParallelLinear</span><span class="p">,</span> <span class="c1"># æ ‡å‡†åˆ—å¹¶è¡Œ</span>
                <span class="n">linear_fc2</span><span class="o">=</span><span class="n">RowParallelLinear</span>     <span class="c1"># æ ‡å‡†è¡Œå¹¶è¡Œ</span>
            <span class="p">),</span>
        <span class="p">),</span>
        <span class="c1"># è¿™æ˜¯ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œå‘Šè¯‰åŠ è½½æƒé‡æ—¶ï¼Œåå­—æ€ä¹ˆå¯¹åº”</span>
        <span class="n">sharded_state_dict_keys_map</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;input_layernorm.&quot;</span><span class="p">:</span> <span class="s2">&quot;self_attention.linear_qkv.layer_norm_&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pre_mlp_layernorm.&quot;</span><span class="p">:</span> <span class="s2">&quot;mlp.linear_fc1.layer_norm_&quot;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">),</span>
<span class="p">)</span>
</code></pre></div>

<p><strong>è§£è¯»ï¼š</strong>
*   è¿™æ˜¯<strong>å…¼å®¹æ€§æœ€å¥½</strong>çš„é…ç½®ã€‚
*   å®ƒä½¿ç”¨çš„æ˜¯ <code>ColumnParallelLinear</code> å’Œ <code>RowParallelLinear</code>ã€‚è¿™æ˜¯ Megatron å®ç°å¤§æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¼ é‡å¹¶è¡Œï¼‰çš„æ ¸å¿ƒå±‚ï¼ŒæŠŠçŸ©é˜µåˆ‡å¼€æ”¾åœ¨ä¸åŒ GPU ä¸Šç®—ã€‚
*   å®ƒæ²¡æœ‰ç”¨é‚£äº›èŠ±å“¨çš„èåˆç®—å­ï¼ˆTEï¼‰ï¼Œç»“æ„æ›´æ ‡å‡†ã€‚</p>
<hr />
<h3>ğŸŸ¢ Task 5: æ€»ç»“ â€”â€” è¿™ä¸ªæ–‡ä»¶åˆ°åº•åœ¨å¹²å˜›ï¼Ÿ</h3>
<p>åšå®Œä»¥ä¸Šä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªæ–‡ä»¶çœ‹ä½œæ˜¯ä¸€ä¸ª <strong>â€œBERT å±‚ç»„è£…å‘å¯¼â€</strong>ã€‚</p>
<ol>
<li><strong>ç›®çš„</strong>ï¼šå®ƒä¸æ‰§è¡Œè®¡ç®—ï¼Œå®ƒåªè´Ÿè´£<strong>å®šä¹‰</strong>ã€‚</li>
<li><strong>æ ¸å¿ƒå†…å®¹</strong>ï¼šæä¾›äº†ä¸¤å¥—æ–¹æ¡ˆï¼ˆSpecï¼‰ï¼š<ul>
<li><strong>æ–¹æ¡ˆ A (TE Spec)</strong>ï¼šå¯Œäººç‰ˆã€‚åˆ©ç”¨ NVIDIA Transformer Engine ç¡¬ä»¶åŠ é€Ÿï¼Œæ”¯æŒ FP8ï¼Œç®—å­é«˜åº¦èåˆã€‚</li>
<li><strong>æ–¹æ¡ˆ B (Local Spec)</strong>ï¼šå¹³æ°‘ç‰ˆ/æ ‡å‡†ç‰ˆã€‚ä½¿ç”¨çº¯ Megatron-Core çš„å¹¶è¡Œå±‚ï¼Œä¾èµ– Apex æˆ– PyTorchï¼Œå…¼å®¹æ€§å¥½ã€‚</li>
</ul>
</li>
<li><strong>BERT ç‰¹æ€§</strong>ï¼šç‰¹åˆ«æŒ‡å®šäº† <code>AttnMaskType.padding</code>ï¼Œè¿™æ˜¯ä¸ºäº†ç¬¦åˆ BERT åŒå‘ç†è§£ä¸Šä¸‹æ–‡çš„ç‰¹æ€§ï¼ˆåŒºåˆ«äº GPT çš„ Causal Maskï¼‰ã€‚</li>
</ol>
<p><strong>ä¸€å¥è¯æ¦‚æ‹¬</strong>ï¼š
å½“ä½ æƒ³è¦è®­ç»ƒä¸€ä¸ª BERT æ¨¡å‹æ—¶ï¼ŒMegatron ä¼šé—®ä½ ï¼šâ€œä½ æƒ³ç”¨ä»€ä¹ˆé…ç½®ï¼Ÿâ€ ä½ å°±æŠŠè¿™ä¸ªæ–‡ä»¶é‡Œçš„ <code>bert_layer_local_spec</code> æˆ–è€… <code>get_bert_layer_with_transformer_engine_spec()</code> ä¸¢ç»™å®ƒï¼Œå®ƒå°±çŸ¥é“æ€ä¹ˆæ­å»ºç½‘ç»œäº†ã€‚</p>