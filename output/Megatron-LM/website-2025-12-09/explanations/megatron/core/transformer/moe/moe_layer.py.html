<h1>megatron/core/transformer/moe/moe_layer.py</h1>
<p>è¿™ä»½ä»£ç ç¡®å®æ¯”è¾ƒå¤æ‚ï¼Œå› ä¸ºå®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ™®é€šçš„ç¥ç»ç½‘ç»œå±‚ï¼Œè€Œæ˜¯ <strong>Megatron-Core</strong> ä¸­ç”¨äºå¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒçš„ <strong>MoEï¼ˆMixture of Expertsï¼Œæ··åˆä¸“å®¶æ¨¡å‹ï¼‰</strong> æ ¸å¿ƒå®ç°ã€‚å®ƒæ¶‰åŠåˆ°äº†å¤šå¡é€šä¿¡ã€å¹¶è¡Œè®¡ç®—å’Œå†…å­˜ä¼˜åŒ–ã€‚</p>
<p>ä¸ºäº†è®©ä½ çœ‹æ‡‚ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ª <code>MoELayer</code> æƒ³è±¡æˆä¸€ä¸ª<strong>å¤§å‹è·¨å›½å…¬å¸çš„ä»»åŠ¡åˆ†å‘ä¸­å¿ƒ</strong>ã€‚</p>
<p>æˆ‘ä¸ºä½ åˆ—äº†ä¸€ä¸ª <strong>â€œä»»åŠ¡æ¸…å• (Todo List)â€</strong>ï¼ŒæŒ‰ç…§æ•°æ®æµåŠ¨çš„é¡ºåºï¼Œä¸€æ­¥æ­¥æ‹†è§£è¿™æ®µä»£ç åœ¨å¹²ä»€ä¹ˆã€‚</p>
<hr />
<h3>ğŸ“ MoE Layer ä»»åŠ¡æ¸…å• (Todo List)</h3>
<p>æˆ‘ä»¬å°†æ•´ä¸ª <code>MoELayer</code> çš„å·¥ä½œæµç¨‹æ‹†è§£ä¸ºä»¥ä¸‹ 5 ä¸ªä¸»è¦ Taskã€‚ä»£ç ä¸­çš„ <code>forward</code> å‡½æ•°å°±æ˜¯æŒ‰ç…§è¿™ä¸ªé¡ºåºæ‰§è¡Œçš„ã€‚</p>
<ul>
<li><strong>[Task 0] å›¢é˜Ÿç»„å»º (åˆå§‹åŒ–é˜¶æ®µ)</strong><ul>
<li>ç¡®å®šè°æ˜¯ä¸“å®¶ï¼Ÿè°æ˜¯å…±äº«ä¸“å®¶ï¼Ÿ</li>
<li>ç¡®å®šæ¯å°æœºå™¨ï¼ˆGPUï¼‰è´Ÿè´£å“ªäº›ä¸“å®¶ï¼Ÿ</li>
</ul>
</li>
<li><strong>[Task 1] ä»»åŠ¡åˆ†è¯Š (Routing &amp; Preprocess)</strong><ul>
<li>çœ‹ä¸€çœ¼è¾“å…¥çš„æ•°æ®ï¼ˆTokenï¼‰ï¼Œå†³å®šæ¯ä¸ªæ•°æ®è¯¥äº¤ç»™å“ªä¸ªä¸“å®¶å¤„ç†ã€‚</li>
</ul>
</li>
<li><strong>[Task 2] å…¬å…±ä»»åŠ¡å¤„ç† (Shared Expert Compute)</strong><ul>
<li>(å¯é€‰) æœ‰äº›åŸºç¡€å·¥ä½œä¸éœ€è¦æ‰¾ä¸“å®¶ï¼Œç”±â€œå…±äº«ä¸“å®¶â€ç»Ÿä¸€å¤„ç†ã€‚</li>
</ul>
</li>
<li><strong>[Task 3] è·¨éƒ¨é—¨æ´¾å• (Dispatch)</strong><ul>
<li><strong>å…³é”®æ­¥éª¤</strong>ï¼šå¦‚æœæŒ‡å®šçš„ä¸“å®¶åœ¨åˆ«çš„æœºå™¨ä¸Šï¼Œéœ€è¦æŠŠæ•°æ®æ‰“åŒ…å‘é€è¿‡å»ï¼ˆé€šä¿¡ï¼‰ã€‚</li>
</ul>
</li>
<li><strong>[Task 4] ä¸“å®¶å¹²æ´» (Expert Compute)</strong><ul>
<li>ä¸“å®¶æ‹¿åˆ°åˆ†é…ç»™è‡ªå·±çš„æ•°æ®ï¼Œå¼€å§‹è®¡ç®—ã€‚</li>
</ul>
</li>
<li><strong>[Task 5] ç»“æœæ±‡æ€» (Combine)</strong><ul>
<li>æŠŠä¸“å®¶ç®—å¥½çš„ç»“æœæ‹¿å›æ¥ï¼Œå‘å›ç»™åŸæ¥çš„è¯·æ±‚è€…ï¼Œå¹¶æŠŠâ€œå…±äº«ä¸“å®¶â€çš„ç»“æœåŠ è¿›å»ã€‚</li>
</ul>
</li>
</ul>
<hr />
<h3>ğŸ” è¯¦ç»†æ­¥éª¤è§£è¯» (å…³è”ä»£ç )</h3>
<p>ä¸‹é¢æˆ‘ç»“åˆä»£ç ä¸­çš„å…·ä½“å‡½æ•°ï¼Œè¯¦ç»†è®²è®²ä¸Šé¢æ¯ä¸ª Task æ˜¯æ€ä¹ˆå®ç°çš„ã€‚</p>
<h4>[Task 0] å›¢é˜Ÿç»„å»º (åˆå§‹åŒ– <code>__init__</code>)</h4>
<p>åœ¨ä»£ç çš„ <code>__init__</code> æ–¹æ³•ä¸­ï¼ŒMoE å±‚åœ¨åšå‡†å¤‡å·¥ä½œï¼š
1.  <strong>åˆ’åˆ†åœ°ç›˜ (<code>self.ep_group</code>)</strong>ï¼š
    *   è¿™æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è®­ç»ƒï¼Œå‡è®¾æœ‰ 8 ä¸ªä¸“å®¶ï¼Œä½ æœ‰ 2 å¼ æ˜¾å¡ã€‚ä»£ç ä¼šè®¡ç®— <code>num_local_experts</code>ï¼Œå³æ¯å¼ å¡é¢†å…» 4 ä¸ªä¸“å®¶ã€‚
    *   <code>self.local_expert_indices</code> è®°å½•äº†è¿™å¼ å¡è´Ÿè´£å“ªå‡ ä¸ªä¸“å®¶ï¼ˆæ¯”å¦‚ Expert 0-3ï¼‰ã€‚
2.  <strong>æ‹›è˜è°ƒåº¦å‘˜ (<code>self.router</code>)</strong>ï¼š
    *   åˆå§‹åŒ– <code>TopKRouter</code>ã€‚å®ƒçš„ä½œç”¨æ˜¯ç»™æ¯ä¸ª Token æ‰“åˆ†ï¼Œé€‰å‡ºå¾—åˆ†æœ€é«˜çš„ K ä¸ªä¸“å®¶ã€‚
3.  <strong>å»ºç«‹ç‰©æµç³»ç»Ÿ (<code>self.token_dispatcher</code>)</strong>ï¼š
    *   è¿™æ˜¯æœ€å¤æ‚çš„ã€‚ä»£ç æ ¹æ®é…ç½®é€‰æ‹© <code>MoEAllGatherTokenDispatcher</code> æˆ– <code>MoEAlltoAllTokenDispatcher</code>ã€‚
    *   <strong>äººè¯</strong>ï¼šè¿™å°±æ˜¯è´Ÿè´£åœ¨ä¸åŒæ˜¾å¡ä¹‹é—´æ¬è¿æ•°æ®çš„â€œå¿«é€’å…¬å¸â€ã€‚</p>
<h4>[Task 1] ä»»åŠ¡åˆ†è¯Š (å‡½æ•° <code>router_and_preprocess</code>)</h4>
<p><strong>ä»£ç ä½ç½®ï¼š</strong> <code>def router_and_preprocess(...)</code></p>
<ol>
<li><strong>æ‰“åˆ†ä¸è·¯ç”±</strong>ï¼š<ul>
<li><code>probs, routing_map = self.router(hidden_states)</code></li>
<li>è·¯ç”±ï¼ˆRouterï¼‰çœ‹ä¸€çœ¼è¾“å…¥çš„æ•°æ®ï¼Œè¾“å‡º <code>routing_map</code>ï¼ˆå»å“ªï¼‰å’Œ <code>probs</code>ï¼ˆæƒé‡å¤šå°‘ï¼‰ã€‚</li>
</ul>
</li>
<li><strong>æ•°æ®å‹ç¼© (å¯é€‰)</strong>ï¼š<ul>
<li><code>if self.config.moe_latent_size:</code></li>
<li>å¦‚æœå¼€å¯äº† latent é€‰é¡¹ï¼Œä¼šå…ˆç”¨ <code>fc1_latent_proj</code> æŠŠæ•°æ®ç»´åº¦å‹ç¼©ä¸€ä¸‹ï¼Œä¸ºäº†åé¢ä¼ è¾“æ—¶çœå¸¦å®½ã€‚</li>
</ul>
</li>
<li><strong>æ‰“åŒ…é¢„å¤„ç†</strong>ï¼š<ul>
<li><code>self.token_dispatcher.dispatch_preprocess</code></li>
<li>æŠŠæ•°æ®æ•´ç†å¥½ï¼Œå‡†å¤‡äº¤ç»™ç‰©æµå‘è´§ã€‚</li>
</ul>
</li>
</ol>
<h4>[Task 2] å…¬å…±ä»»åŠ¡å¤„ç† (å‡½æ•° <code>shared_experts_compute</code>)</h4>
<p><strong>ä»£ç ä½ç½®ï¼š</strong> <code>def shared_experts_compute(...)</code></p>
<ul>
<li><strong>æ¦‚å¿µ</strong>ï¼šç°åœ¨çš„ MoE æ¨¡å‹ï¼ˆå¦‚ DeepSeek-MoE, Qwen-MoEï¼‰é€šå¸¸éƒ½æœ‰ä¸€ä¸ª <strong>Shared Expert</strong>ã€‚å®ƒæ˜¯ä¸å‚ä¸è·¯ç”±çš„ï¼Œæ‰€æœ‰ Token éƒ½è¦ç»è¿‡å®ƒã€‚å°±åƒåŒ»é™¢é‡Œçš„â€œå…¨ç§‘åŒ»ç”Ÿâ€ï¼Œä¸ç®¡ä½ çœ‹ä»€ä¹ˆç—…ï¼Œéƒ½è¦å…ˆé‡ä¸ªè¡€å‹ã€‚</li>
<li><strong>æ‰§è¡Œ</strong>ï¼š<ul>
<li>å¦‚æœé…ç½®äº† <code>use_shared_expert</code>ï¼Œè¿™é‡Œä¼šç›´æ¥è¿è¡Œ <code>self.shared_experts(hidden_states)</code>ã€‚</li>
<li>ä»£ç é‡Œè¿˜åŒ…å«äº†ä¸€å † <code>te_checkpoint</code> æˆ– <code>tensor_parallel.checkpoint</code>ï¼Œè¿™æ˜¯ä¸ºäº†<strong>çœæ˜¾å­˜</strong>ï¼ˆRecomputeæŠ€æœ¯ï¼‰ï¼Œä½ å¯ä»¥æš‚æ—¶å¿½ç•¥ï¼Œåªçœ‹å®ƒæ˜¯åœ¨åšè®¡ç®—ã€‚</li>
</ul>
</li>
</ul>
<h4>[Task 3] è·¨éƒ¨é—¨æ´¾å• (å‡½æ•° <code>dispatch</code>)</h4>
<p><strong>ä»£ç ä½ç½®ï¼š</strong> <code>def dispatch(...)</code></p>
<ul>
<li><strong>æ ¸å¿ƒåŠ¨ä½œ</strong>ï¼š<code>self.token_dispatcher.token_dispatch(...)</code></li>
<li><strong>å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ</strong><ul>
<li>è¿™æ˜¯ä¸€ä¸ª<strong>é€šä¿¡ï¼ˆCommunicationï¼‰</strong>è¿‡ç¨‹ã€‚</li>
<li>å‡è®¾ Token A åœ¨ GPU 1 ä¸Šï¼Œä½† Router æŠŠå®ƒåˆ†ç»™äº† GPU 2 ä¸Šçš„ä¸“å®¶ã€‚</li>
<li>è¿™ä¸ªå‡½æ•°ä¼šè°ƒç”¨åº•å±‚çš„ <code>All-to-All</code> æˆ– <code>All-Gather</code> é€šä¿¡ç®—å­ï¼ŒæŠŠ Token A çš„æ•°æ®ç‰©ç†ä¼ è¾“åˆ° GPU 2 ä¸Šã€‚</li>
<li>æ­¤åˆ»ï¼Œæ•°æ®å·²ç»ç¦»å¼€äº†åŸæœ¬çš„ GPUï¼Œåˆ°è¾¾äº†ä¸“å®¶æ‰€åœ¨çš„ GPUã€‚</li>
</ul>
</li>
</ul>
<h4>[Task 4] ä¸“å®¶å¹²æ´» (å‡½æ•° <code>routed_experts_compute</code>)</h4>
<p><strong>ä»£ç ä½ç½®ï¼š</strong> <code>def routed_experts_compute(...)</code></p>
<ol>
<li><strong>æ¥å•</strong>ï¼š<ul>
<li><code>self.token_dispatcher.dispatch_postprocess</code></li>
<li>ä¸“å®¶æ‰€åœ¨çš„ GPU æ¥æ”¶åˆ°æ¥è‡ªå››é¢å…«æ–¹çš„æ•°æ®ï¼Œæ•´ç†æˆä¸€ä¸ªåˆ—è¡¨ã€‚</li>
</ul>
</li>
<li><strong>å¹²æ´»</strong>ï¼š<ul>
<li><code>self.experts(dispatched_input, ...)</code></li>
<li>è¿™æ˜¯çœŸæ­£çš„ç¥ç»ç½‘ç»œè®¡ç®—ï¼ˆé€šå¸¸æ˜¯ MLP å±‚ï¼‰ã€‚æ­¤æ—¶è®¡ç®—çš„æ˜¯<strong>è¢«åˆ†é…åˆ°è¿™å¼ å¡ä¸Šçš„æ‰€æœ‰ Token</strong>ï¼Œè€Œä¸æ˜¯è¿™å¼ å¡åŸæœ¬ç”Ÿæˆçš„ Tokenã€‚</li>
</ul>
</li>
</ol>
<h4>[Task 5] ç»“æœæ±‡æ€» (å‡½æ•° <code>combine</code>)</h4>
<p><strong>ä»£ç ä½ç½®ï¼š</strong> <code>def combine(...)</code></p>
<ol>
<li><strong>ç»“æœå¯„å›</strong>ï¼š<ul>
<li><code>self.token_dispatcher.token_combine(output)</code></li>
<li>è¿™ä¹Ÿæ˜¯ä¸€æ¬¡<strong>é€šä¿¡</strong>ã€‚ä¸“å®¶ç®—å®Œäº†ï¼Œå¾—æŠŠç»“æœå‘å›ç»™ Token æœ€å¼€å§‹æ‰€åœ¨çš„ GPUã€‚</li>
<li>å¦‚æœä½ æ˜¯ GPU 1ï¼Œä½ å‘å‡ºçš„ Token A è¢« GPU 2 å¤„ç†äº†ï¼Œè¿™ä¸€æ­¥ GPU 2 ä¼šæŠŠç»“æœå‘å›ç»™ä½ ã€‚</li>
</ul>
</li>
<li><strong>åŠ æƒèåˆ</strong>ï¼š<ul>
<li>æ ¹æ® Router ç®—å‡ºæ¥çš„æƒé‡ï¼ˆprobsï¼‰ï¼ŒæŠŠä¸åŒä¸“å®¶çš„ç»“æœåŠ æƒæ±‚å’Œã€‚</li>
</ul>
</li>
<li><strong>åŠ ä¸Šå…±äº«ä¸“å®¶</strong>ï¼š<ul>
<li><code>output = output + shared_expert_output</code></li>
<li>æœ€åï¼ŒæŠŠ [Task 2] ç®—å‡ºæ¥çš„å…±äº«ä¸“å®¶ç»“æœåŠ åˆ°æ€»ç»“æœé‡Œã€‚</li>
</ul>
</li>
</ol>
<hr />
<h3>ğŸ’¡ æ€»ç»“ï¼šè¿™æ®µä»£ç çš„æ ¸å¿ƒé€»è¾‘ (<code>forward</code> å‡½æ•°)</h3>
<p>ä»£ç æœ€åçš„ <code>forward</code> å‡½æ•°æŠŠä¸Šé¢ä¸²èµ·æ¥äº†ï¼Œé€»è¾‘éå¸¸æ¸…æ™°ï¼š</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">custom_forward</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">):</span>
    <span class="c1"># 1. [Task 2] å…±äº«ä¸“å®¶å…ˆç®— (æˆ–è€…å¹¶è¡Œç®—)</span>
    <span class="n">shared_expert_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_experts_compute</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># 2. [Task 1] è·¯ç”±ï¼šå†³å®šå»å“ª</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">router_and_preprocess</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># 3. [Task 3] æ´¾å•ï¼šæŠŠæ•°æ®å‘ç»™åˆ«çš„æ˜¾å¡</span>
    <span class="n">dispatched_input</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dispatch</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>

    <span class="c1"># 4. [Task 4] ä¸“å®¶å¹²æ´»ï¼šå¤„ç†æ”¶åˆ°çš„æ•°æ®</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">mlp_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">routed_experts_compute</span><span class="p">(</span><span class="n">dispatched_input</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">residual</span><span class="p">)</span>

    <span class="c1"># 5. [Task 5] æ±‡æ€»ï¼šæŠŠç»“æœæ”¶å›æ¥ï¼Œå¹¶åŠ ä¸Šå…±äº«ä¸“å®¶çš„ç»“æœ</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">combine</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">shared_expert_output</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">mlp_bias</span>
</code></pre></div>

<p><strong>ä¸ºä»€ä¹ˆä½ è§‰å¾—éš¾æ‡‚ï¼Ÿ</strong>
å› ä¸ºå…¶ä¸­ç©¿æ’äº†å¤§é‡çš„ï¼š
1.  <strong>å¹¶è¡Œå¤„ç†é€»è¾‘</strong> (<code>pg_collection</code>, <code>ep_group</code>)ï¼šå¤„ç†å¤šå¡åä½œã€‚
2.  <strong>æ˜¾å­˜ä¼˜åŒ–</strong> (<code>te_checkpoint</code>, <code>recompute</code>)ï¼šä¸ºäº†èƒ½è·‘æ›´å¤§çš„æ¨¡å‹ï¼Œç”¨è®¡ç®—æ¢æ˜¾å­˜ã€‚
3.  <strong>Transformer Engine (TE)</strong>ï¼šNVIDIA çš„åŠ é€Ÿåº“ï¼Œè®©ä»£ç çœ‹èµ·æ¥ä¸åƒçº¯ PyTorchã€‚</p>
<p>ä½†åœ¨é€»è¾‘å±‚é¢ï¼Œå®ƒå°±æ˜¯ä¸€ä¸ª <strong>åˆ†å‘ -&gt; è®¡ç®— -&gt; æ”¶é›†</strong> çš„è¿‡ç¨‹ã€‚</p>