<h1>megatron/legacy/model/gpt_model.py</h1>
<p>è¿™æ®µä»£ç ç¡®å®æ¯”è¾ƒæ™¦æ¶©ï¼Œå› ä¸ºå®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ™®é€šçš„ PyTorch æ¨¡å‹ï¼Œå®ƒæ˜¯ <strong>NVIDIA Megatron-LM</strong> çš„ä¸€éƒ¨åˆ†ã€‚è¿™æ„å‘³ç€å®ƒé‡Œé¢åŒ…å«äº†å¤§é‡å…³äº<strong>åˆ†å¸ƒå¼è®­ç»ƒï¼ˆDistributed Trainingï¼‰</strong>ã€<strong>æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰</strong>å’Œ<strong>æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰</strong>çš„é€»è¾‘ã€‚</p>
<p>å¦‚æœåªæŠŠå®ƒå½“æˆå•å¡æ¨¡å‹çœ‹ï¼Œä¼šè§‰å¾—å¾ˆå¤šå‚æ•°è«åå…¶å¦™ã€‚</p>
<p>ä¸ºäº†è®©ä½ è¯»æ‡‚å®ƒï¼Œæˆ‘ä¸ºä½ åˆ¶å®šäº†ä¸€ä¸ª <strong>â€œå­¦ä¹ ä»»åŠ¡æ¸…å• (To-Do List)â€</strong>ã€‚æˆ‘ä»¬å°†æŠŠä»£ç æ‹†è§£ï¼Œä¸€æ­¥æ­¥ç‚¹äº®ä½ çš„ç†è§£æŠ€èƒ½æ ‘ã€‚</p>
<hr />
<h3>ğŸ“‹ ä»»åŠ¡æ¸…å•ï¼šæ”»å…‹ GPTModel</h3>
<ol>
<li><strong>Task 01: ç†è§£æ ¸å¿ƒæ¦‚å¿µ â€”â€” ä»€ä¹ˆæ˜¯ Pre/Post Processï¼Ÿï¼ˆæµæ°´çº¿å¹¶è¡Œï¼‰</strong></li>
<li><strong>Task 02: æ‹†è§£æ ¸å¿ƒç»„ä»¶ â€”â€” <code>language_model</code> æ˜¯ä»€ä¹ˆï¼Ÿ</strong></li>
<li><strong>Task 03: ç†è§£æƒé‡å…±äº« â€”â€” ä¸ºä»€ä¹ˆ Embedding æ—¢åœ¨å¤´ä¹Ÿåœ¨å°¾ï¼Ÿ</strong></li>
<li><strong>Task 04: æ”»å…‹éš¾ç‚¹å‡½æ•° â€”â€” <code>post_language_model_processing</code> (è®¡ç®— Loss)</strong></li>
<li><strong>Task 05: ä¸²è”å…¨æµç¨‹ â€”â€” <code>forward</code> å‡½æ•°æ˜¯æ€ä¹ˆèµ°çš„ï¼Ÿ</strong></li>
</ol>
<hr />
<h3>ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡</h3>
<h4>âœ… Task 01: ç†è§£æ ¸å¿ƒæ¦‚å¿µ â€”â€” Pre/Post Process</h4>
<p>åœ¨ Megatron ä¸­ï¼Œä¸€ä¸ªå·¨å¤§çš„ GPT æ¨¡å‹å¯èƒ½è¢«åˆ‡åˆ†æˆå¥½å‡ æ®µï¼Œæ”¾åœ¨ä¸åŒçš„ GPU ä¸Šï¼ˆè¿™å«æµæ°´çº¿å¹¶è¡Œï¼‰ã€‚</p>
<ul>
<li><strong><code>pre_process=True</code></strong>: ä»£è¡¨å½“å‰ GPU è´Ÿè´£æ¨¡å‹çš„<strong>æœ€åº•å±‚</strong>ï¼ˆåŒ…å« Word Embedding è¾“å…¥å±‚ï¼‰ã€‚</li>
<li><strong><code>post_process=True</code></strong>: ä»£è¡¨å½“å‰ GPU è´Ÿè´£æ¨¡å‹çš„<strong>æœ€é¡¶å±‚</strong>ï¼ˆåŒ…å«è¾“å‡ºå±‚å’Œ Loss è®¡ç®—ï¼‰ã€‚</li>
</ul>
<p><strong>ä»£ç å¯¹åº”ï¼š</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">GPTModel</span><span class="p">(</span><span class="n">MegatronModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">pre_process</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">post_process</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># ...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_process</span> <span class="o">=</span> <span class="n">pre_process</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_process</span> <span class="o">=</span> <span class="n">post_process</span>
</code></pre></div>

<ul>
<li><strong>è§‚ç‚¹</strong>: å¦‚æœä½ åœ¨ä¸€å¼ å¡ä¸Šè·‘ï¼Œè¿™ä¸¤ä¸ªéƒ½æ˜¯ Trueã€‚å¦‚æœä½ ç”¨äº† 4 å¼ å¡åšæµæ°´çº¿å¹¶è¡Œï¼š<ul>
<li>ç¬¬ 1 å¼ å¡ï¼š<code>pre=True</code>, <code>post=False</code></li>
<li>ç¬¬ 2ã€3 å¼ å¡ï¼š<code>pre=False</code>, <code>post=False</code> (ä¸­é—´å±‚)</li>
<li>ç¬¬ 4 å¼ å¡ï¼š<code>pre=False</code>, <code>post=True</code></li>
</ul>
</li>
</ul>
<h4>âœ… Task 02: æ‹†è§£æ ¸å¿ƒç»„ä»¶ â€”â€” <code>language_model</code></h4>
<p><code>GPTModel</code> å…¶å®æ˜¯ä¸€ä¸ªâ€œå¤–å£³â€ï¼ŒçœŸæ­£çš„ Transformer å †å å±‚ï¼ˆAttention, MLP ç­‰ï¼‰éƒ½è¢«å°è£…åœ¨ <code>language_model</code> é‡Œã€‚</p>
<p><strong>ä»£ç å¯¹åº”ï¼š</strong></p>
<div class="codehilite"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_language_model_key</span> <span class="o">=</span> <span class="n">get_language_model</span><span class="p">(</span>
    <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
    <span class="c1"># ...</span>
    <span class="n">pre_process</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pre_process</span><span class="p">,</span>
    <span class="n">post_process</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">post_process</span><span class="p">)</span>
</code></pre></div>

<ul>
<li><strong>è§‚ç‚¹</strong>: <code>GPTModel</code> ä¸ç›´æ¥å†™ Attention ä»£ç ï¼Œå®ƒè°ƒç”¨ <code>get_language_model</code> æ¥è·å–ä¸€ä¸ªâ€œèº¯å¹²â€ã€‚å®ƒè´Ÿè´£æŠŠè¾“å…¥æ‰”ç»™è¿™ä¸ªèº¯å¹²ï¼Œç„¶åå¤„ç†èº¯å¹²åå‡ºæ¥çš„ç»“æœã€‚</li>
</ul>
<h4>âœ… Task 03: ç†è§£æƒé‡å…±äº« â€”â€” Untie Embeddings</h4>
<p>GPT æ¨¡å‹æœ‰ä¸€ä¸ªç»å…¸ç‰¹æ€§ï¼š<strong>è¾“å…¥å±‚çš„ Embedding çŸ©é˜µå’Œè¾“å‡ºå±‚çš„åå‘æŠ•å½±çŸ©é˜µé€šå¸¸æ˜¯åŒä¸€ä¸ªï¼ˆå…±äº«æƒé‡ï¼‰</strong>ã€‚</p>
<p><strong>ä»£ç å¯¹åº”ï¼š</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># å¦‚æœ args.untie_embeddings_and_output_weights ä¸º False (é»˜è®¤æ˜¯ Falseï¼Œå³å…±äº«)</span>
<span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">share_embeddings_and_output_weights</span><span class="o">=</span><span class="ow">not</span> <span class="n">args</span><span class="o">.</span><span class="n">untie_embeddings_and_output_weights</span><span class="p">)</span>

<span class="c1"># åœ¨ forward åå¤„ç†æ—¶ï¼š</span>
<span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="o">.</span><span class="n">output_layer</span><span class="o">.</span><span class="n">weight</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">untie_embeddings_and_output_weights</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_embedding_or_output_weight</span><span class="p">()</span>
</code></pre></div>

<ul>
<li><strong>è§‚ç‚¹</strong>:<ul>
<li>è¾“å…¥æ—¶ï¼šToken ID -&gt; å‘é‡ (æŸ¥ Embedding è¡¨)ã€‚</li>
<li>è¾“å‡ºæ—¶ï¼šå‘é‡ -&gt; Token æ¦‚ç‡ (ç”¨åŒä¸€ä¸ª Embedding è¡¨åšçŸ©é˜µä¹˜æ³•)ã€‚</li>
<li>Megatron å¿…é¡»è¦å¤„ç†å¥½è¿™ç§è·¨ GPU çš„å…±äº«ï¼ˆå› ä¸ºè¾“å…¥å±‚åœ¨ç¬¬ 1 å¼ å¡ï¼Œè¾“å‡ºå±‚åœ¨æœ€åä¸€å¼ å¡ï¼Œå…±äº«æƒé‡éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰ã€‚</li>
</ul>
</li>
</ul>
<h4>âœ… Task 04: æ”»å…‹éš¾ç‚¹å‡½æ•° â€”â€” <code>post_language_model_processing</code></h4>
<p>è¿™æ˜¯æ–‡ä»¶å¼€å¤´é‚£ä¸ªç‹¬ç«‹çš„å‡½æ•°ã€‚è¿™æ˜¯æ¨¡å‹çš„<strong>æ”¶å°¾å·¥ä½œ</strong>ã€‚
å½“ Transformer ç®—å®Œåï¼Œä¼šè¾“å‡ºä¸€ä¸ª hidden stateï¼ˆéšè—å±‚çŠ¶æ€ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒå˜æˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡æˆ– Lossã€‚</p>
<p><strong>ä»£ç å¯¹åº”ï¼š</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">post_language_model_processing</span><span class="p">(</span><span class="n">lm_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">logit_weights</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
    <span class="c1"># 1. è®¡ç®— Logits (é¢„æµ‹åˆ†æ•°)</span>
    <span class="c1"># [s b h] -&gt; [s b vocab_size]</span>
    <span class="c1"># æ³¨æ„è¿™é‡Œç”¨äº† parallel_lm_logitsï¼Œæ„æ€æ˜¯è¯è¡¨å¤ªå¤§äº†ï¼Œåˆ‡åˆ†åˆ°ä¸åŒå¡ä¸Šè®¡ç®—</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">parallel_lm_logits</span><span class="p">(</span><span class="n">lm_output</span><span class="p">,</span> <span class="n">logit_weights</span><span class="p">,</span> <span class="n">parallel_output</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼ˆæ¨ç†æ¨¡å¼ï¼‰ï¼Œç›´æ¥è¿”å›é¢„æµ‹ç»“æœ</span>
        <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 2. è®¡ç®— Loss (è®­ç»ƒæ¨¡å¼)</span>
        <span class="k">if</span> <span class="n">fp16_lm_cross_entropy</span><span class="p">:</span>
            <span class="c1"># ä½¿ç”¨åŠç²¾åº¦è®¡ç®—äº¤å‰ç†µ Lossï¼Œä¸ºäº†çœæ˜¾å­˜å’ŒåŠ é€Ÿ</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tensor_parallel</span><span class="o">.</span><span class="n">vocab_parallel_cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tensor_parallel</span><span class="o">.</span><span class="n">vocab_parallel_cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">labels</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div>

<ul>
<li><strong>è§‚ç‚¹</strong>: è¿™ä¸ªå‡½æ•°åšäº†ä¸¤ä»¶äº‹ï¼š<ol>
<li><strong>éšå‘é‡è½¬è¯è¡¨åˆ†æ•°</strong>: $H \times W^T = Logits$ã€‚</li>
<li><strong>ç®— Loss</strong>: å¦‚æœç»™äº†æ­£ç¡®ç­”æ¡ˆ (<code>labels</code>)ï¼Œå°±ç®—äº¤å‰ç†µï¼›æ²¡ç»™å°±è¾“å‡ºé¢„æµ‹å€¼ã€‚ç‰¹åˆ«ä¹‹å¤„åœ¨äºå®ƒç”¨äº† <code>vocab_parallel</code>ï¼ˆè¯è¡¨å¹¶è¡Œï¼‰ï¼Œå› ä¸º GPT çš„è¯è¡¨ï¼ˆæ¯”å¦‚ 5ä¸‡ä¸ªè¯ï¼‰å¾ˆå¤§ï¼Œè®¡ç®—èµ·æ¥å¾ˆå æ˜¾å­˜ï¼ŒMegatron æŠŠå®ƒæ‹†å¼€äº†ã€‚</li>
</ol>
</li>
</ul>
<h4>âœ… Task 05: ä¸²è”å…¨æµç¨‹ â€”â€” <code>forward</code> å‡½æ•°</h4>
<p>ç°åœ¨æŠŠä¸Šé¢æ‰€æœ‰çš„ç‚¹è¿èµ·æ¥ï¼Œçœ‹æ•°æ®æ€ä¹ˆæµåŠ¨çš„ã€‚</p>
<p><strong>ä»£ç å¯¹åº”ï¼š</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="o">...</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>

    <span class="c1"># 1. æ ¸å¿ƒè®¡ç®—ï¼šæŠŠæ•°æ®å–‚ç»™ Transformer èº¯å¹²</span>
    <span class="n">lm_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>

    <span class="c1"># 2. åå¤„ç†åˆ¤æ–­</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_process</span><span class="p">:</span>
        <span class="c1"># å¦‚æœæˆ‘æ˜¯æœ€åä¸€å¼ å¡ï¼ˆæˆ–è€…å•å¡è®­ç»ƒï¼‰ï¼Œæˆ‘æœ‰è´£ä»»è®¡ç®— Loss æˆ– Logits</span>
        <span class="k">return</span> <span class="n">post_language_model_processing</span><span class="p">(</span>
            <span class="n">lm_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
            <span class="c1"># è·å–è¾“å‡ºå±‚çš„æƒé‡ï¼ˆå¯èƒ½æ˜¯å…±äº«çš„ Embeddingï¼‰</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shared_embedding_or_output_weight</span><span class="p">(),</span> 
            <span class="o">...</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># å¦‚æœæˆ‘åªæ˜¯ä¸­é—´çš„å¡ï¼ˆæµæ°´çº¿å¹¶è¡Œï¼‰ï¼Œæˆ‘åªéœ€è¦æŠŠç®—å‡ºæ¥çš„éšå‘é‡è¿”å›</span>
        <span class="c1"># ä¼ ç»™ä¸‹ä¸€å¼ å¡å»å¤„ç†</span>
        <span class="k">return</span> <span class="n">lm_output</span>
</code></pre></div>

<hr />
<h3>ğŸ’¡ æ€»ç»“ï¼šè¿™ç¯‡æ–‡ç« è®²äº†å•¥ï¼Ÿ</h3>
<p>è¿™ç¯‡ä»£ç å®šä¹‰äº† <strong>GPT æ¨¡å‹åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„â€œå¤–éª¨æ¶â€</strong>ã€‚</p>
<ol>
<li>å®ƒ<strong>ä¸è´Ÿè´£</strong>å…·ä½“çš„ Attention è®¡ç®—ï¼ˆé‚£æ˜¯ <code>language_model</code> çš„äº‹ï¼‰ã€‚</li>
<li>å®ƒ<strong>è´Ÿè´£</strong>ï¼š<ul>
<li><strong>ç»„è£…</strong>ï¼šæŠŠ Transformer èº¯å¹²æ‹¿æ¥ã€‚</li>
<li><strong>è°ƒåº¦</strong>ï¼šæ ¹æ®è‡ªå·±æ˜¯ä¸æ˜¯ç¬¬ä¸€å±‚/æœ€åä¸€å±‚ (<code>pre/post process</code>)ï¼Œå†³å®šè¦ä¸è¦åˆå§‹åŒ– Embedding æˆ–è®¡ç®— Lossã€‚</li>
<li><strong>æ¥å£</strong>ï¼šæä¾›ç»Ÿä¸€çš„ <code>forward</code> å‡½æ•°ï¼Œè¾“å…¥ IDï¼Œè¾“å‡º Lossã€‚</li>
<li><strong>å¹¶è¡Œä¼˜åŒ–</strong>ï¼šå¤„ç†åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„ Loss è®¡ç®— (<code>vocab_parallel_cross_entropy</code>) å’Œæƒé‡å…±äº«ã€‚</li>
</ul>
</li>
</ol>
<p><strong>ç®€å•ä¸€å¥è¯ï¼š</strong> è¿™æ˜¯ä¸€ä¸ª<strong>æ”¯æŒè¶…å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒçš„ GPT æ¨¡å‹åŒ…è£…å™¨</strong>ï¼Œå®ƒç®¡ç†ç€æ•°æ®çš„è¾“å…¥ã€è¾“å‡ºä»¥åŠå¦‚ä½•åœ¨å¤šå¼ æ˜¾å¡ä¹‹é—´è®¡ç®—æœ€ç»ˆçš„ Lossã€‚</p>