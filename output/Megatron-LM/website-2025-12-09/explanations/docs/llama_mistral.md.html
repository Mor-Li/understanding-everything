<h1>docs/llama_mistral.md</h1>
<p>这份文档主要是针对 <strong>Megatron-LM</strong>（NVIDIA 开发的一个大规模语言模型训练框架）的用户手册。</p>
<p>简单来说，它的核心目的是：<strong>教你怎么把市面上开源的 Llama-2、Llama-3 和 Mistral 模型，加载到 Megatron-LM 里去运行（进行推理或微调）。</strong></p>
<p>因为这些模型原本的格式（通常是 HuggingFace 格式或 Meta 原生格式）和 Megatron-LM 要求的分布式格式不一样，所以必须经过一个“格式转换”的过程。</p>
<p>为了让你看懂，我把它拆解成一个通用的 <strong>Task List（任务清单）</strong>，然后针对每个模型做详细解释。</p>
<hr />
<h3>📋 通用 Task List (任务清单)</h3>
<p>无论你用 Llama-2, Llama-3 还是 Mistral，流程都是这 4 步：</p>
<ol>
<li><strong>[下载]</strong>：去 HuggingFace 或 Meta 官网申请并下载模型的原始权重文件（Checkpoint）。</li>
<li><strong>[转换]</strong>：使用 Megatron 提供的 Python 脚本，把原始权重转换成 Megatron 能识别的格式（<code>.pt</code> 文件）。</li>
<li><strong>[配置]</strong>：准备启动命令，设置好关键参数（如模型并行度 TP、序列长度等）。</li>
<li><strong>[启动]</strong>：运行 Megatron 进行推理（生成文本）或微调（训练）。</li>
</ol>
<hr />
<h3>📖 逐步详细解读</h3>
<h4>第一部分：Llama-2 模型</h4>
<p>Llama-2 是较早的版本，文档花了很多篇幅讲它。</p>
<ul>
<li><strong>步骤 1：下载</strong><ul>
<li>你可以从 Meta 官网下载（得到 Meta 格式），也可以从 HuggingFace 下载（得到 HF 格式）。</li>
</ul>
</li>
<li><strong>步骤 2：转换 (关键点)</strong><ul>
<li>Megatron 推荐在训练/微调时使用 <code>bf16</code> (bfloat16) 数据类型。</li>
<li><strong>如果你下载的是 Meta 格式</strong>：比较麻烦。你需要先把它转成 HuggingFace 格式，然后再转成 Megatron 格式。</li>
<li><strong>如果你下载的是 HuggingFace 格式</strong>：直接用 <code>tools/checkpoint/loader_llama_mistral.py</code> 脚本转换。</li>
<li><strong>注意参数 <code>TP</code> (Tensor Parallel)</strong>：转换时要指定把模型切成几份。<ul>
<li>7B 模型 -&gt; TP=1</li>
<li>13B 模型 -&gt; TP=2</li>
<li>70B 模型 -&gt; TP=8</li>
</ul>
</li>
</ul>
</li>
<li><strong>步骤 3：启动</strong><ul>
<li>文档给了一长串参数（例如 <code>--seq-length 4096</code>），直接复制使用即可。</li>
</ul>
</li>
<li><strong>关于跑分 (Benchmark)</strong><ul>
<li>文档列出了一堆表格，对比了“原版 Llama-2”和“转换后的 Megatron 版 Llama-2”的得分。</li>
<li><strong>结论</strong>：两者的误差极小（平均误差 0.15%），说明转换是成功的，精度没有丢失。</li>
</ul>
</li>
</ul>
<h4>第二部分：Llama-3.x 模型 (Llama-3 &amp; 3.1)</h4>
<p>这是目前最新的重点。</p>
<ul>
<li><strong>注意</strong>：为了简化代码，Megatron 现在<strong>只支持</strong>转换从 HuggingFace 下载的 Llama-3 权重。</li>
<li><strong>步骤 1：下载</strong><ul>
<li>去 HuggingFace 下载。</li>
</ul>
</li>
<li><strong>步骤 2：转换</strong><ul>
<li>同样使用转换脚本。</li>
<li><strong>TP 设置</strong>：<ul>
<li>1B/3B/8B 模型 -&gt; TP=1</li>
<li>70B 模型 -&gt; TP=8</li>
</ul>
</li>
</ul>
</li>
<li><strong>步骤 3：(可选) 验证</strong><ul>
<li>文档提供了一个脚本 <code>run_text_generation_llama3.sh</code>，让你启动一个服务器，发请求看看生成的文本对不对，以确保转换没问题。</li>
</ul>
</li>
<li><strong>步骤 4：启动</strong><ul>
<li><strong>Llama 3.0 和 3.1 的参数不同</strong>：<ul>
<li>3.0 的序列长度 (<code>seq-length</code>) 是 8192。</li>
<li>3.1 的最大位置嵌入 (<code>max-position-embeddings</code>) 增加到了 131072 (128k)，并且开启了 <code>--use-rope-scaling</code>。</li>
</ul>
</li>
<li><strong>架构细节</strong>：Llama-3 使用了 <code>Group Query Attention (GQA)</code> 和 <code>SwiGLU</code>，这些在启动参数里都要带上。</li>
</ul>
</li>
</ul>
<h4>第三部分：Mistral-7b 模型</h4>
<p>Mistral 和 Llama 架构很像，所以 Megatron 顺便也支持了。</p>
<ul>
<li><strong>版本</strong>：目前支持的是 Mistral-7b v0.3 版本。</li>
<li><strong>流程</strong>：下载 (HuggingFace) -&gt; 转换 -&gt; (可选) 验证 -&gt; 启动。</li>
<li><strong>启动参数</strong>：<ul>
<li>Mistral 也有特定的参数，比如 <code>sliding window attention</code> (滑动窗口注意力)，但在 v0.3 中似乎并未强制使用滑动窗口，词表大小也更大了 (32768)。</li>
</ul>
</li>
</ul>
<h4>第四部分：其他“类 Llama”模型</h4>
<ul>
<li>文档提到像 <strong>Yi-34B</strong> 和 <strong>Qwen2.x (通义千问)</strong> 这种模型，架构其实也是抄 Llama 的。</li>
<li>所以，你可以尝试用 Llama-3 的转换流程来处理这些模型（虽然标明是实验性的）。</li>
</ul>
<hr />
<h3>⚠️ 两个重要的技术备注 (文末提到的)</h3>
<ol>
<li>
<p><strong>数值差异 (Known numerical differences)</strong></p>
<ul>
<li><strong>问题</strong>：你可能会发现，同一个模型在 HuggingFace 里跑出来的数字，和在 Megatron 里跑出来的数字，小数点后几位不一样。</li>
<li><strong>解释</strong>：这是正常的。原因包括 Megatron 为了快把几个矩阵乘法合并了（GEMM），或者归一化（RMSNorm）的处理精度不一样。这不影响模型智能程度。</li>
</ul>
</li>
<li>
<p><strong>Legacy vs Core (旧格式 vs 新格式)</strong></p>
<ul>
<li><strong>观点</strong>：Megatron 正在进行代码重构。</li>
<li><strong>新格式 (<code>--saver core</code>)</strong>：推荐使用。代码路径在 <code>megatron.core...</code>。本文档所有的例子都是基于这个新格式的。</li>
<li><strong>旧格式 (<code>--saver legacy</code>)</strong>：如果你有很多老代码依赖旧的 Megatron 结构，你需要在转换和启动时加上 <code>--use-legacy-models</code> 参数。</li>
</ul>
</li>
</ol>
<h3>总结</h3>
<p>这篇文档就是一个<strong>操作手册</strong>。如果你接到的任务是“在我们的 Megatron 集群上跑 Llama 3”，你就照着文档中 <strong>Llama-3.x</strong> 那一节的命令复制粘贴，修改一下文件路径 (<code>CHECKPOINT_DIR</code>) 即可。</p>