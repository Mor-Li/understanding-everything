<h1>examples/multimodal/nvlm/pretrain_yi_34b_internvit_6b.sh</h1>
<p>这份脚本确实看起来很复杂，因为它包含了很多底层的参数配置。你可以把它想象成<strong>“发射火箭前的检查清单”</strong>。</p>
<p>这份脚本的目标是：<strong>训练一个多模态大模型（NVLM）</strong>。具体来说，是把一个看图的模型（InternViT-6B）和一个说话的模型（Yi-34B）拼接在一起训练。</p>
<p>为了让你看懂，我把这个脚本拆解成一个 <strong>Task To-Do List（任务清单）</strong>，我们一步步来勾选完成：</p>
<hr />
<h3>Task 1: 准备“厨房”和“食材” (环境与路径配置)</h3>
<p>在开始做饭（训练）之前，脚本的第一部分是在告诉电脑东西都在哪。</p>
<ul>
<li><strong>TODO 1.1: 设定工作目录</strong><ul>
<li><code>WORKSPACE</code>, <code>SOURCE</code>, <code>OUTPUT</code>: 告诉程序代码在哪里，训练好的模型存到哪里，日志写在哪里。</li>
</ul>
</li>
<li><strong>TODO 1.2: 确定模型名字</strong><ul>
<li><code>MODEL_NAME</code>: 给这次训练起个名字，比如 <code>mcore-nous-yi34b-internvit-mlp</code>。</li>
</ul>
</li>
<li><strong>TODO 1.3: 找回之前的进度 (Checkpoint)</strong><ul>
<li><code>LOAD_NAME</code>, <code>CHECKPOINT_DIR</code>: 这里指定了一个预训练好的基础模型路径。</li>
<li><strong>关键点</strong>：这说明我们不是从零开始（随机初始化），而是基于 <code>Yi-34B</code> 和 <code>InternViT</code> 已经训练好的权重开始微调。</li>
</ul>
</li>
<li><strong>TODO 1.4: 准备数据</strong><ul>
<li><code>DATA_TRAIN</code>: 指定了训练数据的配置文件（<code>pretrain_blend.yaml</code>）。</li>
</ul>
</li>
</ul>
<hr />
<h3>Task 2: 决定“烹饪模式” (调试 vs 正式训练)</h3>
<p>脚本里有一个 <code>if [[ $DEBUG -eq 1 ]]; then ... else ...</code> 的逻辑。</p>
<ul>
<li><strong>TODO 2.1: 选择模式</strong><ul>
<li>如果开启 <strong>DEBUG（调试模式）</strong>：只用极小的数据（Batch Size = 1），跑得很少，为了快速测试代码有没有报错。</li>
<li>如果开启 <strong>正式模式 (else)</strong>：<ul>
<li><code>BZ=2048</code>: 全局批次大小（一次喂给模型2048条数据）。</li>
<li><code>NW=8</code>: 用8个GPU（或者节点）。</li>
<li><code>SEQ_LEN</code>: 图像序列长度。</li>
<li><code>DECODER_SEQ_LEN</code>: 文本序列长度。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<h3>Task 3: 组装“大脑” (定义模型架构)</h3>
<p>这是最长的一段 <code>OPTIONS="..."</code>，它定义了这个怪兽模型的具体长相。</p>
<ul>
<li><strong>TODO 3.1: 选定语言模型 (大脑)</strong><ul>
<li><code>--language-model-type yi-34b</code>: 使用 Yi-34B 模型作为语言底座。</li>
<li><code>--hidden-size 7168</code>, <code>--num-layers 60</code>: 这是 Yi-34B 的具体参数（很巨大）。</li>
</ul>
</li>
<li><strong>TODO 3.2: 选定视觉模型 (眼睛)</strong><ul>
<li><code>--vision-model-type internvit</code>: 使用 InternViT-6B 作为视觉编码器。</li>
<li><code>--img-h 448</code>: 图片会被缩放到 448x448 像素。</li>
</ul>
</li>
<li><strong>TODO 3.3: 设定并行策略 (怎么切分模型)</strong><ul>
<li><code>--tensor-model-parallel-size 8</code>: <strong>这很重要</strong>。因为 Yi-34B 太大了，一张显卡装不下。这个参数的意思是：把模型切成8份，<strong>8张显卡合起来才能装下这一个模型</strong>（TP=8）。</li>
</ul>
</li>
</ul>
<hr />
<h3>Task 4: 制定“学习计划” (核心训练策略)</h3>
<p>这里藏着这次训练最核心的逻辑，决定了模型怎么学。</p>
<ul>
<li><strong>TODO 4.1: 决定谁该学习，谁该休息</strong><ul>
<li><code>--freeze-LM</code>: <strong>冻结语言模型</strong>。意思是 Yi-34B 的参数不许变。</li>
<li><code>--freeze-ViT</code>: <strong>冻结视觉模型</strong>。意思是 InternViT 的参数也不许变。</li>
<li><strong>核心观点</strong>：既然“大脑”和“眼睛”都冻结了，那我们在训练啥？<ul>
<li>我们在训练 <strong>“连接器” (MLP)</strong>。这通常是多模态训练的第一阶段。目的是让“眼睛”看到的图片信号，经过转化，能被“大脑”理解。</li>
</ul>
</li>
</ul>
</li>
<li><strong>TODO 4.2: 设定学习率</strong><ul>
<li><code>--lr 1e-4</code>: 学习率。</li>
<li><code>--train-samples 122880000</code>: 总共要看大约 1.2 亿个样本。</li>
</ul>
</li>
<li><strong>TODO 4.3: 混合精度训练</strong><ul>
<li><code>--bf16</code>: 使用 <code>bfloat16</code> 格式。这是一种为了省显存并加速训练，同时保持精度的数字格式。</li>
</ul>
</li>
</ul>
<hr />
<h3>Task 5: 启动引擎 (运行命令)</h3>
<p>脚本的最后部分是实际按按钮的动作。</p>
<ul>
<li><strong>TODO 5.1: 本地运行还是集群运行？</strong><ul>
<li>如果不是集群 (<code>BATCH=0</code>)：直接用 <code>torchrun</code> 在这台机器的8张卡上跑。</li>
<li>如果是集群 (<code>else</code>)：用 <code>srun</code> 提交任务到调度系统（Slurm），并加载 Docker 容器。</li>
</ul>
</li>
</ul>
<hr />
<h3>总结：这篇文到底在干啥？</h3>
<p>简单来说，这个脚本在做这件事：</p>
<p><strong>“嘿，电脑！请帮我用 8 张显卡（TP=8）加载 Yi-34B 和 InternViT 模型。把它们俩都锁住（Freeze），只训练它们中间的那个连接层。我要用 1.2 亿的数据量，让 Yi-34B 学会看懂 InternViT 传过来的图片信息。”</strong></p>