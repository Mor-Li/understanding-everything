"""
s2_analyze_stats.py
åˆ†ææŒ‡å®šå­ç›®å½•ä¸‹æ–‡ä»¶çš„ git ä¿®æ”¹ç»Ÿè®¡ä¿¡æ¯
"""

import argparse
from collections import defaultdict
from pathlib import Path

import git
import numpy as np
import tiktoken


# åˆå§‹åŒ– tokenizerï¼ˆä½¿ç”¨ o200k_baseï¼Œå¯¹åº” GPT-5/Gemini ç­‰æ–°æ¨¡å‹ï¼‰
tokenizer = tiktoken.get_encoding("o200k_base")


def count_tokens(file_path: Path) -> int:
    """
    ä½¿ç”¨ tiktoken è®¡ç®—æ–‡ä»¶çš„å®é™… token æ•°é‡

    Args:
        file_path: æ–‡ä»¶è·¯å¾„

    Returns:
        token æ•°é‡
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            tokens = tokenizer.encode(content)
            return len(tokens)
    except:
        return 0


def analyze_repo_stats(repo_path: str, subdir: str):
    """
    åˆ†æä»“åº“æŒ‡å®šå­ç›®å½•çš„ç»Ÿè®¡ä¿¡æ¯

    Args:
        repo_path: ä»“åº“è·¯å¾„
        subdir: å­ç›®å½•ï¼ˆç›¸å¯¹äºä»“åº“æ ¹ç›®å½•ï¼‰
    """
    repo = git.Repo(repo_path)
    repo_root = Path(repo_path)
    file_change_count = defaultdict(int)

    print(f"ğŸ“Š æ­£åœ¨åˆ†æ {subdir}/ çš„ git å†å²...")
    print()

    # ç»Ÿè®¡æ¯ä¸ªæ–‡ä»¶çš„ä¿®æ”¹æ¬¡æ•°
    for commit in repo.iter_commits():
        try:
            if commit.parents:
                diffs = commit.parents[0].diff(commit)
                for diff in diffs:
                    file_path = diff.a_path or diff.b_path
                    if file_path and file_path.startswith(subdir + "/"):
                        file_change_count[file_path] += 1
        except Exception:
            continue

    # åªä¿ç•™å½“å‰å­˜åœ¨çš„æ–‡ä»¶
    existing_files = []
    total_tokens = 0

    print("ğŸ”¢ æ­£åœ¨è®¡ç®— token æ•°é‡ï¼ˆä½¿ç”¨ tiktoken o200k_baseï¼‰...")
    for file_path, count in file_change_count.items():
        full_path = repo_root / file_path
        if full_path.is_file():
            tokens = count_tokens(full_path)
            existing_files.append((file_path, count, tokens))
            total_tokens += tokens

    if not existing_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
        return

    # æŒ‰ä¿®æ”¹æ¬¡æ•°æ’åº
    existing_files.sort(key=lambda x: x[1], reverse=True)

    # æå–ä¿®æ”¹æ¬¡æ•°åˆ—è¡¨
    change_counts = [count for _, count, _ in existing_files]

    # è®¡ç®—åˆ†ä½æ•°
    percentiles = [50, 75, 90, 95, 99]
    percentile_values = np.percentile(change_counts, percentiles)

    # ========== æ‰“å°ç»Ÿè®¡ä¿¡æ¯ ==========

    print("=" * 70)
    print(f"ğŸ“ å­ç›®å½•: {subdir}/")
    print("=" * 70)
    print()

    print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"   - æ€»æ–‡ä»¶æ•°: {len(existing_files)}")
    print(f"   - æ€» Token æ•°: {total_tokens:,} (~{total_tokens/1000:.1f}K tokens)")
    print(f"   - å¹³å‡æ¯æ–‡ä»¶: {total_tokens/len(existing_files):.0f} tokens")
    print()

    print(f"ğŸ”¢ ä¿®æ”¹æ¬¡æ•°åˆ†å¸ƒ:")
    print(f"   - æœ€å°å€¼: {min(change_counts)}")
    print(f"   - æœ€å¤§å€¼: {max(change_counts)}")
    print(f"   - å¹³å‡å€¼: {np.mean(change_counts):.1f}")
    print(f"   - ä¸­ä½æ•°: {np.median(change_counts):.1f}")
    print()

    print(f"ğŸ“Š ä¿®æ”¹æ¬¡æ•°åˆ†ä½æ•°:")
    for p, v in zip(percentiles, percentile_values):
        print(f"   - P{p:2d}: {v:.0f} æ¬¡")
    print()

    # æŒ‰ç™¾åˆ†ä½å±•ç¤ºæ–‡ä»¶æ•°é‡å’Œ token ç»Ÿè®¡
    print("=" * 70)
    print("ğŸ“¦ æŒ‰ä¿®æ”¹é¢‘ç‡åˆ†å±‚ç»Ÿè®¡ (ä»é«˜åˆ°ä½)")
    print("=" * 70)
    print()

    percentages = [1, 5, 10, 20, 30, 50, 80, 90, 100]

    for pct in percentages:
        n_files = int(len(existing_files) * pct / 100)
        if n_files == 0:
            n_files = 1

        top_files = existing_files[:n_files]
        top_tokens = sum(tokens for _, _, tokens in top_files)
        min_changes = min(count for _, count, _ in top_files) if top_files else 0
        max_changes = max(count for _, count, _ in top_files) if top_files else 0

        print(f"å‰ {pct:3d}% æ–‡ä»¶ (Top {n_files:4d}):")
        print(f"   - ä¿®æ”¹æ¬¡æ•°èŒƒå›´: {min_changes:3d} ~ {max_changes:3d}")
        print(f"   - Token æ€»é‡: {top_tokens:,} (~{top_tokens/1000:.1f}K tokens)")
        print(f"   - å¹³å‡æ¯æ–‡ä»¶: {top_tokens/n_files:.0f} tokens")
        print()

    # å±•ç¤º Top 10 æ–‡ä»¶
    print("=" * 70)
    print("ğŸ† Top 10 ä¿®æ”¹æœ€é¢‘ç¹çš„æ–‡ä»¶")
    print("=" * 70)
    print()

    for i, (file_path, count, tokens) in enumerate(existing_files[:10], 1):
        # ç®€åŒ–è·¯å¾„æ˜¾ç¤º
        display_path = file_path
        if len(display_path) > 55:
            display_path = "..." + display_path[-52:]

        print(f"{i:2d}. {display_path}")
        print(f"    ä¿®æ”¹æ¬¡æ•°: {count:3d}  |  Token æ•°: {tokens:,} (~{tokens/1000:.1f}K)")
        print()


def main():
    parser = argparse.ArgumentParser(description="åˆ†æä»“åº“å­ç›®å½•çš„ç»Ÿè®¡ä¿¡æ¯")
    parser.add_argument("repo_path", help="Git ä»“åº“è·¯å¾„")
    parser.add_argument("--subdir", default="mshrl", help="è¦åˆ†æçš„å­ç›®å½•")

    args = parser.parse_args()

    analyze_repo_stats(args.repo_path, args.subdir)


if __name__ == "__main__":
    main()
