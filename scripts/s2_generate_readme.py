"""
s4_generate_readme.py
é€’å½’ç”Ÿæˆå„å±‚çº§ç›®å½•çš„ README.mdï¼ˆè‡ªåº•å‘ä¸Šï¼Œå¼‚æ­¥ç‰ˆæœ¬ï¼‰
"""

import argparse
import asyncio
import logging
import os
from pathlib import Path

import tiktoken
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm as async_tqdm

from utils import get_output_path

# åˆå§‹åŒ– tokenizer
tokenizer = tiktoken.get_encoding("o200k_base")

# é…ç½® logging
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s"
)
logger = logging.getLogger(__name__)

# å¸¸é‡
MAX_TOKENS = 200_000  # 200K token é™åˆ¶

# Prompt æ¨¡æ¿
README_PROMPT = """ä»¥ä¸‹æ˜¯ {folder_path} ç›®å½•ä¸‹çš„å†…å®¹ï¼š

{content}

è¯·ä½ ç”¨æœ€é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ã€ç”¨æ¯”å–»çš„æ–¹å¼æè¿°ä¸€ä¸‹ï¼š
1. å½“å‰è¿™ä¸ªæ–‡ä»¶å¤¹ä¸»è¦è´Ÿè´£ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ
2. è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹çš„å„ä¸ªæ–‡ä»¶/å­æ–‡ä»¶å¤¹åˆ†åˆ«æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ
3. ç»™æˆ‘ä¸€ä¸ªé«˜å±‚çš„è®¤çŸ¥ï¼Œè®©æˆ‘èƒ½å¿«é€Ÿç†è§£è¿™éƒ¨åˆ†ä»£ç çš„ä½œç”¨ã€‚

è¯·ç”¨ç®€æ´ã€é€šä¿—ã€æ˜“æ‡‚çš„è¯­æ°”å›ç­”ï¼Œè¯´ä¸­æ–‡ã€‚"""


def count_tokens(text: str) -> int:
    """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
    return len(tokenizer.encode(text, disallowed_special=()))


def truncate_content(contents: list[tuple[str, str, int]], target_tokens: int) -> list[tuple[str, str]]:
    """
    ç­‰æ¯”ä¾‹æˆªæ–­å†…å®¹ä»¥æ»¡è¶³ token é™åˆ¶

    Args:
        contents: [(name, content, token_count), ...]
        target_tokens: ç›®æ ‡ token æ•°é‡

    Returns:
        [(name, truncated_content), ...]
    """
    total_tokens = sum(tc for _, _, tc in contents)
    ratio = target_tokens / total_tokens

    logger.warning(f"âš ï¸  å†…å®¹è¶…å‡º {MAX_TOKENS:,} tokens é™åˆ¶")
    logger.warning(f"   æ€»é‡: {total_tokens:,} tokens")
    logger.warning(f"   å‹ç¼©æ¯”ä¾‹: {ratio:.2%}")

    truncated = []
    for name, content, token_count in contents:
        # è®¡ç®—è¯¥æ–‡ä»¶åº”ä¿ç•™çš„ token æ•°é‡
        keep_tokens = int(token_count * ratio)

        # ç¼–ç åæˆªæ–­ï¼Œå†è§£ç 
        tokens = tokenizer.encode(content, disallowed_special=())
        truncated_tokens = tokens[:keep_tokens]
        truncated_content = tokenizer.decode(truncated_tokens)

        truncated.append((name, truncated_content))
        logger.warning(f"   - {name}: {token_count:,} â†’ {keep_tokens:,} tokens ({ratio:.2%})")

    return truncated


def collect_folder_content(folder_path: Path, explain_base: Path) -> str:
    """
    æ”¶é›†æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰å†…å®¹ï¼ˆæ–‡ä»¶çš„ .md + å­æ–‡ä»¶å¤¹çš„ README.mdï¼‰

    Args:
        folder_path: å½“å‰æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç›¸å¯¹äº repo æ ¹ç›®å½•ï¼‰
        explain_base: explain è¾“å‡ºçš„åŸºç¡€è·¯å¾„

    Returns:
        åˆå¹¶åçš„å†…å®¹å­—ç¬¦ä¸²
    """
    explain_folder = explain_base / folder_path

    if not explain_folder.exists():
        return ""

    contents = []  # [(name, content, token_count), ...]

    # æ”¶é›†ç›´æ¥å­æ–‡ä»¶çš„ .mdï¼ˆä¸åŒ…æ‹¬ README.mdï¼‰
    for md_file in sorted(explain_folder.glob("*.md")):
        if md_file.name == "README.md":
            continue

        with open(md_file, "r", encoding="utf-8") as f:
            content = f.read()
            token_count = count_tokens(content)
            # å»æ‰ .md åç¼€ä½œä¸ºæ˜¾ç¤ºåç§°
            name = md_file.name[:-3] if md_file.name.endswith(".md") else md_file.name
            contents.append((f"ğŸ“„ {name}", content, token_count))

    # æ”¶é›†å­æ–‡ä»¶å¤¹çš„ README.md
    for subfolder in sorted(explain_folder.iterdir()):
        if subfolder.is_dir():
            readme = subfolder / "README.md"
            if readme.exists():
                with open(readme, "r", encoding="utf-8") as f:
                    content = f.read()
                    token_count = count_tokens(content)
                    contents.append((f"ğŸ“ {subfolder.name}/", content, token_count))

    if not contents:
        return ""

    # è®¡ç®—æ€» token æ•°
    total_tokens = sum(tc for _, _, tc in contents)

    # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œæˆªæ–­
    if total_tokens > MAX_TOKENS:
        contents_text = truncate_content(contents, MAX_TOKENS)
    else:
        contents_text = [(name, content) for name, content, _ in contents]

    # æ‹¼æ¥å†…å®¹
    result = []
    for name, content in contents_text:
        result.append(f"## {name}\n\n{content}\n\n")

    return "".join(result)


async def ask_gemini_async(
    folder_path: str,
    content: str,
    client: AsyncOpenAI,
    model: str = "gemini-3-pro-preview"
) -> str:
    """
    å¼‚æ­¥è°ƒç”¨ Gemini API ç”Ÿæˆ README

    Args:
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
        content: æ–‡ä»¶å¤¹å†…å®¹
        client: AsyncOpenAI å®¢æˆ·ç«¯
        model: ä½¿ç”¨çš„æ¨¡å‹

    Returns:
        README å†…å®¹ï¼ˆMarkdown æ ¼å¼ï¼‰
    """
    prompt = README_PROMPT.format(folder_path=folder_path, content=content)

    try:
        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=32000,
            temperature=0.7,
        )

        finish_reason = response.choices[0].finish_reason
        content = response.choices[0].message.content or ""

        if finish_reason == "length":
            content += "\n\n_ï¼ˆæ³¨ï¼šå“åº”å› é•¿åº¦é™åˆ¶è¢«æˆªæ–­ï¼‰_"

        return content.strip()
    except Exception as e:
        return f"# README ç”Ÿæˆå¤±è´¥\n\né”™è¯¯ä¿¡æ¯: {str(e)}"


async def generate_readme_async(
    folder_path: Path,
    explain_base: Path,
    client: AsyncOpenAI,
    force: bool = False,
    model: str = "gemini-3-pro-preview",
    semaphore: asyncio.Semaphore | None = None,
) -> tuple[Path, bool]:
    """
    å¼‚æ­¥ç”Ÿæˆå•ä¸ªæ–‡ä»¶å¤¹çš„ README.md

    Args:
        folder_path: å½“å‰æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç›¸å¯¹äº repo æ ¹ç›®å½•ï¼‰
        explain_base: explain è¾“å‡ºçš„åŸºç¡€è·¯å¾„
        client: AsyncOpenAI å®¢æˆ·ç«¯
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆ
        model: ä½¿ç”¨çš„æ¨¡å‹
        semaphore: ä¿¡å·é‡ï¼Œç”¨äºæ§åˆ¶å¹¶å‘æ•°

    Returns:
        (æ–‡ä»¶å¤¹è·¯å¾„, æ˜¯å¦æˆåŠŸ)
    """
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
    if semaphore:
        async with semaphore:
            return await _generate_readme_impl(
                folder_path, explain_base, client, force, model
            )
    else:
        return await _generate_readme_impl(
            folder_path, explain_base, client, force, model
        )


async def _generate_readme_impl(
    folder_path: Path,
    explain_base: Path,
    client: AsyncOpenAI,
    force: bool,
    model: str,
) -> tuple[Path, bool]:
    """
    å®é™…çš„ README ç”Ÿæˆå®ç°
    """
    explain_folder = explain_base / folder_path

    if not explain_folder.exists():
        return (folder_path, False)

    # æ£€æŸ¥å½“å‰æ–‡ä»¶å¤¹æ˜¯å¦å·²æœ‰ README.md
    readme_path = explain_folder / "README.md"
    if readme_path.exists() and not force:
        return (folder_path, True)  # è·³è¿‡ï¼Œè§†ä¸ºæˆåŠŸ

    # æ”¶é›†å½“å‰æ–‡ä»¶å¤¹çš„å†…å®¹
    content = collect_folder_content(folder_path, explain_base)

    if not content:
        return (folder_path, False)

    # è°ƒç”¨ Geminiï¼ˆå¼‚æ­¥ï¼‰
    try:
        readme_content = await ask_gemini_async(str(folder_path), content, client, model)
    except Exception as e:
        logger.error(f"âŒ API è°ƒç”¨å¤±è´¥ {folder_path}: {e}")
        return (folder_path, False)

    # ä¿å­˜ç»“æœ
    try:
        readme_path.parent.mkdir(parents=True, exist_ok=True)
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(f"# {folder_path}\n\n")
            f.write(readme_content)
        return (folder_path, True)
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å¤±è´¥ {folder_path}: {e}")
        return (folder_path, False)


def find_all_folders(explain_base: Path, root_folder: Path) -> list[Path]:
    """
    æ‰¾åˆ°æ‰€æœ‰éœ€è¦ç”Ÿæˆ README çš„æ–‡ä»¶å¤¹ï¼ˆè‡ªåº•å‘ä¸Šæ’åºï¼‰

    Args:
        explain_base: explain è¾“å‡ºçš„åŸºç¡€è·¯å¾„
        root_folder: æ ¹æ–‡ä»¶å¤¹ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰

    Returns:
        æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ï¼ˆç›¸å¯¹è·¯å¾„ï¼ŒæŒ‰æ·±åº¦ä»æ·±åˆ°æµ…æ’åºï¼‰
    """
    explain_folder = explain_base / root_folder

    if not explain_folder.exists():
        return []

    folders = []

    def walk(current_path: Path):
        """é€’å½’éå†æ–‡ä»¶å¤¹"""
        for item in current_path.iterdir():
            if item.is_dir():
                rel_path = item.relative_to(explain_base)
                folders.append(rel_path)
                walk(item)

    # ä»æ ¹æ–‡ä»¶å¤¹å¼€å§‹éå†
    folders.append(root_folder)
    walk(explain_folder)

    # æŒ‰æ·±åº¦ä»æ·±åˆ°æµ…æ’åºï¼ˆæ·±åº¦ = è·¯å¾„ä¸­çš„ / æ•°é‡ï¼‰
    folders.sort(key=lambda p: len(p.parts), reverse=True)

    return folders


async def process_folders_batch(
    folders: list[Path],
    explain_base: Path,
    force: bool,
    model: str,
    max_workers: int = 8,
):
    """
    æ‰¹é‡å¼‚æ­¥å¤„ç†æ–‡ä»¶å¤¹

    Args:
        folders: æ–‡ä»¶å¤¹åˆ—è¡¨ï¼ˆæŒ‰æ·±åº¦ä»æ·±åˆ°æµ…æ’åºï¼‰
        explain_base: explain è¾“å‡ºçš„åŸºç¡€è·¯å¾„
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆ
        model: ä½¿ç”¨çš„æ¨¡å‹
        max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤ 8ï¼‰
    """
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL")

    if not api_key:
        raise ValueError("éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")

    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncOpenAI(api_key=api_key, base_url=base_url)

    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(max_workers)

    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
    tasks = [
        generate_readme_async(
            folder_path, explain_base, client, force, model, semaphore
        )
        for folder_path in folders
    ]

    # ä½¿ç”¨ tqdm å¼‚æ­¥è¿›åº¦æ¡
    print(f"\nâš¡ ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘ worker å¤„ç† {len(tasks)} ä¸ªæ–‡ä»¶å¤¹")
    print()

    results = []
    for coro in async_tqdm.as_completed(tasks, desc="ç”Ÿæˆ README", unit="folder"):
        result = await coro
        results.append(result)

    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for _, success in results if success)
    return success_count, len(results)


async def main_async():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é€’å½’ç”Ÿæˆå„å±‚çº§ç›®å½•çš„ README.mdï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰")
    parser.add_argument("repo_path", help="Git ä»“åº“è·¯å¾„")
    parser.add_argument("--subdir", default="", help="è¦åˆ†æçš„å­ç›®å½•ï¼ˆé»˜è®¤ä¸ºç©ºï¼Œåˆ†ææ•´ä¸ªä»“åº“ï¼‰")
    parser.add_argument("--output", "-o", help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šoutput/<repo_name>/explainï¼‰")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°ç”Ÿæˆ")
    parser.add_argument("--model", "-m", default="gemini-3-pro-preview", help="ä½¿ç”¨çš„æ¨¡å‹")
    parser.add_argument("--workers", "-w", type=int, default=8, help="æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤ï¼š8ï¼‰")

    args = parser.parse_args()

    # é»˜è®¤è¾“å‡ºè·¯å¾„ï¼šoutput/<repo_name>/explain-<date>
    if args.output is None:
        # ä½¿ç”¨ä»“åº“åä½œä¸º subdir å‚æ•°ä¼ ç»™ get_output_path
        repo_name = Path(args.repo_path).name
        args.output = get_output_path(args.repo_path, repo_name, "explain")

    explain_base = Path(args.output)

    # å¦‚æœ subdir ä¸ºç©ºï¼Œä½¿ç”¨ "." è¡¨ç¤ºæ ¹ç›®å½•
    root_folder = Path(args.subdir) if args.subdir else Path(".")

    print(f"ğŸš€ å¼€å§‹ä¸º {args.subdir if args.subdir else 'æ•´ä¸ªä»“åº“'}/ ç”Ÿæˆå±‚çº§ README")
    print()

    # æ‰¾åˆ°æ‰€æœ‰æ–‡ä»¶å¤¹ï¼ˆè‡ªåº•å‘ä¸Šï¼‰
    folders = find_all_folders(explain_base, root_folder)

    if not folders:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶å¤¹")
        return

    print(f"ğŸ“Š æ‰¾åˆ° {len(folders)} ä¸ªæ–‡ä»¶å¤¹ï¼ˆè‡ªåº•å‘ä¸Šï¼‰")
    print()

    # å¼‚æ­¥æ‰¹é‡å¤„ç†
    success_count, total_count = await process_folders_batch(
        folders,
        explain_base,
        args.force,
        args.model,
        args.workers,
    )

    print(f"\nğŸ‰ å®Œæˆï¼æˆåŠŸç”Ÿæˆ {success_count}/{total_count} ä¸ª README")


def main():
    """åŒæ­¥å…¥å£ï¼Œè¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°"""
    asyncio.run(main_async())


if __name__ == "__main__":
    main()
